{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "CS106_FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "fabulous-brook",
        "floating-dinner",
        "nonprofit-topic",
        "coastal-missile",
        "above-recorder",
        "assured-listening"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elegant-montgomery"
      },
      "source": [
        "## Preparations"
      ],
      "id": "elegant-montgomery"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWe3IcKoqSXz"
      },
      "source": [
        "## Tham khảo\n",
        "\n",
        "\n",
        "*   https://github.com/clam004/RL-Chat-pytorch\n",
        "*   https://github.com/GameDisplayer/DRL4DG\n",
        "*   https://pytorch.org/tutorials/beginner/chatbot_tutorial.html\n"
      ],
      "id": "BWe3IcKoqSXz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "comparable-vancouver"
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "import torch\n",
        "from torch.jit import script, trace\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "from io import open\n",
        "import itertools\n",
        "import math\n",
        "import numpy as np\n",
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
        "#device = torch.device(\"cpu\")"
      ],
      "id": "comparable-vancouver",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyOftyVvXBaW"
      },
      "source": [
        "## fix lỗi tensor "
      ],
      "id": "NyOftyVvXBaW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW_Mbb6c_PB7",
        "outputId": "f85a745b-f21e-4361-8fde-9a4d330ce338"
      },
      "source": [
        "!pip install torch==1.6.0 torchvision==0.7.0"
      ],
      "id": "eW_Mbb6c_PB7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torch==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/5e/35140615fc1f925023f489e71086a9ecc188053d263d3594237281284d82/torch-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (748.8MB)\n",
            "\u001b[K     |████████████████████████████████| 748.8MB 17kB/s \n",
            "\u001b[?25hCollecting torchvision==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4d/b5/60d5eb61f1880707a5749fea43e0ec76f27dfe69391cdec953ab5da5e676/torchvision-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0) (7.1.2)\n",
            "\u001b[31mERROR: torchtext 0.10.0 has requirement torch==1.9.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torch, torchvision\n",
            "  Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "Successfully installed torch-1.6.0 torchvision-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoxGOcKJlMc1"
      },
      "source": [
        "## Load data from drive"
      ],
      "id": "IoxGOcKJlMc1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ize9UY-KlQXl",
        "outputId": "e97f8e13-62c1-4144-8277-f65043ce4a86"
      },
      "source": [
        "\n",
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "id": "ize9UY-KlQXl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QijmK0Zz_aKR"
      },
      "source": [
        "# Tạo Folder CS106 và clone data từ link git sau [data](https://github.com/GameDisplayer/DRL4DG/tree/main/chatbot)"
      ],
      "id": "QijmK0Zz_aKR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyWS6--lmE86",
        "outputId": "3ac757c4-8caf-471f-8762-869d0feea7be"
      },
      "source": [
        "cd /content/drive/MyDrive/CS106"
      ],
      "id": "zyWS6--lmE86",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fabulous-brook"
      },
      "source": [
        "## Load & Preprocess Data "
      ],
      "id": "fabulous-brook"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auburn-beaver"
      },
      "source": [
        "Let's have a look to our dialydialog dataset : https://www.aclweb.org/anthology/I17-1099/"
      ],
      "id": "auburn-beaver"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spare-father",
        "outputId": "b73090de-9d17-4e8f-c71c-564e272362b5"
      },
      "source": [
        "corpus_name = \"dailydialog\"\n",
        "corpus = os.path.join(\"data\", corpus_name)\n",
        "\n",
        "def printLines(file, n=10):\n",
        "    with open(file, 'r', encoding=\"utf-8\") as datafile:\n",
        "        lines = datafile.readlines()\n",
        "    for line in lines[:n]:\n",
        "        print(line)\n",
        "\n",
        "printLines(os.path.join(corpus, \"dialogues_text.txt\"))"
      ],
      "id": "spare-father",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The kitchen stinks . __eou__ I'll throw out the garbage . __eou__\n",
            "\n",
            "So Dick , how about getting some coffee for tonight ? __eou__ Coffee ? I don ’ t honestly like that kind of stuff . __eou__ Come on , you can at least try a little , besides your cigarette . __eou__ What ’ s wrong with that ? Cigarette is the thing I go crazy for . __eou__ Not for me , Dick . __eou__\n",
            "\n",
            "Are things still going badly with your houseguest ? __eou__ Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw . __eou__ Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law . __eou__ You ’ re right . Everything is probably going to come to a head tonight . I ’ ll keep you informed . __eou__\n",
            "\n",
            "Would you mind waiting a while ? __eou__ Well , how long will it be ? __eou__ I'm not sure . But I'll get a table ready as fast as I can . __eou__ OK . We'll wait . __eou__\n",
            "\n",
            "Are you going to the annual party ? I can give you a ride if you need one . __eou__ Thanks a lot . That's the favor I was going to ask you for . __eou__ The pleasure is mine . __eou__\n",
            "\n",
            "Isn ’ t he the best instructor ? I think he ’ s so hot . Wow ! I really feel energized , don ’ t you ? __eou__ I swear , I ’ m going to kill you for this . __eou__ What ’ s wrong ? Didn ’ t you think it was fun ? ! __eou__ Oh , yeah ! I had a blast ! I love sweating like a pig with a bunch of pot bellies who all smell bad . Sorry , I ’ m just not into this health kick . __eou__ Oh , no , get off it . It wasn ’ t such a killer class . You just have to get into it . Like they say , no pain , no gain . __eou__ I am wiped out . Thank you . __eou__ Look , next time get yourself some comfy shoes . You ’ re gonna come back again with me , aren ’ t you ? __eou__ Never ! But thank you for inviting me . __eou__ Come on . You ’ ll feel better after we hit the showers . __eou__\n",
            "\n",
            "Can I take your order now or do you still want to look at the menu ? __eou__ Well , I want a fillet steak , medium , but my little girl doesn't care for steak . Could she have something else instead ? __eou__ Certainly . How about spaghetti with clams and shrimps . __eou__ Sounds delicious . OK . She'll try that . __eou__\n",
            "\n",
            "Can you manage chopsticks ? __eou__ Why not ? See . __eou__ Good mastery . How do you like our Chinese food ? __eou__ Oh , great ! It's delicious . You see , I am already putting on weight . There is one thing I don't like however , MSG . __eou__ What's wrong with MSG ? It helps to bring out the taste of the food . __eou__ According to some studies it may cause cancer . __eou__ Oh , don't let that worry you . If that were true , China wouldn't have such a large population . __eou__ I just happen to have a question for you guys . Why do the Chinese cook the vegetables ? You see what I mean is that most vitamin are destroyed when heated . __eou__ I don't know exactly . It's a tradition . Maybe it's for sanitary reasons . __eou__\n",
            "\n",
            "I'm exhausted . __eou__ Okay , let's go home . __eou__\n",
            "\n",
            "Good evening . Welcome to Cherry's . Do you have a reservation ? __eou__ No , we don't . __eou__ How many of you , please ? __eou__ Six , including two kids . __eou__ I'm afraid all the big tables are taken . __eou__\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "floating-dinner"
      },
      "source": [
        "## Create formatted data file"
      ],
      "id": "floating-dinner"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "current-access"
      },
      "source": [
        "We'll create a formatted data file in which each line contains a tab-separated query sentence and a response sentence pair.\n",
        "\n",
        "The following functions pase the `dailogues_text.txt` data file.\n",
        "- `loadLines` splits each line of the file into conversations\n",
        "- `extractSentencePairs` extracts pairs of sentences from conversations"
      ],
      "id": "current-access"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "confidential-consideration"
      },
      "source": [
        "# Splits each line of the file into a dictionary of fields\n",
        "def loadLines(fileName):\n",
        "    conversations = []\n",
        "    with open(fileName, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split(\"__eou__\")\n",
        "            conversations.append(values)\n",
        "    return conversations\n",
        "\n",
        "\n",
        "# Extracts pairs of sentences from conversations\n",
        "def extractSentencePairs(conversations):\n",
        "    qa_pairs = []\n",
        "    for conversation in conversations:\n",
        "        # Iterate over all the lines of the conversation\n",
        "        for i in range(len(conversation) - 1):  # We ignore the last line (no answer for it)\n",
        "            inputLine = conversation[i].strip()\n",
        "            targetLine = conversation[i+1].strip()\n",
        "            # Filter wrong samples (if one of the lists is empty)\n",
        "            if inputLine and targetLine:\n",
        "                qa_pairs.append([inputLine, targetLine])\n",
        "    return qa_pairs"
      ],
      "id": "confidential-consideration",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "southwest-attempt"
      },
      "source": [
        "Now we call the above functions to create a new file : `formatted_dialogues_text.txt`"
      ],
      "id": "southwest-attempt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inner-stuff",
        "outputId": "39b2d4bc-877b-4374-932e-4ca03653f508"
      },
      "source": [
        "# Define path to new file\n",
        "datafile = os.path.join(corpus, \"formatted_dialogues_text.txt\")\n",
        "\n",
        "delimiter = '\\t'\n",
        "# Unescape the delimiter\n",
        "delimiter = str(codecs.decode(delimiter, \"unicode_escape\"))\n",
        "\n",
        "print(\"\\nLoading conversations...\")\n",
        "conversations = loadLines(os.path.join(corpus, \"dialogues_text.txt\"))\n",
        "\n",
        "# Write new csv file\n",
        "print(\"\\nWriting newly formatted file...\")\n",
        "with open(datafile, 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter=delimiter, lineterminator='\\n')\n",
        "    for pair in extractSentencePairs(conversations):\n",
        "        writer.writerow(pair)\n",
        "\n",
        "# Print a sample of lines\n",
        "print(\"\\nSample lines from file:\")\n",
        "printLines(datafile)"
      ],
      "id": "inner-stuff",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Loading conversations...\n",
            "\n",
            "Writing newly formatted file...\n",
            "\n",
            "Sample lines from file:\n",
            "The kitchen stinks .\tI'll throw out the garbage .\n",
            "\n",
            "So Dick , how about getting some coffee for tonight ?\tCoffee ? I don ’ t honestly like that kind of stuff .\n",
            "\n",
            "Coffee ? I don ’ t honestly like that kind of stuff .\tCome on , you can at least try a little , besides your cigarette .\n",
            "\n",
            "Come on , you can at least try a little , besides your cigarette .\tWhat ’ s wrong with that ? Cigarette is the thing I go crazy for .\n",
            "\n",
            "What ’ s wrong with that ? Cigarette is the thing I go crazy for .\tNot for me , Dick .\n",
            "\n",
            "Are things still going badly with your houseguest ?\tGetting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw .\n",
            "\n",
            "Getting worse . Now he ’ s eating me out of house and home . I ’ Ve tried talking to him but it all goes in one ear and out the other . He makes himself at home , which is fine . But what really gets me is that yesterday he walked into the living room in the raw and I had company over ! That was the last straw .\tLeo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law .\n",
            "\n",
            "Leo , I really think you ’ re beating around the bush with this guy . I know he used to be your best friend in college , but I really think it ’ s time to lay down the law .\tYou ’ re right . Everything is probably going to come to a head tonight . I ’ ll keep you informed .\n",
            "\n",
            "Would you mind waiting a while ?\tWell , how long will it be ?\n",
            "\n",
            "Well , how long will it be ?\tI'm not sure . But I'll get a table ready as fast as I can .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nonprofit-topic"
      },
      "source": [
        "## Load and trim data"
      ],
      "id": "nonprofit-topic"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conservative-gallery"
      },
      "source": [
        "Now let's create a vocabulary and load query/response sentence pairs into memory.\n",
        "\n",
        "First we must create a mapping of each word to a discrete numerical space (the index value).\n",
        "\n",
        "Voc class keeps the mapping from words to indexes, a reverse mapping of indexes to words, a count of each word and a total word count.\n",
        "There are 3 central methods :\n",
        "- `addWord` to add a word to the vocabulary\n",
        "- `addSentence` to add all words in a sentence\n",
        "- `trim` for trimming infrequently seen words"
      ],
      "id": "conservative-gallery"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "organized-letters"
      },
      "source": [
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count SOS, EOS, PAD\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Remove words below a certain count threshold\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = []\n",
        "\n",
        "        for k, v in self.word2count.items():\n",
        "            if v >= min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        # Reinitialize dictionaries\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3 # Count default tokens\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)"
      ],
      "id": "organized-letters",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "competent-concentration"
      },
      "source": [
        "Before assemble our vocabulary and query/response sentence pairs we must perform some preprocessing.\n",
        "\n",
        "1. Convert the Unicode strings to ASCII with `unicodeToAscii`.\n",
        "2. Convert all letters to lowercase and trim all non-letter characters except basic punctuation `normalizeString`\n",
        "3. Filter out sentences witg length greater than the `MAX_LENGTH` threshold in `filterPairs`\n"
      ],
      "id": "competent-concentration"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "recent-iceland"
      },
      "source": [
        "#handle dull_responses now\n",
        "dull_responses = [\"I do not know what you are talking about.\", \"I do not know.\", \n",
        " \"You do not know.\", \"You know what I mean.\", \"I know what you mean.\", \n",
        " \"You know what I am saying.\", \"You do not know anything.\"]"
      ],
      "id": "recent-iceland",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quick-festival"
      },
      "source": [
        "MAX_LENGTH = 15  # Maximum sentence length to consider\n",
        "\n",
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# https://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "# Read query/response pairs and return a voc object\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    # Read the file and split into lines\n",
        "    lines = open(datafile, encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
        "def filterPair(p):\n",
        "    # Input sequences need to preserve the last word for EOS token\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "# Filter pairs using filterPair condition\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "\n",
        "# Using the functions defined above, return a populated voc object and pairs list\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for d in dull_responses:\n",
        "        voc.addSentence(d)\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs"
      ],
      "id": "quick-festival",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vertical-jewel"
      },
      "source": [
        "Finally assmble voc and pairs"
      ],
      "id": "vertical-jewel"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "threaded-smell",
        "outputId": "6ce60831-e328-4595-fbd2-61c9d1ad2466"
      },
      "source": [
        "# Load/Assemble voc and pairs\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "voc, pairs = loadPrepareData(corpus, corpus_name, datafile, save_dir)\n",
        "# Print some pairs to validate\n",
        "print(\"\\npairs:\")\n",
        "for pair in pairs[:10]:\n",
        "    print(pair)"
      ],
      "id": "threaded-smell",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 89862 sentence pairs\n",
            "Trimmed to 43724 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 10126\n",
            "\n",
            "pairs:\n",
            "['the kitchen stinks .', 'i ll throw out the garbage .']\n",
            "['so dick how about getting some coffee for tonight ?', 'coffee ? i don t honestly like that kind of stuff .']\n",
            "['coffee ? i don t honestly like that kind of stuff .', 'come on you can at least try a little besides your cigarette .']\n",
            "['would you mind waiting a while ?', 'well how long will it be ?']\n",
            "['i swear i m going to kill you for this .', 'what s wrong ? didn t you think it was fun ? !']\n",
            "['never ! but thank you for inviting me .', 'come on . you ll feel better after we hit the showers .']\n",
            "['certainly . how about spaghetti with clams and shrimps .', 'sounds delicious . ok . she ll try that .']\n",
            "['can you manage chopsticks ?', 'why not ? see .']\n",
            "['why not ? see .', 'good mastery . how do you like our chinese food ?']\n",
            "['i m exhausted .', 'okay let s go home .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5hUhrqp_EQh"
      },
      "source": [
        ""
      ],
      "id": "O5hUhrqp_EQh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coastal-missile"
      },
      "source": [
        "### Trimming rarely used words out of vocab"
      ],
      "id": "coastal-missile"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "choice-virginia"
      },
      "source": [
        "One tactic beneficial to achieve faster convergence during training is trimming rarely used words out of our vocabulary.\n",
        "\n",
        "1. Trim words used under `MIN_COUNT` threshold using `voc.trim`\n",
        "2. Filter out pairs with trimmed words"
      ],
      "id": "choice-virginia"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chinese-lawyer",
        "outputId": "7fa0142b-8e9f-4908-86e1-b47901844d9e"
      },
      "source": [
        "MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    # Trim words used under the MIN_COUNT from the voc\n",
        "    voc.trim(MIN_COUNT)\n",
        "    # Filter out pairs with trimmed words\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "        # Check input sentence\n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "        # Check output sentence\n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in voc.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "\n",
        "# Trim voc and pairs\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT)"
      ],
      "id": "chinese-lawyer",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keep_words 6159 / 10123 = 0.6084\n",
            "Trimmed from 43724 pairs to 38716, 0.8855 of total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "above-recorder"
      },
      "source": [
        "## Prepare Data for Models"
      ],
      "id": "above-recorder"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "specified-fancy"
      },
      "source": [
        "BATCH TECHNIQUE\n",
        "\n",
        "To accomodate sentences of different sizes in the same batch we make our batched input tensor of shape `(max_length, batch_size)` where sentences shorter than the max_length are zeropadded after the `EOS_token`.\n",
        "\n",
        "- `inputVar` function handles the process of converting sentences to tensor. It returns a tensor of `lengths` for each sequence in the batch for the decoder.\n",
        "- `outputVar` function performs the same as `inputVar` but returns a binary mask tensor and a maximum target sentence length.\n",
        "- `batch2TrainData` takes a bunch of pairs and returns the input and target tensors"
      ],
      "id": "specified-fancy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "terminal-crystal",
        "outputId": "26d5f70a-2e4d-46a7-818e-363b0858c532"
      },
      "source": [
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for i, seq in enumerate(l):\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_token:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m\n",
        "\n",
        "# Returns padded input sequence tensor and lengths\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "# Returns padded target sequence tensor, padding mask, and max target length\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "# Returns all items for a given batch of pairs\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "\n",
        "# Example for validation\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"input_variable:\", input_variable)\n",
        "print(\"lengths:\", lengths)\n",
        "print(\"target_variable:\", target_variable)\n",
        "print(\"mask:\", mask)\n",
        "print(\"max_target_len:\", max_target_len)"
      ],
      "id": "terminal-crystal",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input_variable: tensor([[ 130,    7,   50,  811,  187],\n",
            "        [  15, 1727,    8,   16,   15],\n",
            "        [  74,   37,   34,   17,    7],\n",
            "        [   8, 1753,   26,   58,  159],\n",
            "        [  15,   99,   27,  183,   57],\n",
            "        [ 154,    4,  239,  245,   30],\n",
            "        [  16,    8,   30,   71,    2],\n",
            "        [  95,   34,    2,    2,    0],\n",
            "        [  48,  486,    0,    0,    0],\n",
            "        [2653,   30,    0,    0,    0],\n",
            "        [ 111,    2,    0,    0,    0],\n",
            "        [  30,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "lengths: tensor([13, 11,  8,  8,  7])\n",
            "target_variable: tensor([[ 156,   16,  130,  176,  145],\n",
            "        [   8,   34,   74,    9,    8],\n",
            "        [   9, 5403,    8,    8,  348],\n",
            "        [  15,  164,   15,   61,  184],\n",
            "        [   2,  374,    2,   30,   62],\n",
            "        [   0,   15,    0,    2,   13],\n",
            "        [   0,  107,    0,    0,  177],\n",
            "        [   0,    8,    0,    0,   30],\n",
            "        [   0,  826,    0,    0,   80],\n",
            "        [   0,   98,    0,    0,  139],\n",
            "        [   0,  580,    0,    0,   45],\n",
            "        [   0,   30,    0,    0, 1024],\n",
            "        [   0,    2,    0,    0, 1157],\n",
            "        [   0,    0,    0,    0,   15],\n",
            "        [   0,    0,    0,    0,    2]])\n",
            "mask: tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [False,  True, False,  True,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False, False, False, False,  True],\n",
            "        [False, False, False, False,  True]])\n",
            "max_target_len: 15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "affecting-andorra"
      },
      "source": [
        "## Seq2Seq"
      ],
      "id": "affecting-andorra"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "difficult-success"
      },
      "source": [
        "The brain of our chatbot is a sequence to sequence model.\n",
        "\n",
        "Một RNN hoạt động như một _encoder_ mã hóa chuỗi đầu vào có độ dài thay đổi thành vectơ ngữ cảnh có độ dài cố định (lớp ẩn cuối cùng của RNN).\n",
        "RNN thứ hai là _decoder_ nhận đầu vào một từ và một vectơ ngữ cảnh và trả về một dự đoán cho từ tiếp theo trong chuỗi."
      ],
      "id": "difficult-success"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "curious-fancy"
      },
      "source": [
        " ### Encoder"
      ],
      "id": "curious-fancy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conventional-salon"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Bộ mã hóa RNN lặp qua các mã thông báo và xuất ra một vectơ \"đầu ra\" và một vectơ \"sate ẩn\". vectơ trạng thái ẩn được chuyển sang bước thời gian tiếp theo trong khi vectơ đầu ra là bộ ghi.\n",
        "Bộ mã hóa chuyển đổi bối cảnh mà nó nhìn thấy tại mỗi điểm trong chuỗi thành một tập hợp các điểm trong không gian chiều cao. Bộ giải mã sẽ sử dụng nó để tạo ra từ đầu ra.\n",
        "\n",
        "Chúng tôi sử dụng Thiết bị định kỳ lặp lại nhiều lớp hai chiều.\n",
        "Nó mang lại lợi thế cho việc mã hóa cả ngữ cảnh trong quá khứ và tương lai!\n",
        "\n",
        "Computation Graph :\n",
        "1. Convert word indexed to embeddings\n",
        "2. Pack padded batch of sequences for RNN module\n",
        "3. Forward pass through GRU\n",
        "4. Unpack padding\n",
        "5. Sum bidirectional GRU outputs\n",
        "6. Return output and final hidden state"
      ],
      "id": "conventional-salon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greenhouse-relaxation"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
        "        #   because our input size is a word embedding with number of features == hidden_size\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        # Convert word indexes to embeddings\n",
        "        embedded = self.embedding(input_seq)\n",
        "        # Pack padded batch of sequences for RNN module\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        # Forward pass through GRU\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        # Unpack padding\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        # Sum bidirectional GRU outputs\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
        "        # Return output and final hidden state\n",
        "        return outputs, hidden"
      ],
      "id": "greenhouse-relaxation",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blind-elimination"
      },
      "source": [
        "### Decoder"
      ],
      "id": "blind-elimination"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suitable-velvet"
      },
      "source": [
        "The decoder RNN uses the encoder's context vectors and internal hidden states to generate the next word of the sequence.\n",
        "It continues generating words until an `EOS_token`.\n",
        "The problem with a vanilla seq2seq decoder is that if we rely woley on the context vector it will have information loss. (especially with long input sequences).\n",
        "\n",
        "-> `attention mechanism` allows the decoder to pay attention to certain parts of the input sequence."
      ],
      "id": "suitable-velvet"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "municipal-lunch"
      },
      "source": [
        "# Luong attention layer\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        # Calculate the attention weights (energies) based on the given method\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        # Transpose max_length and batch_size dimensions\n",
        "        attn_energies = attn_energies.t()\n",
        "\n",
        "        # Return the softmax normalized probability scores (with added dimension)\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
      ],
      "id": "municipal-lunch",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dressed-artist"
      },
      "source": [
        "Now that the attention submodule is implemented let's dive into the actual decoder model.\n",
        "\n",
        "Computation Graph:\n",
        "1. Get embedding of current input word.\n",
        "2. Forward through unidirectional GRU\n",
        "3. Calculate attention weights from the current GRU output\n",
        "4. Multiply attention weights to encoder outputs to get a new context vector\n",
        "5. Concatenate weighted context vector and GRU using Luong eq\n",
        "6. Predict next word\n",
        "7. Return output and final hidden state"
      ],
      "id": "dressed-artist"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prescribed-suspect"
      },
      "source": [
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        # Keep for reference\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Define layers\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        # Note: we run this one step (word) at a time\n",
        "        # Get embedding of current input word\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        # Forward through unidirectional GRU\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        # Calculate attention weights from the current GRU output\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        # Predict next word using Luong eq. 6\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        # Return output and final hidden state\n",
        "        return output, hidden"
      ],
      "id": "prescribed-suspect",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "constitutional-trance"
      },
      "source": [
        "## Training "
      ],
      "id": "constitutional-trance"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "decimal-rouge"
      },
      "source": [
        "`maskNNLLLoss` calculates the average negative log likelihood of the elements that correspond to a 1 in the mask tensor."
      ],
      "id": "decimal-rouge"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "improving-screen"
      },
      "source": [
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    loss = loss.to(device)\n",
        "    return loss, nTotal.item()"
      ],
      "id": "improving-screen",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alive-exception"
      },
      "source": [
        "Single training iteration (single batch of inputs)\n",
        "\n",
        "Couple of clever tricks :\n",
        "- `teacher forcing` at some probability (set by `teacher_forcing_ratio`) current target word is used as the decoder's next input rather than using the decoder's current guess.\n",
        "- `gradient clipping` commonly used technique for countering the \"exploding gradient\" problem."
      ],
      "id": "alive-exception"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "processed-calculation"
      },
      "source": [
        "Sequence of operations :\n",
        "1. Forward pass entire input batch through encoder\n",
        "2. initialize decoder inputs as SOS_token and hidden state as the encoder's final hidden state\n",
        "3. Forward input batch sequence through decoder one time step at a time\n",
        "4. If teacher forcing : set next decoder input as the current target else : set next decoder input as the current decoder ouptput\n",
        "5. Calculate and accumulate loss\n",
        "6. Perform backpropagation\n",
        "7. Clip gradients\n",
        "8. Update encoder and decoder"
      ],
      "id": "processed-calculation"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taken-investing"
      },
      "source": [
        "\n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
        "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
        "\n",
        "    # Zero gradients\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    # Initialize variables\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "\n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    # Perform backpropatation\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip gradients: gradients are modified in place\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    # Adjust model weights\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals"
      ],
      "id": "taken-investing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appreciated-gardening"
      },
      "source": [
        "Training iterations + save our model to run inferences or continue training !"
      ],
      "id": "appreciated-gardening"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faced-memory"
      },
      "source": [
        "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
      ],
      "id": "faced-memory",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auburn-weekly"
      },
      "source": [
        "## Gready decoding for generating sentences"
      ],
      "id": "auburn-weekly"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stopped-greenhouse"
      },
      "source": [
        "Decoding method when training is not using teacher forcing. At each time step we choose the word from decoder_output with the highest softmax value."
      ],
      "id": "stopped-greenhouse"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boxed-subject"
      },
      "source": [
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        # Forward input through encoder model\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "        # Initialize decoder input with SOS_token\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
        "        # Initialize tensors to append decoded word s to\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "        # Iteratively decode one word token at a time\n",
        "        for _ in range(max_length):\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            # Obtain most likely word token and its softmax score\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            # Record token and score\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            # Prepare current token to be next decoder input (add a dimension)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "        # Return collections of word tokens and scores\n",
        "        return all_tokens, all_scores"
      ],
      "id": "boxed-subject",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "industrial-farming"
      },
      "source": [
        "## Evaluation of model"
      ],
      "id": "industrial-farming"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unlike-blast"
      },
      "source": [
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    ### Format input sentence as a batch\n",
        "    # words -> indexes\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    # Create lengths tensor\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    # Transpose dimensions of batch to match models' expectations\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    # Use appropriate device\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    # Decode sentence with searcher\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    # indexes -> words\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n",
        "\n",
        "\n",
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            print(\"Quesion\")\n",
        "            print(input_sentence)\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            \n",
        "            \n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "id": "unlike-blast",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "removable-institute"
      },
      "source": [
        "## Run model"
      ],
      "id": "removable-institute"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "comfortable-producer"
      },
      "source": [
        "possible to laod from checkpoint"
      ],
      "id": "comfortable-producer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "understanding-relief",
        "outputId": "b178d058-254e-47e3-c66f-256a939c5ae5"
      },
      "source": [
        "# Configure models\n",
        "model_name = 'cb_model'\n",
        "attn_model = 'dot'\n",
        "#attn_model = 'general'\n",
        "#attn_model = 'concat'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "# Set checkpoint to load from; set to None if starting from scratch\n",
        "loadFilename ='data/save/cb_model/dailydialog/2-2_500/20000_checkpoint.tar'\n",
        "#loadFilename ='data/save/RL_model_seq/dailydialog/15000_checkpoint.tar'\n",
        "\n",
        "checkpoint_iter = 10000 # 4000\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                            '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                            '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "#print(loadFilename)\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    #checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "id": "understanding-relief",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naJccGoiBKPm"
      },
      "source": [
        "# Kết quả của việc CB model"
      ],
      "id": "naJccGoiBKPm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sound-belief"
      },
      "source": [
        "## Train !"
      ],
      "id": "sound-belief"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "formed-explanation",
        "scrolled": true,
        "outputId": "7e247646-1b01-4a39-aebd-c27f6960d961"
      },
      "source": [
        "# Configure training/optimization\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 30000 #4000\n",
        "print_every = 1\n",
        "save_every = 1000\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name, loadFilename)"
      ],
      "id": "formed-explanation",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration: 25001; Percent complete: 83.3%; Average loss: 0.0795\n",
            "Iteration: 25002; Percent complete: 83.3%; Average loss: 0.1402\n",
            "Iteration: 25003; Percent complete: 83.3%; Average loss: 0.1663\n",
            "Iteration: 25004; Percent complete: 83.3%; Average loss: 0.0794\n",
            "Iteration: 25005; Percent complete: 83.4%; Average loss: 0.1038\n",
            "Iteration: 25006; Percent complete: 83.4%; Average loss: 0.0893\n",
            "Iteration: 25007; Percent complete: 83.4%; Average loss: 0.1438\n",
            "Iteration: 25008; Percent complete: 83.4%; Average loss: 0.1357\n",
            "Iteration: 25009; Percent complete: 83.4%; Average loss: 0.0935\n",
            "Iteration: 25010; Percent complete: 83.4%; Average loss: 0.1245\n",
            "Iteration: 25011; Percent complete: 83.4%; Average loss: 0.1360\n",
            "Iteration: 25012; Percent complete: 83.4%; Average loss: 0.1170\n",
            "Iteration: 25013; Percent complete: 83.4%; Average loss: 0.0856\n",
            "Iteration: 25014; Percent complete: 83.4%; Average loss: 0.1183\n",
            "Iteration: 25015; Percent complete: 83.4%; Average loss: 0.1401\n",
            "Iteration: 25016; Percent complete: 83.4%; Average loss: 0.1376\n",
            "Iteration: 25017; Percent complete: 83.4%; Average loss: 0.0915\n",
            "Iteration: 25018; Percent complete: 83.4%; Average loss: 0.1380\n",
            "Iteration: 25019; Percent complete: 83.4%; Average loss: 0.1460\n",
            "Iteration: 25020; Percent complete: 83.4%; Average loss: 0.1905\n",
            "Iteration: 25021; Percent complete: 83.4%; Average loss: 0.1861\n",
            "Iteration: 25022; Percent complete: 83.4%; Average loss: 0.1247\n",
            "Iteration: 25023; Percent complete: 83.4%; Average loss: 0.1335\n",
            "Iteration: 25024; Percent complete: 83.4%; Average loss: 0.0961\n",
            "Iteration: 25025; Percent complete: 83.4%; Average loss: 0.0977\n",
            "Iteration: 25026; Percent complete: 83.4%; Average loss: 0.2031\n",
            "Iteration: 25027; Percent complete: 83.4%; Average loss: 0.1417\n",
            "Iteration: 25028; Percent complete: 83.4%; Average loss: 0.1465\n",
            "Iteration: 25029; Percent complete: 83.4%; Average loss: 0.1213\n",
            "Iteration: 25030; Percent complete: 83.4%; Average loss: 0.1058\n",
            "Iteration: 25031; Percent complete: 83.4%; Average loss: 0.0955\n",
            "Iteration: 25032; Percent complete: 83.4%; Average loss: 0.1093\n",
            "Iteration: 25033; Percent complete: 83.4%; Average loss: 0.1860\n",
            "Iteration: 25034; Percent complete: 83.4%; Average loss: 0.0879\n",
            "Iteration: 25035; Percent complete: 83.5%; Average loss: 0.1300\n",
            "Iteration: 25036; Percent complete: 83.5%; Average loss: 0.1684\n",
            "Iteration: 25037; Percent complete: 83.5%; Average loss: 0.1441\n",
            "Iteration: 25038; Percent complete: 83.5%; Average loss: 0.1688\n",
            "Iteration: 25039; Percent complete: 83.5%; Average loss: 0.1130\n",
            "Iteration: 25040; Percent complete: 83.5%; Average loss: 0.1366\n",
            "Iteration: 25041; Percent complete: 83.5%; Average loss: 0.0960\n",
            "Iteration: 25042; Percent complete: 83.5%; Average loss: 0.2196\n",
            "Iteration: 25043; Percent complete: 83.5%; Average loss: 0.1608\n",
            "Iteration: 25044; Percent complete: 83.5%; Average loss: 0.1507\n",
            "Iteration: 25045; Percent complete: 83.5%; Average loss: 0.1572\n",
            "Iteration: 25046; Percent complete: 83.5%; Average loss: 0.0876\n",
            "Iteration: 25047; Percent complete: 83.5%; Average loss: 0.1316\n",
            "Iteration: 25048; Percent complete: 83.5%; Average loss: 0.1311\n",
            "Iteration: 25049; Percent complete: 83.5%; Average loss: 0.1483\n",
            "Iteration: 25050; Percent complete: 83.5%; Average loss: 0.1325\n",
            "Iteration: 25051; Percent complete: 83.5%; Average loss: 0.1342\n",
            "Iteration: 25052; Percent complete: 83.5%; Average loss: 0.1254\n",
            "Iteration: 25053; Percent complete: 83.5%; Average loss: 0.1472\n",
            "Iteration: 25054; Percent complete: 83.5%; Average loss: 0.1279\n",
            "Iteration: 25055; Percent complete: 83.5%; Average loss: 0.1027\n",
            "Iteration: 25056; Percent complete: 83.5%; Average loss: 0.1683\n",
            "Iteration: 25057; Percent complete: 83.5%; Average loss: 0.0998\n",
            "Iteration: 25058; Percent complete: 83.5%; Average loss: 0.1292\n",
            "Iteration: 25059; Percent complete: 83.5%; Average loss: 0.1133\n",
            "Iteration: 25060; Percent complete: 83.5%; Average loss: 0.1556\n",
            "Iteration: 25061; Percent complete: 83.5%; Average loss: 0.1332\n",
            "Iteration: 25062; Percent complete: 83.5%; Average loss: 0.0664\n",
            "Iteration: 25063; Percent complete: 83.5%; Average loss: 0.1336\n",
            "Iteration: 25064; Percent complete: 83.5%; Average loss: 0.1158\n",
            "Iteration: 25065; Percent complete: 83.5%; Average loss: 0.1395\n",
            "Iteration: 25066; Percent complete: 83.6%; Average loss: 0.1406\n",
            "Iteration: 25067; Percent complete: 83.6%; Average loss: 0.0978\n",
            "Iteration: 25068; Percent complete: 83.6%; Average loss: 0.1291\n",
            "Iteration: 25069; Percent complete: 83.6%; Average loss: 0.1546\n",
            "Iteration: 25070; Percent complete: 83.6%; Average loss: 0.1554\n",
            "Iteration: 25071; Percent complete: 83.6%; Average loss: 0.0972\n",
            "Iteration: 25072; Percent complete: 83.6%; Average loss: 0.1234\n",
            "Iteration: 25073; Percent complete: 83.6%; Average loss: 0.1762\n",
            "Iteration: 25074; Percent complete: 83.6%; Average loss: 0.1471\n",
            "Iteration: 25075; Percent complete: 83.6%; Average loss: 0.1445\n",
            "Iteration: 25076; Percent complete: 83.6%; Average loss: 0.1476\n",
            "Iteration: 25077; Percent complete: 83.6%; Average loss: 0.1706\n",
            "Iteration: 25078; Percent complete: 83.6%; Average loss: 0.1391\n",
            "Iteration: 25079; Percent complete: 83.6%; Average loss: 0.1085\n",
            "Iteration: 25080; Percent complete: 83.6%; Average loss: 0.1412\n",
            "Iteration: 25081; Percent complete: 83.6%; Average loss: 0.1311\n",
            "Iteration: 25082; Percent complete: 83.6%; Average loss: 0.1270\n",
            "Iteration: 25083; Percent complete: 83.6%; Average loss: 0.1789\n",
            "Iteration: 25084; Percent complete: 83.6%; Average loss: 0.1506\n",
            "Iteration: 25085; Percent complete: 83.6%; Average loss: 0.1098\n",
            "Iteration: 25086; Percent complete: 83.6%; Average loss: 0.1384\n",
            "Iteration: 25087; Percent complete: 83.6%; Average loss: 0.1121\n",
            "Iteration: 25088; Percent complete: 83.6%; Average loss: 0.1525\n",
            "Iteration: 25089; Percent complete: 83.6%; Average loss: 0.1425\n",
            "Iteration: 25090; Percent complete: 83.6%; Average loss: 0.1668\n",
            "Iteration: 25091; Percent complete: 83.6%; Average loss: 0.1233\n",
            "Iteration: 25092; Percent complete: 83.6%; Average loss: 0.1395\n",
            "Iteration: 25093; Percent complete: 83.6%; Average loss: 0.1132\n",
            "Iteration: 25094; Percent complete: 83.6%; Average loss: 0.1750\n",
            "Iteration: 25095; Percent complete: 83.7%; Average loss: 0.1250\n",
            "Iteration: 25096; Percent complete: 83.7%; Average loss: 0.1391\n",
            "Iteration: 25097; Percent complete: 83.7%; Average loss: 0.0993\n",
            "Iteration: 25098; Percent complete: 83.7%; Average loss: 0.1097\n",
            "Iteration: 25099; Percent complete: 83.7%; Average loss: 0.1188\n",
            "Iteration: 25100; Percent complete: 83.7%; Average loss: 0.1310\n",
            "Iteration: 25101; Percent complete: 83.7%; Average loss: 0.1470\n",
            "Iteration: 25102; Percent complete: 83.7%; Average loss: 0.0938\n",
            "Iteration: 25103; Percent complete: 83.7%; Average loss: 0.1296\n",
            "Iteration: 25104; Percent complete: 83.7%; Average loss: 0.1143\n",
            "Iteration: 25105; Percent complete: 83.7%; Average loss: 0.1807\n",
            "Iteration: 25106; Percent complete: 83.7%; Average loss: 0.1532\n",
            "Iteration: 25107; Percent complete: 83.7%; Average loss: 0.1588\n",
            "Iteration: 25108; Percent complete: 83.7%; Average loss: 0.1157\n",
            "Iteration: 25109; Percent complete: 83.7%; Average loss: 0.1288\n",
            "Iteration: 25110; Percent complete: 83.7%; Average loss: 0.1363\n",
            "Iteration: 25111; Percent complete: 83.7%; Average loss: 0.1290\n",
            "Iteration: 25112; Percent complete: 83.7%; Average loss: 0.1585\n",
            "Iteration: 25113; Percent complete: 83.7%; Average loss: 0.1626\n",
            "Iteration: 25114; Percent complete: 83.7%; Average loss: 0.1186\n",
            "Iteration: 25115; Percent complete: 83.7%; Average loss: 0.1303\n",
            "Iteration: 25116; Percent complete: 83.7%; Average loss: 0.1346\n",
            "Iteration: 25117; Percent complete: 83.7%; Average loss: 0.1930\n",
            "Iteration: 25118; Percent complete: 83.7%; Average loss: 0.1439\n",
            "Iteration: 25119; Percent complete: 83.7%; Average loss: 0.1437\n",
            "Iteration: 25120; Percent complete: 83.7%; Average loss: 0.1276\n",
            "Iteration: 25121; Percent complete: 83.7%; Average loss: 0.1447\n",
            "Iteration: 25122; Percent complete: 83.7%; Average loss: 0.1348\n",
            "Iteration: 25123; Percent complete: 83.7%; Average loss: 0.1008\n",
            "Iteration: 25124; Percent complete: 83.7%; Average loss: 0.1060\n",
            "Iteration: 25125; Percent complete: 83.8%; Average loss: 0.1171\n",
            "Iteration: 25126; Percent complete: 83.8%; Average loss: 0.1297\n",
            "Iteration: 25127; Percent complete: 83.8%; Average loss: 0.1313\n",
            "Iteration: 25128; Percent complete: 83.8%; Average loss: 0.1281\n",
            "Iteration: 25129; Percent complete: 83.8%; Average loss: 0.1123\n",
            "Iteration: 25130; Percent complete: 83.8%; Average loss: 0.0917\n",
            "Iteration: 25131; Percent complete: 83.8%; Average loss: 0.1555\n",
            "Iteration: 25132; Percent complete: 83.8%; Average loss: 0.0779\n",
            "Iteration: 25133; Percent complete: 83.8%; Average loss: 0.1389\n",
            "Iteration: 25134; Percent complete: 83.8%; Average loss: 0.1347\n",
            "Iteration: 25135; Percent complete: 83.8%; Average loss: 0.1507\n",
            "Iteration: 25136; Percent complete: 83.8%; Average loss: 0.1661\n",
            "Iteration: 25137; Percent complete: 83.8%; Average loss: 0.1423\n",
            "Iteration: 25138; Percent complete: 83.8%; Average loss: 0.1217\n",
            "Iteration: 25139; Percent complete: 83.8%; Average loss: 0.1201\n",
            "Iteration: 25140; Percent complete: 83.8%; Average loss: 0.0912\n",
            "Iteration: 25141; Percent complete: 83.8%; Average loss: 0.1219\n",
            "Iteration: 25142; Percent complete: 83.8%; Average loss: 0.1610\n",
            "Iteration: 25143; Percent complete: 83.8%; Average loss: 0.1326\n",
            "Iteration: 25144; Percent complete: 83.8%; Average loss: 0.1348\n",
            "Iteration: 25145; Percent complete: 83.8%; Average loss: 0.1295\n",
            "Iteration: 25146; Percent complete: 83.8%; Average loss: 0.0766\n",
            "Iteration: 25147; Percent complete: 83.8%; Average loss: 0.1614\n",
            "Iteration: 25148; Percent complete: 83.8%; Average loss: 0.1404\n",
            "Iteration: 25149; Percent complete: 83.8%; Average loss: 0.1296\n",
            "Iteration: 25150; Percent complete: 83.8%; Average loss: 0.0903\n",
            "Iteration: 25151; Percent complete: 83.8%; Average loss: 0.1885\n",
            "Iteration: 25152; Percent complete: 83.8%; Average loss: 0.1448\n",
            "Iteration: 25153; Percent complete: 83.8%; Average loss: 0.1239\n",
            "Iteration: 25154; Percent complete: 83.8%; Average loss: 0.1189\n",
            "Iteration: 25155; Percent complete: 83.9%; Average loss: 0.1517\n",
            "Iteration: 25156; Percent complete: 83.9%; Average loss: 0.1653\n",
            "Iteration: 25157; Percent complete: 83.9%; Average loss: 0.1029\n",
            "Iteration: 25158; Percent complete: 83.9%; Average loss: 0.1327\n",
            "Iteration: 25159; Percent complete: 83.9%; Average loss: 0.1210\n",
            "Iteration: 25160; Percent complete: 83.9%; Average loss: 0.2203\n",
            "Iteration: 25161; Percent complete: 83.9%; Average loss: 0.1039\n",
            "Iteration: 25162; Percent complete: 83.9%; Average loss: 0.1281\n",
            "Iteration: 25163; Percent complete: 83.9%; Average loss: 0.1367\n",
            "Iteration: 25164; Percent complete: 83.9%; Average loss: 0.1003\n",
            "Iteration: 25165; Percent complete: 83.9%; Average loss: 0.1388\n",
            "Iteration: 25166; Percent complete: 83.9%; Average loss: 0.1128\n",
            "Iteration: 25167; Percent complete: 83.9%; Average loss: 0.1753\n",
            "Iteration: 25168; Percent complete: 83.9%; Average loss: 0.1041\n",
            "Iteration: 25169; Percent complete: 83.9%; Average loss: 0.1470\n",
            "Iteration: 25170; Percent complete: 83.9%; Average loss: 0.1159\n",
            "Iteration: 25171; Percent complete: 83.9%; Average loss: 0.1153\n",
            "Iteration: 25172; Percent complete: 83.9%; Average loss: 0.0996\n",
            "Iteration: 25173; Percent complete: 83.9%; Average loss: 0.0977\n",
            "Iteration: 25174; Percent complete: 83.9%; Average loss: 0.1310\n",
            "Iteration: 25175; Percent complete: 83.9%; Average loss: 0.1064\n",
            "Iteration: 25176; Percent complete: 83.9%; Average loss: 0.1339\n",
            "Iteration: 25177; Percent complete: 83.9%; Average loss: 0.1128\n",
            "Iteration: 25178; Percent complete: 83.9%; Average loss: 0.1145\n",
            "Iteration: 25179; Percent complete: 83.9%; Average loss: 0.1130\n",
            "Iteration: 25180; Percent complete: 83.9%; Average loss: 0.0801\n",
            "Iteration: 25181; Percent complete: 83.9%; Average loss: 0.1769\n",
            "Iteration: 25182; Percent complete: 83.9%; Average loss: 0.1431\n",
            "Iteration: 25183; Percent complete: 83.9%; Average loss: 0.0841\n",
            "Iteration: 25184; Percent complete: 83.9%; Average loss: 0.1286\n",
            "Iteration: 25185; Percent complete: 84.0%; Average loss: 0.1740\n",
            "Iteration: 25186; Percent complete: 84.0%; Average loss: 0.1353\n",
            "Iteration: 25187; Percent complete: 84.0%; Average loss: 0.1376\n",
            "Iteration: 25188; Percent complete: 84.0%; Average loss: 0.1498\n",
            "Iteration: 25189; Percent complete: 84.0%; Average loss: 0.1988\n",
            "Iteration: 25190; Percent complete: 84.0%; Average loss: 0.1056\n",
            "Iteration: 25191; Percent complete: 84.0%; Average loss: 0.1348\n",
            "Iteration: 25192; Percent complete: 84.0%; Average loss: 0.1086\n",
            "Iteration: 25193; Percent complete: 84.0%; Average loss: 0.1516\n",
            "Iteration: 25194; Percent complete: 84.0%; Average loss: 0.1272\n",
            "Iteration: 25195; Percent complete: 84.0%; Average loss: 0.1440\n",
            "Iteration: 25196; Percent complete: 84.0%; Average loss: 0.1479\n",
            "Iteration: 25197; Percent complete: 84.0%; Average loss: 0.1268\n",
            "Iteration: 25198; Percent complete: 84.0%; Average loss: 0.1749\n",
            "Iteration: 25199; Percent complete: 84.0%; Average loss: 0.1003\n",
            "Iteration: 25200; Percent complete: 84.0%; Average loss: 0.1163\n",
            "Iteration: 25201; Percent complete: 84.0%; Average loss: 0.1371\n",
            "Iteration: 25202; Percent complete: 84.0%; Average loss: 0.1710\n",
            "Iteration: 25203; Percent complete: 84.0%; Average loss: 0.1817\n",
            "Iteration: 25204; Percent complete: 84.0%; Average loss: 0.1122\n",
            "Iteration: 25205; Percent complete: 84.0%; Average loss: 0.1505\n",
            "Iteration: 25206; Percent complete: 84.0%; Average loss: 0.1860\n",
            "Iteration: 25207; Percent complete: 84.0%; Average loss: 0.1385\n",
            "Iteration: 25208; Percent complete: 84.0%; Average loss: 0.1440\n",
            "Iteration: 25209; Percent complete: 84.0%; Average loss: 0.0852\n",
            "Iteration: 25210; Percent complete: 84.0%; Average loss: 0.1034\n",
            "Iteration: 25211; Percent complete: 84.0%; Average loss: 0.1407\n",
            "Iteration: 25212; Percent complete: 84.0%; Average loss: 0.1180\n",
            "Iteration: 25213; Percent complete: 84.0%; Average loss: 0.1499\n",
            "Iteration: 25214; Percent complete: 84.0%; Average loss: 0.0967\n",
            "Iteration: 25215; Percent complete: 84.0%; Average loss: 0.0874\n",
            "Iteration: 25216; Percent complete: 84.1%; Average loss: 0.1197\n",
            "Iteration: 25217; Percent complete: 84.1%; Average loss: 0.1305\n",
            "Iteration: 25218; Percent complete: 84.1%; Average loss: 0.1635\n",
            "Iteration: 25219; Percent complete: 84.1%; Average loss: 0.1236\n",
            "Iteration: 25220; Percent complete: 84.1%; Average loss: 0.1004\n",
            "Iteration: 25221; Percent complete: 84.1%; Average loss: 0.1084\n",
            "Iteration: 25222; Percent complete: 84.1%; Average loss: 0.1595\n",
            "Iteration: 25223; Percent complete: 84.1%; Average loss: 0.1395\n",
            "Iteration: 25224; Percent complete: 84.1%; Average loss: 0.1073\n",
            "Iteration: 25225; Percent complete: 84.1%; Average loss: 0.1090\n",
            "Iteration: 25226; Percent complete: 84.1%; Average loss: 0.1039\n",
            "Iteration: 25227; Percent complete: 84.1%; Average loss: 0.1292\n",
            "Iteration: 25228; Percent complete: 84.1%; Average loss: 0.1013\n",
            "Iteration: 25229; Percent complete: 84.1%; Average loss: 0.1028\n",
            "Iteration: 25230; Percent complete: 84.1%; Average loss: 0.1309\n",
            "Iteration: 25231; Percent complete: 84.1%; Average loss: 0.0697\n",
            "Iteration: 25232; Percent complete: 84.1%; Average loss: 0.1365\n",
            "Iteration: 25233; Percent complete: 84.1%; Average loss: 0.1193\n",
            "Iteration: 25234; Percent complete: 84.1%; Average loss: 0.1510\n",
            "Iteration: 25235; Percent complete: 84.1%; Average loss: 0.0975\n",
            "Iteration: 25236; Percent complete: 84.1%; Average loss: 0.1287\n",
            "Iteration: 25237; Percent complete: 84.1%; Average loss: 0.1075\n",
            "Iteration: 25238; Percent complete: 84.1%; Average loss: 0.1298\n",
            "Iteration: 25239; Percent complete: 84.1%; Average loss: 0.1228\n",
            "Iteration: 25240; Percent complete: 84.1%; Average loss: 0.1328\n",
            "Iteration: 25241; Percent complete: 84.1%; Average loss: 0.1760\n",
            "Iteration: 25242; Percent complete: 84.1%; Average loss: 0.1195\n",
            "Iteration: 25243; Percent complete: 84.1%; Average loss: 0.1129\n",
            "Iteration: 25244; Percent complete: 84.1%; Average loss: 0.1368\n",
            "Iteration: 25245; Percent complete: 84.2%; Average loss: 0.1121\n",
            "Iteration: 25246; Percent complete: 84.2%; Average loss: 0.1376\n",
            "Iteration: 25247; Percent complete: 84.2%; Average loss: 0.1161\n",
            "Iteration: 25248; Percent complete: 84.2%; Average loss: 0.1113\n",
            "Iteration: 25249; Percent complete: 84.2%; Average loss: 0.1278\n",
            "Iteration: 25250; Percent complete: 84.2%; Average loss: 0.1538\n",
            "Iteration: 25251; Percent complete: 84.2%; Average loss: 0.1752\n",
            "Iteration: 25252; Percent complete: 84.2%; Average loss: 0.1076\n",
            "Iteration: 25253; Percent complete: 84.2%; Average loss: 0.1263\n",
            "Iteration: 25254; Percent complete: 84.2%; Average loss: 0.0912\n",
            "Iteration: 25255; Percent complete: 84.2%; Average loss: 0.1000\n",
            "Iteration: 25256; Percent complete: 84.2%; Average loss: 0.1116\n",
            "Iteration: 25257; Percent complete: 84.2%; Average loss: 0.1346\n",
            "Iteration: 25258; Percent complete: 84.2%; Average loss: 0.1278\n",
            "Iteration: 25259; Percent complete: 84.2%; Average loss: 0.0910\n",
            "Iteration: 25260; Percent complete: 84.2%; Average loss: 0.0899\n",
            "Iteration: 25261; Percent complete: 84.2%; Average loss: 0.1435\n",
            "Iteration: 25262; Percent complete: 84.2%; Average loss: 0.1443\n",
            "Iteration: 25263; Percent complete: 84.2%; Average loss: 0.0733\n",
            "Iteration: 25264; Percent complete: 84.2%; Average loss: 0.1277\n",
            "Iteration: 25265; Percent complete: 84.2%; Average loss: 0.1336\n",
            "Iteration: 25266; Percent complete: 84.2%; Average loss: 0.1188\n",
            "Iteration: 25267; Percent complete: 84.2%; Average loss: 0.1073\n",
            "Iteration: 25268; Percent complete: 84.2%; Average loss: 0.1210\n",
            "Iteration: 25269; Percent complete: 84.2%; Average loss: 0.1206\n",
            "Iteration: 25270; Percent complete: 84.2%; Average loss: 0.1451\n",
            "Iteration: 25271; Percent complete: 84.2%; Average loss: 0.1328\n",
            "Iteration: 25272; Percent complete: 84.2%; Average loss: 0.1338\n",
            "Iteration: 25273; Percent complete: 84.2%; Average loss: 0.1361\n",
            "Iteration: 25274; Percent complete: 84.2%; Average loss: 0.1924\n",
            "Iteration: 25275; Percent complete: 84.2%; Average loss: 0.1121\n",
            "Iteration: 25276; Percent complete: 84.3%; Average loss: 0.1053\n",
            "Iteration: 25277; Percent complete: 84.3%; Average loss: 0.1247\n",
            "Iteration: 25278; Percent complete: 84.3%; Average loss: 0.0597\n",
            "Iteration: 25279; Percent complete: 84.3%; Average loss: 0.1190\n",
            "Iteration: 25280; Percent complete: 84.3%; Average loss: 0.2046\n",
            "Iteration: 25281; Percent complete: 84.3%; Average loss: 0.1180\n",
            "Iteration: 25282; Percent complete: 84.3%; Average loss: 0.1254\n",
            "Iteration: 25283; Percent complete: 84.3%; Average loss: 0.1383\n",
            "Iteration: 25284; Percent complete: 84.3%; Average loss: 0.1414\n",
            "Iteration: 25285; Percent complete: 84.3%; Average loss: 0.0969\n",
            "Iteration: 25286; Percent complete: 84.3%; Average loss: 0.1008\n",
            "Iteration: 25287; Percent complete: 84.3%; Average loss: 0.1228\n",
            "Iteration: 25288; Percent complete: 84.3%; Average loss: 0.0947\n",
            "Iteration: 25289; Percent complete: 84.3%; Average loss: 0.1230\n",
            "Iteration: 25290; Percent complete: 84.3%; Average loss: 0.1199\n",
            "Iteration: 25291; Percent complete: 84.3%; Average loss: 0.1629\n",
            "Iteration: 25292; Percent complete: 84.3%; Average loss: 0.1372\n",
            "Iteration: 25293; Percent complete: 84.3%; Average loss: 0.1336\n",
            "Iteration: 25294; Percent complete: 84.3%; Average loss: 0.1159\n",
            "Iteration: 25295; Percent complete: 84.3%; Average loss: 0.1361\n",
            "Iteration: 25296; Percent complete: 84.3%; Average loss: 0.1654\n",
            "Iteration: 25297; Percent complete: 84.3%; Average loss: 0.1000\n",
            "Iteration: 25298; Percent complete: 84.3%; Average loss: 0.1191\n",
            "Iteration: 25299; Percent complete: 84.3%; Average loss: 0.1487\n",
            "Iteration: 25300; Percent complete: 84.3%; Average loss: 0.1259\n",
            "Iteration: 25301; Percent complete: 84.3%; Average loss: 0.1777\n",
            "Iteration: 25302; Percent complete: 84.3%; Average loss: 0.0798\n",
            "Iteration: 25303; Percent complete: 84.3%; Average loss: 0.0980\n",
            "Iteration: 25304; Percent complete: 84.3%; Average loss: 0.1377\n",
            "Iteration: 25305; Percent complete: 84.4%; Average loss: 0.1120\n",
            "Iteration: 25306; Percent complete: 84.4%; Average loss: 0.1613\n",
            "Iteration: 25307; Percent complete: 84.4%; Average loss: 0.1143\n",
            "Iteration: 25308; Percent complete: 84.4%; Average loss: 0.1345\n",
            "Iteration: 25309; Percent complete: 84.4%; Average loss: 0.1714\n",
            "Iteration: 25310; Percent complete: 84.4%; Average loss: 0.1311\n",
            "Iteration: 25311; Percent complete: 84.4%; Average loss: 0.1215\n",
            "Iteration: 25312; Percent complete: 84.4%; Average loss: 0.1261\n",
            "Iteration: 25313; Percent complete: 84.4%; Average loss: 0.1276\n",
            "Iteration: 25314; Percent complete: 84.4%; Average loss: 0.0990\n",
            "Iteration: 25315; Percent complete: 84.4%; Average loss: 0.1984\n",
            "Iteration: 25316; Percent complete: 84.4%; Average loss: 0.1399\n",
            "Iteration: 25317; Percent complete: 84.4%; Average loss: 0.1320\n",
            "Iteration: 25318; Percent complete: 84.4%; Average loss: 0.1525\n",
            "Iteration: 25319; Percent complete: 84.4%; Average loss: 0.1575\n",
            "Iteration: 25320; Percent complete: 84.4%; Average loss: 0.0925\n",
            "Iteration: 25321; Percent complete: 84.4%; Average loss: 0.1588\n",
            "Iteration: 25322; Percent complete: 84.4%; Average loss: 0.1117\n",
            "Iteration: 25323; Percent complete: 84.4%; Average loss: 0.1258\n",
            "Iteration: 25324; Percent complete: 84.4%; Average loss: 0.1125\n",
            "Iteration: 25325; Percent complete: 84.4%; Average loss: 0.0923\n",
            "Iteration: 25326; Percent complete: 84.4%; Average loss: 0.1125\n",
            "Iteration: 25327; Percent complete: 84.4%; Average loss: 0.0971\n",
            "Iteration: 25328; Percent complete: 84.4%; Average loss: 0.0908\n",
            "Iteration: 25329; Percent complete: 84.4%; Average loss: 0.1113\n",
            "Iteration: 25330; Percent complete: 84.4%; Average loss: 0.1263\n",
            "Iteration: 25331; Percent complete: 84.4%; Average loss: 0.1044\n",
            "Iteration: 25332; Percent complete: 84.4%; Average loss: 0.1047\n",
            "Iteration: 25333; Percent complete: 84.4%; Average loss: 0.1240\n",
            "Iteration: 25334; Percent complete: 84.4%; Average loss: 0.1588\n",
            "Iteration: 25335; Percent complete: 84.5%; Average loss: 0.1303\n",
            "Iteration: 25336; Percent complete: 84.5%; Average loss: 0.1369\n",
            "Iteration: 25337; Percent complete: 84.5%; Average loss: 0.1121\n",
            "Iteration: 25338; Percent complete: 84.5%; Average loss: 0.1387\n",
            "Iteration: 25339; Percent complete: 84.5%; Average loss: 0.1387\n",
            "Iteration: 25340; Percent complete: 84.5%; Average loss: 0.1524\n",
            "Iteration: 25341; Percent complete: 84.5%; Average loss: 0.1508\n",
            "Iteration: 25342; Percent complete: 84.5%; Average loss: 0.1053\n",
            "Iteration: 25343; Percent complete: 84.5%; Average loss: 0.1411\n",
            "Iteration: 25344; Percent complete: 84.5%; Average loss: 0.1152\n",
            "Iteration: 25345; Percent complete: 84.5%; Average loss: 0.1517\n",
            "Iteration: 25346; Percent complete: 84.5%; Average loss: 0.1325\n",
            "Iteration: 25347; Percent complete: 84.5%; Average loss: 0.1808\n",
            "Iteration: 25348; Percent complete: 84.5%; Average loss: 0.0966\n",
            "Iteration: 25349; Percent complete: 84.5%; Average loss: 0.1226\n",
            "Iteration: 25350; Percent complete: 84.5%; Average loss: 0.1398\n",
            "Iteration: 25351; Percent complete: 84.5%; Average loss: 0.1679\n",
            "Iteration: 25352; Percent complete: 84.5%; Average loss: 0.1341\n",
            "Iteration: 25353; Percent complete: 84.5%; Average loss: 0.1099\n",
            "Iteration: 25354; Percent complete: 84.5%; Average loss: 0.1126\n",
            "Iteration: 25355; Percent complete: 84.5%; Average loss: 0.0805\n",
            "Iteration: 25356; Percent complete: 84.5%; Average loss: 0.1117\n",
            "Iteration: 25357; Percent complete: 84.5%; Average loss: 0.1391\n",
            "Iteration: 25358; Percent complete: 84.5%; Average loss: 0.1708\n",
            "Iteration: 25359; Percent complete: 84.5%; Average loss: 0.0883\n",
            "Iteration: 25360; Percent complete: 84.5%; Average loss: 0.1092\n",
            "Iteration: 25361; Percent complete: 84.5%; Average loss: 0.1505\n",
            "Iteration: 25362; Percent complete: 84.5%; Average loss: 0.1278\n",
            "Iteration: 25363; Percent complete: 84.5%; Average loss: 0.1234\n",
            "Iteration: 25364; Percent complete: 84.5%; Average loss: 0.1467\n",
            "Iteration: 25365; Percent complete: 84.5%; Average loss: 0.1337\n",
            "Iteration: 25366; Percent complete: 84.6%; Average loss: 0.1013\n",
            "Iteration: 25367; Percent complete: 84.6%; Average loss: 0.1258\n",
            "Iteration: 25368; Percent complete: 84.6%; Average loss: 0.1232\n",
            "Iteration: 25369; Percent complete: 84.6%; Average loss: 0.1010\n",
            "Iteration: 25370; Percent complete: 84.6%; Average loss: 0.0827\n",
            "Iteration: 25371; Percent complete: 84.6%; Average loss: 0.1108\n",
            "Iteration: 25372; Percent complete: 84.6%; Average loss: 0.1336\n",
            "Iteration: 25373; Percent complete: 84.6%; Average loss: 0.1328\n",
            "Iteration: 25374; Percent complete: 84.6%; Average loss: 0.1641\n",
            "Iteration: 25375; Percent complete: 84.6%; Average loss: 0.1670\n",
            "Iteration: 25376; Percent complete: 84.6%; Average loss: 0.1036\n",
            "Iteration: 25377; Percent complete: 84.6%; Average loss: 0.0956\n",
            "Iteration: 25378; Percent complete: 84.6%; Average loss: 0.1364\n",
            "Iteration: 25379; Percent complete: 84.6%; Average loss: 0.1277\n",
            "Iteration: 25380; Percent complete: 84.6%; Average loss: 0.1374\n",
            "Iteration: 25381; Percent complete: 84.6%; Average loss: 0.1535\n",
            "Iteration: 25382; Percent complete: 84.6%; Average loss: 0.0907\n",
            "Iteration: 25383; Percent complete: 84.6%; Average loss: 0.1025\n",
            "Iteration: 25384; Percent complete: 84.6%; Average loss: 0.1302\n",
            "Iteration: 25385; Percent complete: 84.6%; Average loss: 0.1359\n",
            "Iteration: 25386; Percent complete: 84.6%; Average loss: 0.1735\n",
            "Iteration: 25387; Percent complete: 84.6%; Average loss: 0.0985\n",
            "Iteration: 25388; Percent complete: 84.6%; Average loss: 0.1477\n",
            "Iteration: 25389; Percent complete: 84.6%; Average loss: 0.1466\n",
            "Iteration: 25390; Percent complete: 84.6%; Average loss: 0.1411\n",
            "Iteration: 25391; Percent complete: 84.6%; Average loss: 0.1197\n",
            "Iteration: 25392; Percent complete: 84.6%; Average loss: 0.1314\n",
            "Iteration: 25393; Percent complete: 84.6%; Average loss: 0.1295\n",
            "Iteration: 25394; Percent complete: 84.6%; Average loss: 0.0949\n",
            "Iteration: 25395; Percent complete: 84.7%; Average loss: 0.1211\n",
            "Iteration: 25396; Percent complete: 84.7%; Average loss: 0.1150\n",
            "Iteration: 25397; Percent complete: 84.7%; Average loss: 0.0789\n",
            "Iteration: 25398; Percent complete: 84.7%; Average loss: 0.1402\n",
            "Iteration: 25399; Percent complete: 84.7%; Average loss: 0.1366\n",
            "Iteration: 25400; Percent complete: 84.7%; Average loss: 0.1534\n",
            "Iteration: 25401; Percent complete: 84.7%; Average loss: 0.1003\n",
            "Iteration: 25402; Percent complete: 84.7%; Average loss: 0.1095\n",
            "Iteration: 25403; Percent complete: 84.7%; Average loss: 0.0564\n",
            "Iteration: 25404; Percent complete: 84.7%; Average loss: 0.1268\n",
            "Iteration: 25405; Percent complete: 84.7%; Average loss: 0.1094\n",
            "Iteration: 25406; Percent complete: 84.7%; Average loss: 0.1583\n",
            "Iteration: 25407; Percent complete: 84.7%; Average loss: 0.1636\n",
            "Iteration: 25408; Percent complete: 84.7%; Average loss: 0.0924\n",
            "Iteration: 25409; Percent complete: 84.7%; Average loss: 0.1326\n",
            "Iteration: 25410; Percent complete: 84.7%; Average loss: 0.0941\n",
            "Iteration: 25411; Percent complete: 84.7%; Average loss: 0.1597\n",
            "Iteration: 25412; Percent complete: 84.7%; Average loss: 0.0921\n",
            "Iteration: 25413; Percent complete: 84.7%; Average loss: 0.0880\n",
            "Iteration: 25414; Percent complete: 84.7%; Average loss: 0.1316\n",
            "Iteration: 25415; Percent complete: 84.7%; Average loss: 0.1086\n",
            "Iteration: 25416; Percent complete: 84.7%; Average loss: 0.0994\n",
            "Iteration: 25417; Percent complete: 84.7%; Average loss: 0.1276\n",
            "Iteration: 25418; Percent complete: 84.7%; Average loss: 0.1379\n",
            "Iteration: 25419; Percent complete: 84.7%; Average loss: 0.1454\n",
            "Iteration: 25420; Percent complete: 84.7%; Average loss: 0.0915\n",
            "Iteration: 25421; Percent complete: 84.7%; Average loss: 0.1533\n",
            "Iteration: 25422; Percent complete: 84.7%; Average loss: 0.1213\n",
            "Iteration: 25423; Percent complete: 84.7%; Average loss: 0.1357\n",
            "Iteration: 25424; Percent complete: 84.7%; Average loss: 0.1187\n",
            "Iteration: 25425; Percent complete: 84.8%; Average loss: 0.1072\n",
            "Iteration: 25426; Percent complete: 84.8%; Average loss: 0.1505\n",
            "Iteration: 25427; Percent complete: 84.8%; Average loss: 0.0889\n",
            "Iteration: 25428; Percent complete: 84.8%; Average loss: 0.1758\n",
            "Iteration: 25429; Percent complete: 84.8%; Average loss: 0.1267\n",
            "Iteration: 25430; Percent complete: 84.8%; Average loss: 0.1396\n",
            "Iteration: 25431; Percent complete: 84.8%; Average loss: 0.0971\n",
            "Iteration: 25432; Percent complete: 84.8%; Average loss: 0.1460\n",
            "Iteration: 25433; Percent complete: 84.8%; Average loss: 0.1377\n",
            "Iteration: 25434; Percent complete: 84.8%; Average loss: 0.0913\n",
            "Iteration: 25435; Percent complete: 84.8%; Average loss: 0.0951\n",
            "Iteration: 25436; Percent complete: 84.8%; Average loss: 0.1288\n",
            "Iteration: 25437; Percent complete: 84.8%; Average loss: 0.1600\n",
            "Iteration: 25438; Percent complete: 84.8%; Average loss: 0.1796\n",
            "Iteration: 25439; Percent complete: 84.8%; Average loss: 0.1452\n",
            "Iteration: 25440; Percent complete: 84.8%; Average loss: 0.1159\n",
            "Iteration: 25441; Percent complete: 84.8%; Average loss: 0.1155\n",
            "Iteration: 25442; Percent complete: 84.8%; Average loss: 0.1095\n",
            "Iteration: 25443; Percent complete: 84.8%; Average loss: 0.1196\n",
            "Iteration: 25444; Percent complete: 84.8%; Average loss: 0.1175\n",
            "Iteration: 25445; Percent complete: 84.8%; Average loss: 0.1034\n",
            "Iteration: 25446; Percent complete: 84.8%; Average loss: 0.0932\n",
            "Iteration: 25447; Percent complete: 84.8%; Average loss: 0.1120\n",
            "Iteration: 25448; Percent complete: 84.8%; Average loss: 0.1157\n",
            "Iteration: 25449; Percent complete: 84.8%; Average loss: 0.1487\n",
            "Iteration: 25450; Percent complete: 84.8%; Average loss: 0.1242\n",
            "Iteration: 25451; Percent complete: 84.8%; Average loss: 0.1632\n",
            "Iteration: 25452; Percent complete: 84.8%; Average loss: 0.1229\n",
            "Iteration: 25453; Percent complete: 84.8%; Average loss: 0.1291\n",
            "Iteration: 25454; Percent complete: 84.8%; Average loss: 0.1026\n",
            "Iteration: 25455; Percent complete: 84.9%; Average loss: 0.1074\n",
            "Iteration: 25456; Percent complete: 84.9%; Average loss: 0.1184\n",
            "Iteration: 25457; Percent complete: 84.9%; Average loss: 0.2051\n",
            "Iteration: 25458; Percent complete: 84.9%; Average loss: 0.1877\n",
            "Iteration: 25459; Percent complete: 84.9%; Average loss: 0.1517\n",
            "Iteration: 25460; Percent complete: 84.9%; Average loss: 0.1247\n",
            "Iteration: 25461; Percent complete: 84.9%; Average loss: 0.1025\n",
            "Iteration: 25462; Percent complete: 84.9%; Average loss: 0.1080\n",
            "Iteration: 25463; Percent complete: 84.9%; Average loss: 0.1349\n",
            "Iteration: 25464; Percent complete: 84.9%; Average loss: 0.1260\n",
            "Iteration: 25465; Percent complete: 84.9%; Average loss: 0.1363\n",
            "Iteration: 25466; Percent complete: 84.9%; Average loss: 0.1265\n",
            "Iteration: 25467; Percent complete: 84.9%; Average loss: 0.1396\n",
            "Iteration: 25468; Percent complete: 84.9%; Average loss: 0.1197\n",
            "Iteration: 25469; Percent complete: 84.9%; Average loss: 0.1161\n",
            "Iteration: 25470; Percent complete: 84.9%; Average loss: 0.1363\n",
            "Iteration: 25471; Percent complete: 84.9%; Average loss: 0.1349\n",
            "Iteration: 25472; Percent complete: 84.9%; Average loss: 0.0787\n",
            "Iteration: 25473; Percent complete: 84.9%; Average loss: 0.1042\n",
            "Iteration: 25474; Percent complete: 84.9%; Average loss: 0.0747\n",
            "Iteration: 25475; Percent complete: 84.9%; Average loss: 0.1316\n",
            "Iteration: 25476; Percent complete: 84.9%; Average loss: 0.1368\n",
            "Iteration: 25477; Percent complete: 84.9%; Average loss: 0.0919\n",
            "Iteration: 25478; Percent complete: 84.9%; Average loss: 0.1181\n",
            "Iteration: 25479; Percent complete: 84.9%; Average loss: 0.1233\n",
            "Iteration: 25480; Percent complete: 84.9%; Average loss: 0.1469\n",
            "Iteration: 25481; Percent complete: 84.9%; Average loss: 0.1597\n",
            "Iteration: 25482; Percent complete: 84.9%; Average loss: 0.0934\n",
            "Iteration: 25483; Percent complete: 84.9%; Average loss: 0.1281\n",
            "Iteration: 25484; Percent complete: 84.9%; Average loss: 0.0726\n",
            "Iteration: 25485; Percent complete: 85.0%; Average loss: 0.1195\n",
            "Iteration: 25486; Percent complete: 85.0%; Average loss: 0.1705\n",
            "Iteration: 25487; Percent complete: 85.0%; Average loss: 0.1000\n",
            "Iteration: 25488; Percent complete: 85.0%; Average loss: 0.1373\n",
            "Iteration: 25489; Percent complete: 85.0%; Average loss: 0.1136\n",
            "Iteration: 25490; Percent complete: 85.0%; Average loss: 0.1184\n",
            "Iteration: 25491; Percent complete: 85.0%; Average loss: 0.0527\n",
            "Iteration: 25492; Percent complete: 85.0%; Average loss: 0.1317\n",
            "Iteration: 25493; Percent complete: 85.0%; Average loss: 0.1326\n",
            "Iteration: 25494; Percent complete: 85.0%; Average loss: 0.1534\n",
            "Iteration: 25495; Percent complete: 85.0%; Average loss: 0.1360\n",
            "Iteration: 25496; Percent complete: 85.0%; Average loss: 0.1156\n",
            "Iteration: 25497; Percent complete: 85.0%; Average loss: 0.1442\n",
            "Iteration: 25498; Percent complete: 85.0%; Average loss: 0.1407\n",
            "Iteration: 25499; Percent complete: 85.0%; Average loss: 0.1557\n",
            "Iteration: 25500; Percent complete: 85.0%; Average loss: 0.1356\n",
            "Iteration: 25501; Percent complete: 85.0%; Average loss: 0.1500\n",
            "Iteration: 25502; Percent complete: 85.0%; Average loss: 0.1252\n",
            "Iteration: 25503; Percent complete: 85.0%; Average loss: 0.1073\n",
            "Iteration: 25504; Percent complete: 85.0%; Average loss: 0.0734\n",
            "Iteration: 25505; Percent complete: 85.0%; Average loss: 0.1188\n",
            "Iteration: 25506; Percent complete: 85.0%; Average loss: 0.1019\n",
            "Iteration: 25507; Percent complete: 85.0%; Average loss: 0.1262\n",
            "Iteration: 25508; Percent complete: 85.0%; Average loss: 0.1409\n",
            "Iteration: 25509; Percent complete: 85.0%; Average loss: 0.1128\n",
            "Iteration: 25510; Percent complete: 85.0%; Average loss: 0.1193\n",
            "Iteration: 25511; Percent complete: 85.0%; Average loss: 0.0625\n",
            "Iteration: 25512; Percent complete: 85.0%; Average loss: 0.1214\n",
            "Iteration: 25513; Percent complete: 85.0%; Average loss: 0.1055\n",
            "Iteration: 25514; Percent complete: 85.0%; Average loss: 0.1686\n",
            "Iteration: 25515; Percent complete: 85.0%; Average loss: 0.1217\n",
            "Iteration: 25516; Percent complete: 85.1%; Average loss: 0.0990\n",
            "Iteration: 25517; Percent complete: 85.1%; Average loss: 0.1036\n",
            "Iteration: 25518; Percent complete: 85.1%; Average loss: 0.1687\n",
            "Iteration: 25519; Percent complete: 85.1%; Average loss: 0.1588\n",
            "Iteration: 25520; Percent complete: 85.1%; Average loss: 0.1620\n",
            "Iteration: 25521; Percent complete: 85.1%; Average loss: 0.2031\n",
            "Iteration: 25522; Percent complete: 85.1%; Average loss: 0.1432\n",
            "Iteration: 25523; Percent complete: 85.1%; Average loss: 0.1381\n",
            "Iteration: 25524; Percent complete: 85.1%; Average loss: 0.1760\n",
            "Iteration: 25525; Percent complete: 85.1%; Average loss: 0.1477\n",
            "Iteration: 25526; Percent complete: 85.1%; Average loss: 0.1226\n",
            "Iteration: 25527; Percent complete: 85.1%; Average loss: 0.1084\n",
            "Iteration: 25528; Percent complete: 85.1%; Average loss: 0.1472\n",
            "Iteration: 25529; Percent complete: 85.1%; Average loss: 0.1042\n",
            "Iteration: 25530; Percent complete: 85.1%; Average loss: 0.0910\n",
            "Iteration: 25531; Percent complete: 85.1%; Average loss: 0.1060\n",
            "Iteration: 25532; Percent complete: 85.1%; Average loss: 0.1353\n",
            "Iteration: 25533; Percent complete: 85.1%; Average loss: 0.1094\n",
            "Iteration: 25534; Percent complete: 85.1%; Average loss: 0.1026\n",
            "Iteration: 25535; Percent complete: 85.1%; Average loss: 0.1052\n",
            "Iteration: 25536; Percent complete: 85.1%; Average loss: 0.0979\n",
            "Iteration: 25537; Percent complete: 85.1%; Average loss: 0.1197\n",
            "Iteration: 25538; Percent complete: 85.1%; Average loss: 0.1699\n",
            "Iteration: 25539; Percent complete: 85.1%; Average loss: 0.0814\n",
            "Iteration: 25540; Percent complete: 85.1%; Average loss: 0.1000\n",
            "Iteration: 25541; Percent complete: 85.1%; Average loss: 0.1250\n",
            "Iteration: 25542; Percent complete: 85.1%; Average loss: 0.1309\n",
            "Iteration: 25543; Percent complete: 85.1%; Average loss: 0.1050\n",
            "Iteration: 25544; Percent complete: 85.1%; Average loss: 0.1336\n",
            "Iteration: 25545; Percent complete: 85.2%; Average loss: 0.1209\n",
            "Iteration: 25546; Percent complete: 85.2%; Average loss: 0.1563\n",
            "Iteration: 25547; Percent complete: 85.2%; Average loss: 0.1635\n",
            "Iteration: 25548; Percent complete: 85.2%; Average loss: 0.0989\n",
            "Iteration: 25549; Percent complete: 85.2%; Average loss: 0.1298\n",
            "Iteration: 25550; Percent complete: 85.2%; Average loss: 0.1018\n",
            "Iteration: 25551; Percent complete: 85.2%; Average loss: 0.1104\n",
            "Iteration: 25552; Percent complete: 85.2%; Average loss: 0.1123\n",
            "Iteration: 25553; Percent complete: 85.2%; Average loss: 0.0821\n",
            "Iteration: 25554; Percent complete: 85.2%; Average loss: 0.1243\n",
            "Iteration: 25555; Percent complete: 85.2%; Average loss: 0.1171\n",
            "Iteration: 25556; Percent complete: 85.2%; Average loss: 0.1219\n",
            "Iteration: 25557; Percent complete: 85.2%; Average loss: 0.0976\n",
            "Iteration: 25558; Percent complete: 85.2%; Average loss: 0.1064\n",
            "Iteration: 25559; Percent complete: 85.2%; Average loss: 0.1222\n",
            "Iteration: 25560; Percent complete: 85.2%; Average loss: 0.1387\n",
            "Iteration: 25561; Percent complete: 85.2%; Average loss: 0.0943\n",
            "Iteration: 25562; Percent complete: 85.2%; Average loss: 0.1644\n",
            "Iteration: 25563; Percent complete: 85.2%; Average loss: 0.1074\n",
            "Iteration: 25564; Percent complete: 85.2%; Average loss: 0.1288\n",
            "Iteration: 25565; Percent complete: 85.2%; Average loss: 0.1136\n",
            "Iteration: 25566; Percent complete: 85.2%; Average loss: 0.1587\n",
            "Iteration: 25567; Percent complete: 85.2%; Average loss: 0.0957\n",
            "Iteration: 25568; Percent complete: 85.2%; Average loss: 0.1368\n",
            "Iteration: 25569; Percent complete: 85.2%; Average loss: 0.1584\n",
            "Iteration: 25570; Percent complete: 85.2%; Average loss: 0.1405\n",
            "Iteration: 25571; Percent complete: 85.2%; Average loss: 0.1158\n",
            "Iteration: 25572; Percent complete: 85.2%; Average loss: 0.1439\n",
            "Iteration: 25573; Percent complete: 85.2%; Average loss: 0.1589\n",
            "Iteration: 25574; Percent complete: 85.2%; Average loss: 0.1096\n",
            "Iteration: 25575; Percent complete: 85.2%; Average loss: 0.1498\n",
            "Iteration: 25576; Percent complete: 85.3%; Average loss: 0.0986\n",
            "Iteration: 25577; Percent complete: 85.3%; Average loss: 0.1203\n",
            "Iteration: 25578; Percent complete: 85.3%; Average loss: 0.0950\n",
            "Iteration: 25579; Percent complete: 85.3%; Average loss: 0.1236\n",
            "Iteration: 25580; Percent complete: 85.3%; Average loss: 0.1519\n",
            "Iteration: 25581; Percent complete: 85.3%; Average loss: 0.1079\n",
            "Iteration: 25582; Percent complete: 85.3%; Average loss: 0.1652\n",
            "Iteration: 25583; Percent complete: 85.3%; Average loss: 0.1435\n",
            "Iteration: 25584; Percent complete: 85.3%; Average loss: 0.1226\n",
            "Iteration: 25585; Percent complete: 85.3%; Average loss: 0.1503\n",
            "Iteration: 25586; Percent complete: 85.3%; Average loss: 0.1361\n",
            "Iteration: 25587; Percent complete: 85.3%; Average loss: 0.1025\n",
            "Iteration: 25588; Percent complete: 85.3%; Average loss: 0.0899\n",
            "Iteration: 25589; Percent complete: 85.3%; Average loss: 0.1063\n",
            "Iteration: 25590; Percent complete: 85.3%; Average loss: 0.0969\n",
            "Iteration: 25591; Percent complete: 85.3%; Average loss: 0.1071\n",
            "Iteration: 25592; Percent complete: 85.3%; Average loss: 0.1823\n",
            "Iteration: 25593; Percent complete: 85.3%; Average loss: 0.1667\n",
            "Iteration: 25594; Percent complete: 85.3%; Average loss: 0.1299\n",
            "Iteration: 25595; Percent complete: 85.3%; Average loss: 0.1081\n",
            "Iteration: 25596; Percent complete: 85.3%; Average loss: 0.1037\n",
            "Iteration: 25597; Percent complete: 85.3%; Average loss: 0.1224\n",
            "Iteration: 25598; Percent complete: 85.3%; Average loss: 0.0986\n",
            "Iteration: 25599; Percent complete: 85.3%; Average loss: 0.0994\n",
            "Iteration: 25600; Percent complete: 85.3%; Average loss: 0.1804\n",
            "Iteration: 25601; Percent complete: 85.3%; Average loss: 0.1172\n",
            "Iteration: 25602; Percent complete: 85.3%; Average loss: 0.1168\n",
            "Iteration: 25603; Percent complete: 85.3%; Average loss: 0.1407\n",
            "Iteration: 25604; Percent complete: 85.3%; Average loss: 0.1149\n",
            "Iteration: 25605; Percent complete: 85.4%; Average loss: 0.1142\n",
            "Iteration: 25606; Percent complete: 85.4%; Average loss: 0.1185\n",
            "Iteration: 25607; Percent complete: 85.4%; Average loss: 0.1101\n",
            "Iteration: 25608; Percent complete: 85.4%; Average loss: 0.1216\n",
            "Iteration: 25609; Percent complete: 85.4%; Average loss: 0.1080\n",
            "Iteration: 25610; Percent complete: 85.4%; Average loss: 0.1411\n",
            "Iteration: 25611; Percent complete: 85.4%; Average loss: 0.1238\n",
            "Iteration: 25612; Percent complete: 85.4%; Average loss: 0.1802\n",
            "Iteration: 25613; Percent complete: 85.4%; Average loss: 0.1251\n",
            "Iteration: 25614; Percent complete: 85.4%; Average loss: 0.0817\n",
            "Iteration: 25615; Percent complete: 85.4%; Average loss: 0.0994\n",
            "Iteration: 25616; Percent complete: 85.4%; Average loss: 0.1074\n",
            "Iteration: 25617; Percent complete: 85.4%; Average loss: 0.1441\n",
            "Iteration: 25618; Percent complete: 85.4%; Average loss: 0.1564\n",
            "Iteration: 25619; Percent complete: 85.4%; Average loss: 0.0959\n",
            "Iteration: 25620; Percent complete: 85.4%; Average loss: 0.1007\n",
            "Iteration: 25621; Percent complete: 85.4%; Average loss: 0.1440\n",
            "Iteration: 25622; Percent complete: 85.4%; Average loss: 0.1030\n",
            "Iteration: 25623; Percent complete: 85.4%; Average loss: 0.1209\n",
            "Iteration: 25624; Percent complete: 85.4%; Average loss: 0.0814\n",
            "Iteration: 25625; Percent complete: 85.4%; Average loss: 0.1389\n",
            "Iteration: 25626; Percent complete: 85.4%; Average loss: 0.1472\n",
            "Iteration: 25627; Percent complete: 85.4%; Average loss: 0.1248\n",
            "Iteration: 25628; Percent complete: 85.4%; Average loss: 0.1459\n",
            "Iteration: 25629; Percent complete: 85.4%; Average loss: 0.1693\n",
            "Iteration: 25630; Percent complete: 85.4%; Average loss: 0.1609\n",
            "Iteration: 25631; Percent complete: 85.4%; Average loss: 0.1488\n",
            "Iteration: 25632; Percent complete: 85.4%; Average loss: 0.1286\n",
            "Iteration: 25633; Percent complete: 85.4%; Average loss: 0.0930\n",
            "Iteration: 25634; Percent complete: 85.4%; Average loss: 0.1175\n",
            "Iteration: 25635; Percent complete: 85.5%; Average loss: 0.1024\n",
            "Iteration: 25636; Percent complete: 85.5%; Average loss: 0.1410\n",
            "Iteration: 25637; Percent complete: 85.5%; Average loss: 0.1118\n",
            "Iteration: 25638; Percent complete: 85.5%; Average loss: 0.1029\n",
            "Iteration: 25639; Percent complete: 85.5%; Average loss: 0.0995\n",
            "Iteration: 25640; Percent complete: 85.5%; Average loss: 0.1537\n",
            "Iteration: 25641; Percent complete: 85.5%; Average loss: 0.1281\n",
            "Iteration: 25642; Percent complete: 85.5%; Average loss: 0.1053\n",
            "Iteration: 25643; Percent complete: 85.5%; Average loss: 0.1166\n",
            "Iteration: 25644; Percent complete: 85.5%; Average loss: 0.0899\n",
            "Iteration: 25645; Percent complete: 85.5%; Average loss: 0.1147\n",
            "Iteration: 25646; Percent complete: 85.5%; Average loss: 0.1007\n",
            "Iteration: 25647; Percent complete: 85.5%; Average loss: 0.1186\n",
            "Iteration: 25648; Percent complete: 85.5%; Average loss: 0.1896\n",
            "Iteration: 25649; Percent complete: 85.5%; Average loss: 0.1139\n",
            "Iteration: 25650; Percent complete: 85.5%; Average loss: 0.1176\n",
            "Iteration: 25651; Percent complete: 85.5%; Average loss: 0.1429\n",
            "Iteration: 25652; Percent complete: 85.5%; Average loss: 0.1234\n",
            "Iteration: 25653; Percent complete: 85.5%; Average loss: 0.1782\n",
            "Iteration: 25654; Percent complete: 85.5%; Average loss: 0.0961\n",
            "Iteration: 25655; Percent complete: 85.5%; Average loss: 0.1050\n",
            "Iteration: 25656; Percent complete: 85.5%; Average loss: 0.1129\n",
            "Iteration: 25657; Percent complete: 85.5%; Average loss: 0.1192\n",
            "Iteration: 25658; Percent complete: 85.5%; Average loss: 0.0900\n",
            "Iteration: 25659; Percent complete: 85.5%; Average loss: 0.1529\n",
            "Iteration: 25660; Percent complete: 85.5%; Average loss: 0.0935\n",
            "Iteration: 25661; Percent complete: 85.5%; Average loss: 0.0943\n",
            "Iteration: 25662; Percent complete: 85.5%; Average loss: 0.1219\n",
            "Iteration: 25663; Percent complete: 85.5%; Average loss: 0.1293\n",
            "Iteration: 25664; Percent complete: 85.5%; Average loss: 0.1423\n",
            "Iteration: 25665; Percent complete: 85.5%; Average loss: 0.1424\n",
            "Iteration: 25666; Percent complete: 85.6%; Average loss: 0.1189\n",
            "Iteration: 25667; Percent complete: 85.6%; Average loss: 0.1025\n",
            "Iteration: 25668; Percent complete: 85.6%; Average loss: 0.1327\n",
            "Iteration: 25669; Percent complete: 85.6%; Average loss: 0.1213\n",
            "Iteration: 25670; Percent complete: 85.6%; Average loss: 0.1751\n",
            "Iteration: 25671; Percent complete: 85.6%; Average loss: 0.1653\n",
            "Iteration: 25672; Percent complete: 85.6%; Average loss: 0.1287\n",
            "Iteration: 25673; Percent complete: 85.6%; Average loss: 0.1534\n",
            "Iteration: 25674; Percent complete: 85.6%; Average loss: 0.1400\n",
            "Iteration: 25675; Percent complete: 85.6%; Average loss: 0.1144\n",
            "Iteration: 25676; Percent complete: 85.6%; Average loss: 0.1171\n",
            "Iteration: 25677; Percent complete: 85.6%; Average loss: 0.1582\n",
            "Iteration: 25678; Percent complete: 85.6%; Average loss: 0.1188\n",
            "Iteration: 25679; Percent complete: 85.6%; Average loss: 0.1601\n",
            "Iteration: 25680; Percent complete: 85.6%; Average loss: 0.1459\n",
            "Iteration: 25681; Percent complete: 85.6%; Average loss: 0.0941\n",
            "Iteration: 25682; Percent complete: 85.6%; Average loss: 0.1130\n",
            "Iteration: 25683; Percent complete: 85.6%; Average loss: 0.0836\n",
            "Iteration: 25684; Percent complete: 85.6%; Average loss: 0.1748\n",
            "Iteration: 25685; Percent complete: 85.6%; Average loss: 0.0921\n",
            "Iteration: 25686; Percent complete: 85.6%; Average loss: 0.1259\n",
            "Iteration: 25687; Percent complete: 85.6%; Average loss: 0.1377\n",
            "Iteration: 25688; Percent complete: 85.6%; Average loss: 0.1093\n",
            "Iteration: 25689; Percent complete: 85.6%; Average loss: 0.0770\n",
            "Iteration: 25690; Percent complete: 85.6%; Average loss: 0.1065\n",
            "Iteration: 25691; Percent complete: 85.6%; Average loss: 0.1001\n",
            "Iteration: 25692; Percent complete: 85.6%; Average loss: 0.1348\n",
            "Iteration: 25693; Percent complete: 85.6%; Average loss: 0.1261\n",
            "Iteration: 25694; Percent complete: 85.6%; Average loss: 0.1020\n",
            "Iteration: 25695; Percent complete: 85.7%; Average loss: 0.1120\n",
            "Iteration: 25696; Percent complete: 85.7%; Average loss: 0.1087\n",
            "Iteration: 25697; Percent complete: 85.7%; Average loss: 0.1201\n",
            "Iteration: 25698; Percent complete: 85.7%; Average loss: 0.1580\n",
            "Iteration: 25699; Percent complete: 85.7%; Average loss: 0.1219\n",
            "Iteration: 25700; Percent complete: 85.7%; Average loss: 0.1185\n",
            "Iteration: 25701; Percent complete: 85.7%; Average loss: 0.1207\n",
            "Iteration: 25702; Percent complete: 85.7%; Average loss: 0.0811\n",
            "Iteration: 25703; Percent complete: 85.7%; Average loss: 0.1466\n",
            "Iteration: 25704; Percent complete: 85.7%; Average loss: 0.1333\n",
            "Iteration: 25705; Percent complete: 85.7%; Average loss: 0.1194\n",
            "Iteration: 25706; Percent complete: 85.7%; Average loss: 0.1582\n",
            "Iteration: 25707; Percent complete: 85.7%; Average loss: 0.1259\n",
            "Iteration: 25708; Percent complete: 85.7%; Average loss: 0.1311\n",
            "Iteration: 25709; Percent complete: 85.7%; Average loss: 0.1164\n",
            "Iteration: 25710; Percent complete: 85.7%; Average loss: 0.1918\n",
            "Iteration: 25711; Percent complete: 85.7%; Average loss: 0.1192\n",
            "Iteration: 25712; Percent complete: 85.7%; Average loss: 0.1142\n",
            "Iteration: 25713; Percent complete: 85.7%; Average loss: 0.1306\n",
            "Iteration: 25714; Percent complete: 85.7%; Average loss: 0.1068\n",
            "Iteration: 25715; Percent complete: 85.7%; Average loss: 0.1647\n",
            "Iteration: 25716; Percent complete: 85.7%; Average loss: 0.0889\n",
            "Iteration: 25717; Percent complete: 85.7%; Average loss: 0.1154\n",
            "Iteration: 25718; Percent complete: 85.7%; Average loss: 0.1571\n",
            "Iteration: 25719; Percent complete: 85.7%; Average loss: 0.1774\n",
            "Iteration: 25720; Percent complete: 85.7%; Average loss: 0.0960\n",
            "Iteration: 25721; Percent complete: 85.7%; Average loss: 0.1097\n",
            "Iteration: 25722; Percent complete: 85.7%; Average loss: 0.0809\n",
            "Iteration: 25723; Percent complete: 85.7%; Average loss: 0.1497\n",
            "Iteration: 25724; Percent complete: 85.7%; Average loss: 0.1836\n",
            "Iteration: 25725; Percent complete: 85.8%; Average loss: 0.1206\n",
            "Iteration: 25726; Percent complete: 85.8%; Average loss: 0.1104\n",
            "Iteration: 25727; Percent complete: 85.8%; Average loss: 0.0866\n",
            "Iteration: 25728; Percent complete: 85.8%; Average loss: 0.1712\n",
            "Iteration: 25729; Percent complete: 85.8%; Average loss: 0.1191\n",
            "Iteration: 25730; Percent complete: 85.8%; Average loss: 0.1536\n",
            "Iteration: 25731; Percent complete: 85.8%; Average loss: 0.1593\n",
            "Iteration: 25732; Percent complete: 85.8%; Average loss: 0.1165\n",
            "Iteration: 25733; Percent complete: 85.8%; Average loss: 0.0896\n",
            "Iteration: 25734; Percent complete: 85.8%; Average loss: 0.1397\n",
            "Iteration: 25735; Percent complete: 85.8%; Average loss: 0.1309\n",
            "Iteration: 25736; Percent complete: 85.8%; Average loss: 0.1537\n",
            "Iteration: 25737; Percent complete: 85.8%; Average loss: 0.1029\n",
            "Iteration: 25738; Percent complete: 85.8%; Average loss: 0.1006\n",
            "Iteration: 25739; Percent complete: 85.8%; Average loss: 0.1428\n",
            "Iteration: 25740; Percent complete: 85.8%; Average loss: 0.1031\n",
            "Iteration: 25741; Percent complete: 85.8%; Average loss: 0.1377\n",
            "Iteration: 25742; Percent complete: 85.8%; Average loss: 0.1484\n",
            "Iteration: 25743; Percent complete: 85.8%; Average loss: 0.1282\n",
            "Iteration: 25744; Percent complete: 85.8%; Average loss: 0.1328\n",
            "Iteration: 25745; Percent complete: 85.8%; Average loss: 0.1121\n",
            "Iteration: 25746; Percent complete: 85.8%; Average loss: 0.1718\n",
            "Iteration: 25747; Percent complete: 85.8%; Average loss: 0.1304\n",
            "Iteration: 25748; Percent complete: 85.8%; Average loss: 0.1023\n",
            "Iteration: 25749; Percent complete: 85.8%; Average loss: 0.0574\n",
            "Iteration: 25750; Percent complete: 85.8%; Average loss: 0.1476\n",
            "Iteration: 25751; Percent complete: 85.8%; Average loss: 0.1548\n",
            "Iteration: 25752; Percent complete: 85.8%; Average loss: 0.0871\n",
            "Iteration: 25753; Percent complete: 85.8%; Average loss: 0.1342\n",
            "Iteration: 25754; Percent complete: 85.8%; Average loss: 0.1046\n",
            "Iteration: 25755; Percent complete: 85.9%; Average loss: 0.1529\n",
            "Iteration: 25756; Percent complete: 85.9%; Average loss: 0.1044\n",
            "Iteration: 25757; Percent complete: 85.9%; Average loss: 0.0916\n",
            "Iteration: 25758; Percent complete: 85.9%; Average loss: 0.1511\n",
            "Iteration: 25759; Percent complete: 85.9%; Average loss: 0.1298\n",
            "Iteration: 25760; Percent complete: 85.9%; Average loss: 0.1059\n",
            "Iteration: 25761; Percent complete: 85.9%; Average loss: 0.1196\n",
            "Iteration: 25762; Percent complete: 85.9%; Average loss: 0.1330\n",
            "Iteration: 25763; Percent complete: 85.9%; Average loss: 0.1145\n",
            "Iteration: 25764; Percent complete: 85.9%; Average loss: 0.1715\n",
            "Iteration: 25765; Percent complete: 85.9%; Average loss: 0.1183\n",
            "Iteration: 25766; Percent complete: 85.9%; Average loss: 0.1757\n",
            "Iteration: 25767; Percent complete: 85.9%; Average loss: 0.1137\n",
            "Iteration: 25768; Percent complete: 85.9%; Average loss: 0.1125\n",
            "Iteration: 25769; Percent complete: 85.9%; Average loss: 0.1242\n",
            "Iteration: 25770; Percent complete: 85.9%; Average loss: 0.1914\n",
            "Iteration: 25771; Percent complete: 85.9%; Average loss: 0.1243\n",
            "Iteration: 25772; Percent complete: 85.9%; Average loss: 0.1363\n",
            "Iteration: 25773; Percent complete: 85.9%; Average loss: 0.1019\n",
            "Iteration: 25774; Percent complete: 85.9%; Average loss: 0.1072\n",
            "Iteration: 25775; Percent complete: 85.9%; Average loss: 0.1322\n",
            "Iteration: 25776; Percent complete: 85.9%; Average loss: 0.1196\n",
            "Iteration: 25777; Percent complete: 85.9%; Average loss: 0.1635\n",
            "Iteration: 25778; Percent complete: 85.9%; Average loss: 0.1574\n",
            "Iteration: 25779; Percent complete: 85.9%; Average loss: 0.1156\n",
            "Iteration: 25780; Percent complete: 85.9%; Average loss: 0.1703\n",
            "Iteration: 25781; Percent complete: 85.9%; Average loss: 0.1225\n",
            "Iteration: 25782; Percent complete: 85.9%; Average loss: 0.1066\n",
            "Iteration: 25783; Percent complete: 85.9%; Average loss: 0.1425\n",
            "Iteration: 25784; Percent complete: 85.9%; Average loss: 0.1688\n",
            "Iteration: 25785; Percent complete: 86.0%; Average loss: 0.1202\n",
            "Iteration: 25786; Percent complete: 86.0%; Average loss: 0.0908\n",
            "Iteration: 25787; Percent complete: 86.0%; Average loss: 0.1302\n",
            "Iteration: 25788; Percent complete: 86.0%; Average loss: 0.1654\n",
            "Iteration: 25789; Percent complete: 86.0%; Average loss: 0.1219\n",
            "Iteration: 25790; Percent complete: 86.0%; Average loss: 0.1398\n",
            "Iteration: 25791; Percent complete: 86.0%; Average loss: 0.0970\n",
            "Iteration: 25792; Percent complete: 86.0%; Average loss: 0.1607\n",
            "Iteration: 25793; Percent complete: 86.0%; Average loss: 0.1355\n",
            "Iteration: 25794; Percent complete: 86.0%; Average loss: 0.1111\n",
            "Iteration: 25795; Percent complete: 86.0%; Average loss: 0.1593\n",
            "Iteration: 25796; Percent complete: 86.0%; Average loss: 0.1005\n",
            "Iteration: 25797; Percent complete: 86.0%; Average loss: 0.1107\n",
            "Iteration: 25798; Percent complete: 86.0%; Average loss: 0.1197\n",
            "Iteration: 25799; Percent complete: 86.0%; Average loss: 0.1302\n",
            "Iteration: 25800; Percent complete: 86.0%; Average loss: 0.1712\n",
            "Iteration: 25801; Percent complete: 86.0%; Average loss: 0.1062\n",
            "Iteration: 25802; Percent complete: 86.0%; Average loss: 0.1162\n",
            "Iteration: 25803; Percent complete: 86.0%; Average loss: 0.1098\n",
            "Iteration: 25804; Percent complete: 86.0%; Average loss: 0.0829\n",
            "Iteration: 25805; Percent complete: 86.0%; Average loss: 0.1164\n",
            "Iteration: 25806; Percent complete: 86.0%; Average loss: 0.1201\n",
            "Iteration: 25807; Percent complete: 86.0%; Average loss: 0.1456\n",
            "Iteration: 25808; Percent complete: 86.0%; Average loss: 0.1204\n",
            "Iteration: 25809; Percent complete: 86.0%; Average loss: 0.0953\n",
            "Iteration: 25810; Percent complete: 86.0%; Average loss: 0.1117\n",
            "Iteration: 25811; Percent complete: 86.0%; Average loss: 0.1271\n",
            "Iteration: 25812; Percent complete: 86.0%; Average loss: 0.1165\n",
            "Iteration: 25813; Percent complete: 86.0%; Average loss: 0.1514\n",
            "Iteration: 25814; Percent complete: 86.0%; Average loss: 0.1303\n",
            "Iteration: 25815; Percent complete: 86.1%; Average loss: 0.1365\n",
            "Iteration: 25816; Percent complete: 86.1%; Average loss: 0.1543\n",
            "Iteration: 25817; Percent complete: 86.1%; Average loss: 0.0850\n",
            "Iteration: 25818; Percent complete: 86.1%; Average loss: 0.0998\n",
            "Iteration: 25819; Percent complete: 86.1%; Average loss: 0.1434\n",
            "Iteration: 25820; Percent complete: 86.1%; Average loss: 0.1140\n",
            "Iteration: 25821; Percent complete: 86.1%; Average loss: 0.1378\n",
            "Iteration: 25822; Percent complete: 86.1%; Average loss: 0.0789\n",
            "Iteration: 25823; Percent complete: 86.1%; Average loss: 0.1507\n",
            "Iteration: 25824; Percent complete: 86.1%; Average loss: 0.1506\n",
            "Iteration: 25825; Percent complete: 86.1%; Average loss: 0.0753\n",
            "Iteration: 25826; Percent complete: 86.1%; Average loss: 0.1264\n",
            "Iteration: 25827; Percent complete: 86.1%; Average loss: 0.1263\n",
            "Iteration: 25828; Percent complete: 86.1%; Average loss: 0.1667\n",
            "Iteration: 25829; Percent complete: 86.1%; Average loss: 0.1086\n",
            "Iteration: 25830; Percent complete: 86.1%; Average loss: 0.1221\n",
            "Iteration: 25831; Percent complete: 86.1%; Average loss: 0.2114\n",
            "Iteration: 25832; Percent complete: 86.1%; Average loss: 0.0915\n",
            "Iteration: 25833; Percent complete: 86.1%; Average loss: 0.1164\n",
            "Iteration: 25834; Percent complete: 86.1%; Average loss: 0.1236\n",
            "Iteration: 25835; Percent complete: 86.1%; Average loss: 0.0979\n",
            "Iteration: 25836; Percent complete: 86.1%; Average loss: 0.1065\n",
            "Iteration: 25837; Percent complete: 86.1%; Average loss: 0.1085\n",
            "Iteration: 25838; Percent complete: 86.1%; Average loss: 0.1199\n",
            "Iteration: 25839; Percent complete: 86.1%; Average loss: 0.1000\n",
            "Iteration: 25840; Percent complete: 86.1%; Average loss: 0.1162\n",
            "Iteration: 25841; Percent complete: 86.1%; Average loss: 0.1499\n",
            "Iteration: 25842; Percent complete: 86.1%; Average loss: 0.0696\n",
            "Iteration: 25843; Percent complete: 86.1%; Average loss: 0.1110\n",
            "Iteration: 25844; Percent complete: 86.1%; Average loss: 0.1194\n",
            "Iteration: 25845; Percent complete: 86.2%; Average loss: 0.1978\n",
            "Iteration: 25846; Percent complete: 86.2%; Average loss: 0.1360\n",
            "Iteration: 25847; Percent complete: 86.2%; Average loss: 0.1340\n",
            "Iteration: 25848; Percent complete: 86.2%; Average loss: 0.1198\n",
            "Iteration: 25849; Percent complete: 86.2%; Average loss: 0.1493\n",
            "Iteration: 25850; Percent complete: 86.2%; Average loss: 0.1576\n",
            "Iteration: 25851; Percent complete: 86.2%; Average loss: 0.1030\n",
            "Iteration: 25852; Percent complete: 86.2%; Average loss: 0.0863\n",
            "Iteration: 25853; Percent complete: 86.2%; Average loss: 0.0986\n",
            "Iteration: 25854; Percent complete: 86.2%; Average loss: 0.1128\n",
            "Iteration: 25855; Percent complete: 86.2%; Average loss: 0.0837\n",
            "Iteration: 25856; Percent complete: 86.2%; Average loss: 0.1308\n",
            "Iteration: 25857; Percent complete: 86.2%; Average loss: 0.0960\n",
            "Iteration: 25858; Percent complete: 86.2%; Average loss: 0.1519\n",
            "Iteration: 25859; Percent complete: 86.2%; Average loss: 0.0873\n",
            "Iteration: 25860; Percent complete: 86.2%; Average loss: 0.1812\n",
            "Iteration: 25861; Percent complete: 86.2%; Average loss: 0.1061\n",
            "Iteration: 25862; Percent complete: 86.2%; Average loss: 0.1298\n",
            "Iteration: 25863; Percent complete: 86.2%; Average loss: 0.1121\n",
            "Iteration: 25864; Percent complete: 86.2%; Average loss: 0.0952\n",
            "Iteration: 25865; Percent complete: 86.2%; Average loss: 0.1112\n",
            "Iteration: 25866; Percent complete: 86.2%; Average loss: 0.1269\n",
            "Iteration: 25867; Percent complete: 86.2%; Average loss: 0.1321\n",
            "Iteration: 25868; Percent complete: 86.2%; Average loss: 0.1182\n",
            "Iteration: 25869; Percent complete: 86.2%; Average loss: 0.1153\n",
            "Iteration: 25870; Percent complete: 86.2%; Average loss: 0.1522\n",
            "Iteration: 25871; Percent complete: 86.2%; Average loss: 0.1487\n",
            "Iteration: 25872; Percent complete: 86.2%; Average loss: 0.1132\n",
            "Iteration: 25873; Percent complete: 86.2%; Average loss: 0.1492\n",
            "Iteration: 25874; Percent complete: 86.2%; Average loss: 0.1479\n",
            "Iteration: 25875; Percent complete: 86.2%; Average loss: 0.1285\n",
            "Iteration: 25876; Percent complete: 86.3%; Average loss: 0.1331\n",
            "Iteration: 25877; Percent complete: 86.3%; Average loss: 0.1049\n",
            "Iteration: 25878; Percent complete: 86.3%; Average loss: 0.0934\n",
            "Iteration: 25879; Percent complete: 86.3%; Average loss: 0.1750\n",
            "Iteration: 25880; Percent complete: 86.3%; Average loss: 0.1310\n",
            "Iteration: 25881; Percent complete: 86.3%; Average loss: 0.1208\n",
            "Iteration: 25882; Percent complete: 86.3%; Average loss: 0.1388\n",
            "Iteration: 25883; Percent complete: 86.3%; Average loss: 0.1667\n",
            "Iteration: 25884; Percent complete: 86.3%; Average loss: 0.1369\n",
            "Iteration: 25885; Percent complete: 86.3%; Average loss: 0.0968\n",
            "Iteration: 25886; Percent complete: 86.3%; Average loss: 0.1115\n",
            "Iteration: 25887; Percent complete: 86.3%; Average loss: 0.1302\n",
            "Iteration: 25888; Percent complete: 86.3%; Average loss: 0.1304\n",
            "Iteration: 25889; Percent complete: 86.3%; Average loss: 0.1177\n",
            "Iteration: 25890; Percent complete: 86.3%; Average loss: 0.1217\n",
            "Iteration: 25891; Percent complete: 86.3%; Average loss: 0.1338\n",
            "Iteration: 25892; Percent complete: 86.3%; Average loss: 0.1197\n",
            "Iteration: 25893; Percent complete: 86.3%; Average loss: 0.1250\n",
            "Iteration: 25894; Percent complete: 86.3%; Average loss: 0.1170\n",
            "Iteration: 25895; Percent complete: 86.3%; Average loss: 0.1663\n",
            "Iteration: 25896; Percent complete: 86.3%; Average loss: 0.1726\n",
            "Iteration: 25897; Percent complete: 86.3%; Average loss: 0.1824\n",
            "Iteration: 25898; Percent complete: 86.3%; Average loss: 0.1487\n",
            "Iteration: 25899; Percent complete: 86.3%; Average loss: 0.1107\n",
            "Iteration: 25900; Percent complete: 86.3%; Average loss: 0.1070\n",
            "Iteration: 25901; Percent complete: 86.3%; Average loss: 0.1101\n",
            "Iteration: 25902; Percent complete: 86.3%; Average loss: 0.0925\n",
            "Iteration: 25903; Percent complete: 86.3%; Average loss: 0.0892\n",
            "Iteration: 25904; Percent complete: 86.3%; Average loss: 0.1563\n",
            "Iteration: 25905; Percent complete: 86.4%; Average loss: 0.1230\n",
            "Iteration: 25906; Percent complete: 86.4%; Average loss: 0.1234\n",
            "Iteration: 25907; Percent complete: 86.4%; Average loss: 0.0993\n",
            "Iteration: 25908; Percent complete: 86.4%; Average loss: 0.1222\n",
            "Iteration: 25909; Percent complete: 86.4%; Average loss: 0.1082\n",
            "Iteration: 25910; Percent complete: 86.4%; Average loss: 0.1067\n",
            "Iteration: 25911; Percent complete: 86.4%; Average loss: 0.0962\n",
            "Iteration: 25912; Percent complete: 86.4%; Average loss: 0.1125\n",
            "Iteration: 25913; Percent complete: 86.4%; Average loss: 0.1156\n",
            "Iteration: 25914; Percent complete: 86.4%; Average loss: 0.1141\n",
            "Iteration: 25915; Percent complete: 86.4%; Average loss: 0.1042\n",
            "Iteration: 25916; Percent complete: 86.4%; Average loss: 0.1504\n",
            "Iteration: 25917; Percent complete: 86.4%; Average loss: 0.1277\n",
            "Iteration: 25918; Percent complete: 86.4%; Average loss: 0.1251\n",
            "Iteration: 25919; Percent complete: 86.4%; Average loss: 0.1014\n",
            "Iteration: 25920; Percent complete: 86.4%; Average loss: 0.1109\n",
            "Iteration: 25921; Percent complete: 86.4%; Average loss: 0.1348\n",
            "Iteration: 25922; Percent complete: 86.4%; Average loss: 0.1202\n",
            "Iteration: 25923; Percent complete: 86.4%; Average loss: 0.1121\n",
            "Iteration: 25924; Percent complete: 86.4%; Average loss: 0.1149\n",
            "Iteration: 25925; Percent complete: 86.4%; Average loss: 0.1178\n",
            "Iteration: 25926; Percent complete: 86.4%; Average loss: 0.0771\n",
            "Iteration: 25927; Percent complete: 86.4%; Average loss: 0.1086\n",
            "Iteration: 25928; Percent complete: 86.4%; Average loss: 0.1424\n",
            "Iteration: 25929; Percent complete: 86.4%; Average loss: 0.1399\n",
            "Iteration: 25930; Percent complete: 86.4%; Average loss: 0.1267\n",
            "Iteration: 25931; Percent complete: 86.4%; Average loss: 0.0966\n",
            "Iteration: 25932; Percent complete: 86.4%; Average loss: 0.1116\n",
            "Iteration: 25933; Percent complete: 86.4%; Average loss: 0.1445\n",
            "Iteration: 25934; Percent complete: 86.4%; Average loss: 0.1312\n",
            "Iteration: 25935; Percent complete: 86.5%; Average loss: 0.1373\n",
            "Iteration: 25936; Percent complete: 86.5%; Average loss: 0.1671\n",
            "Iteration: 25937; Percent complete: 86.5%; Average loss: 0.1580\n",
            "Iteration: 25938; Percent complete: 86.5%; Average loss: 0.1296\n",
            "Iteration: 25939; Percent complete: 86.5%; Average loss: 0.1052\n",
            "Iteration: 25940; Percent complete: 86.5%; Average loss: 0.1483\n",
            "Iteration: 25941; Percent complete: 86.5%; Average loss: 0.1224\n",
            "Iteration: 25942; Percent complete: 86.5%; Average loss: 0.0742\n",
            "Iteration: 25943; Percent complete: 86.5%; Average loss: 0.1327\n",
            "Iteration: 25944; Percent complete: 86.5%; Average loss: 0.0878\n",
            "Iteration: 25945; Percent complete: 86.5%; Average loss: 0.1007\n",
            "Iteration: 25946; Percent complete: 86.5%; Average loss: 0.0968\n",
            "Iteration: 25947; Percent complete: 86.5%; Average loss: 0.0970\n",
            "Iteration: 25948; Percent complete: 86.5%; Average loss: 0.1160\n",
            "Iteration: 25949; Percent complete: 86.5%; Average loss: 0.0882\n",
            "Iteration: 25950; Percent complete: 86.5%; Average loss: 0.0840\n",
            "Iteration: 25951; Percent complete: 86.5%; Average loss: 0.1481\n",
            "Iteration: 25952; Percent complete: 86.5%; Average loss: 0.1335\n",
            "Iteration: 25953; Percent complete: 86.5%; Average loss: 0.1178\n",
            "Iteration: 25954; Percent complete: 86.5%; Average loss: 0.1251\n",
            "Iteration: 25955; Percent complete: 86.5%; Average loss: 0.1099\n",
            "Iteration: 25956; Percent complete: 86.5%; Average loss: 0.1207\n",
            "Iteration: 25957; Percent complete: 86.5%; Average loss: 0.1340\n",
            "Iteration: 25958; Percent complete: 86.5%; Average loss: 0.1174\n",
            "Iteration: 25959; Percent complete: 86.5%; Average loss: 0.1162\n",
            "Iteration: 25960; Percent complete: 86.5%; Average loss: 0.1095\n",
            "Iteration: 25961; Percent complete: 86.5%; Average loss: 0.0886\n",
            "Iteration: 25962; Percent complete: 86.5%; Average loss: 0.1038\n",
            "Iteration: 25963; Percent complete: 86.5%; Average loss: 0.0915\n",
            "Iteration: 25964; Percent complete: 86.5%; Average loss: 0.1267\n",
            "Iteration: 25965; Percent complete: 86.6%; Average loss: 0.1212\n",
            "Iteration: 25966; Percent complete: 86.6%; Average loss: 0.0902\n",
            "Iteration: 25967; Percent complete: 86.6%; Average loss: 0.1319\n",
            "Iteration: 25968; Percent complete: 86.6%; Average loss: 0.1162\n",
            "Iteration: 25969; Percent complete: 86.6%; Average loss: 0.0744\n",
            "Iteration: 25970; Percent complete: 86.6%; Average loss: 0.1345\n",
            "Iteration: 25971; Percent complete: 86.6%; Average loss: 0.1305\n",
            "Iteration: 25972; Percent complete: 86.6%; Average loss: 0.1355\n",
            "Iteration: 25973; Percent complete: 86.6%; Average loss: 0.1448\n",
            "Iteration: 25974; Percent complete: 86.6%; Average loss: 0.1353\n",
            "Iteration: 25975; Percent complete: 86.6%; Average loss: 0.1469\n",
            "Iteration: 25976; Percent complete: 86.6%; Average loss: 0.0896\n",
            "Iteration: 25977; Percent complete: 86.6%; Average loss: 0.1124\n",
            "Iteration: 25978; Percent complete: 86.6%; Average loss: 0.1200\n",
            "Iteration: 25979; Percent complete: 86.6%; Average loss: 0.1634\n",
            "Iteration: 25980; Percent complete: 86.6%; Average loss: 0.1703\n",
            "Iteration: 25981; Percent complete: 86.6%; Average loss: 0.0920\n",
            "Iteration: 25982; Percent complete: 86.6%; Average loss: 0.1240\n",
            "Iteration: 25983; Percent complete: 86.6%; Average loss: 0.1524\n",
            "Iteration: 25984; Percent complete: 86.6%; Average loss: 0.1099\n",
            "Iteration: 25985; Percent complete: 86.6%; Average loss: 0.1533\n",
            "Iteration: 25986; Percent complete: 86.6%; Average loss: 0.1010\n",
            "Iteration: 25987; Percent complete: 86.6%; Average loss: 0.1074\n",
            "Iteration: 25988; Percent complete: 86.6%; Average loss: 0.1484\n",
            "Iteration: 25989; Percent complete: 86.6%; Average loss: 0.1385\n",
            "Iteration: 25990; Percent complete: 86.6%; Average loss: 0.1620\n",
            "Iteration: 25991; Percent complete: 86.6%; Average loss: 0.1014\n",
            "Iteration: 25992; Percent complete: 86.6%; Average loss: 0.1335\n",
            "Iteration: 25993; Percent complete: 86.6%; Average loss: 0.1330\n",
            "Iteration: 25994; Percent complete: 86.6%; Average loss: 0.1718\n",
            "Iteration: 25995; Percent complete: 86.7%; Average loss: 0.1006\n",
            "Iteration: 25996; Percent complete: 86.7%; Average loss: 0.1168\n",
            "Iteration: 25997; Percent complete: 86.7%; Average loss: 0.1136\n",
            "Iteration: 25998; Percent complete: 86.7%; Average loss: 0.1084\n",
            "Iteration: 25999; Percent complete: 86.7%; Average loss: 0.1048\n",
            "Iteration: 26000; Percent complete: 86.7%; Average loss: 0.1103\n",
            "Iteration: 26001; Percent complete: 86.7%; Average loss: 0.1163\n",
            "Iteration: 26002; Percent complete: 86.7%; Average loss: 0.1185\n",
            "Iteration: 26003; Percent complete: 86.7%; Average loss: 0.1020\n",
            "Iteration: 26004; Percent complete: 86.7%; Average loss: 0.1493\n",
            "Iteration: 26005; Percent complete: 86.7%; Average loss: 0.1182\n",
            "Iteration: 26006; Percent complete: 86.7%; Average loss: 0.1072\n",
            "Iteration: 26007; Percent complete: 86.7%; Average loss: 0.1326\n",
            "Iteration: 26008; Percent complete: 86.7%; Average loss: 0.1014\n",
            "Iteration: 26009; Percent complete: 86.7%; Average loss: 0.1157\n",
            "Iteration: 26010; Percent complete: 86.7%; Average loss: 0.1170\n",
            "Iteration: 26011; Percent complete: 86.7%; Average loss: 0.1378\n",
            "Iteration: 26012; Percent complete: 86.7%; Average loss: 0.1092\n",
            "Iteration: 26013; Percent complete: 86.7%; Average loss: 0.1096\n",
            "Iteration: 26014; Percent complete: 86.7%; Average loss: 0.1271\n",
            "Iteration: 26015; Percent complete: 86.7%; Average loss: 0.1646\n",
            "Iteration: 26016; Percent complete: 86.7%; Average loss: 0.1497\n",
            "Iteration: 26017; Percent complete: 86.7%; Average loss: 0.1447\n",
            "Iteration: 26018; Percent complete: 86.7%; Average loss: 0.1532\n",
            "Iteration: 26019; Percent complete: 86.7%; Average loss: 0.1518\n",
            "Iteration: 26020; Percent complete: 86.7%; Average loss: 0.1371\n",
            "Iteration: 26021; Percent complete: 86.7%; Average loss: 0.1065\n",
            "Iteration: 26022; Percent complete: 86.7%; Average loss: 0.1419\n",
            "Iteration: 26023; Percent complete: 86.7%; Average loss: 0.1713\n",
            "Iteration: 26024; Percent complete: 86.7%; Average loss: 0.1166\n",
            "Iteration: 26025; Percent complete: 86.8%; Average loss: 0.1182\n",
            "Iteration: 26026; Percent complete: 86.8%; Average loss: 0.1315\n",
            "Iteration: 26027; Percent complete: 86.8%; Average loss: 0.0901\n",
            "Iteration: 26028; Percent complete: 86.8%; Average loss: 0.1086\n",
            "Iteration: 26029; Percent complete: 86.8%; Average loss: 0.1389\n",
            "Iteration: 26030; Percent complete: 86.8%; Average loss: 0.1317\n",
            "Iteration: 26031; Percent complete: 86.8%; Average loss: 0.0839\n",
            "Iteration: 26032; Percent complete: 86.8%; Average loss: 0.1128\n",
            "Iteration: 26033; Percent complete: 86.8%; Average loss: 0.1306\n",
            "Iteration: 26034; Percent complete: 86.8%; Average loss: 0.1306\n",
            "Iteration: 26035; Percent complete: 86.8%; Average loss: 0.1426\n",
            "Iteration: 26036; Percent complete: 86.8%; Average loss: 0.0924\n",
            "Iteration: 26037; Percent complete: 86.8%; Average loss: 0.1406\n",
            "Iteration: 26038; Percent complete: 86.8%; Average loss: 0.1146\n",
            "Iteration: 26039; Percent complete: 86.8%; Average loss: 0.1803\n",
            "Iteration: 26040; Percent complete: 86.8%; Average loss: 0.1201\n",
            "Iteration: 26041; Percent complete: 86.8%; Average loss: 0.1380\n",
            "Iteration: 26042; Percent complete: 86.8%; Average loss: 0.1198\n",
            "Iteration: 26043; Percent complete: 86.8%; Average loss: 0.1506\n",
            "Iteration: 26044; Percent complete: 86.8%; Average loss: 0.1238\n",
            "Iteration: 26045; Percent complete: 86.8%; Average loss: 0.1047\n",
            "Iteration: 26046; Percent complete: 86.8%; Average loss: 0.1715\n",
            "Iteration: 26047; Percent complete: 86.8%; Average loss: 0.1211\n",
            "Iteration: 26048; Percent complete: 86.8%; Average loss: 0.1514\n",
            "Iteration: 26049; Percent complete: 86.8%; Average loss: 0.1013\n",
            "Iteration: 26050; Percent complete: 86.8%; Average loss: 0.1180\n",
            "Iteration: 26051; Percent complete: 86.8%; Average loss: 0.1143\n",
            "Iteration: 26052; Percent complete: 86.8%; Average loss: 0.1287\n",
            "Iteration: 26053; Percent complete: 86.8%; Average loss: 0.1025\n",
            "Iteration: 26054; Percent complete: 86.8%; Average loss: 0.1707\n",
            "Iteration: 26055; Percent complete: 86.9%; Average loss: 0.1321\n",
            "Iteration: 26056; Percent complete: 86.9%; Average loss: 0.1742\n",
            "Iteration: 26057; Percent complete: 86.9%; Average loss: 0.0946\n",
            "Iteration: 26058; Percent complete: 86.9%; Average loss: 0.1389\n",
            "Iteration: 26059; Percent complete: 86.9%; Average loss: 0.1367\n",
            "Iteration: 26060; Percent complete: 86.9%; Average loss: 0.1332\n",
            "Iteration: 26061; Percent complete: 86.9%; Average loss: 0.0838\n",
            "Iteration: 26062; Percent complete: 86.9%; Average loss: 0.1098\n",
            "Iteration: 26063; Percent complete: 86.9%; Average loss: 0.1373\n",
            "Iteration: 26064; Percent complete: 86.9%; Average loss: 0.1204\n",
            "Iteration: 26065; Percent complete: 86.9%; Average loss: 0.1610\n",
            "Iteration: 26066; Percent complete: 86.9%; Average loss: 0.1320\n",
            "Iteration: 26067; Percent complete: 86.9%; Average loss: 0.1750\n",
            "Iteration: 26068; Percent complete: 86.9%; Average loss: 0.1351\n",
            "Iteration: 26069; Percent complete: 86.9%; Average loss: 0.1194\n",
            "Iteration: 26070; Percent complete: 86.9%; Average loss: 0.1294\n",
            "Iteration: 26071; Percent complete: 86.9%; Average loss: 0.1185\n",
            "Iteration: 26072; Percent complete: 86.9%; Average loss: 0.1291\n",
            "Iteration: 26073; Percent complete: 86.9%; Average loss: 0.0835\n",
            "Iteration: 26074; Percent complete: 86.9%; Average loss: 0.1555\n",
            "Iteration: 26075; Percent complete: 86.9%; Average loss: 0.1501\n",
            "Iteration: 26076; Percent complete: 86.9%; Average loss: 0.1113\n",
            "Iteration: 26077; Percent complete: 86.9%; Average loss: 0.1319\n",
            "Iteration: 26078; Percent complete: 86.9%; Average loss: 0.1263\n",
            "Iteration: 26079; Percent complete: 86.9%; Average loss: 0.1309\n",
            "Iteration: 26080; Percent complete: 86.9%; Average loss: 0.1725\n",
            "Iteration: 26081; Percent complete: 86.9%; Average loss: 0.1238\n",
            "Iteration: 26082; Percent complete: 86.9%; Average loss: 0.1065\n",
            "Iteration: 26083; Percent complete: 86.9%; Average loss: 0.1003\n",
            "Iteration: 26084; Percent complete: 86.9%; Average loss: 0.1132\n",
            "Iteration: 26085; Percent complete: 87.0%; Average loss: 0.1214\n",
            "Iteration: 26086; Percent complete: 87.0%; Average loss: 0.0899\n",
            "Iteration: 26087; Percent complete: 87.0%; Average loss: 0.1151\n",
            "Iteration: 26088; Percent complete: 87.0%; Average loss: 0.1257\n",
            "Iteration: 26089; Percent complete: 87.0%; Average loss: 0.1436\n",
            "Iteration: 26090; Percent complete: 87.0%; Average loss: 0.1247\n",
            "Iteration: 26091; Percent complete: 87.0%; Average loss: 0.0846\n",
            "Iteration: 26092; Percent complete: 87.0%; Average loss: 0.1519\n",
            "Iteration: 26093; Percent complete: 87.0%; Average loss: 0.0736\n",
            "Iteration: 26094; Percent complete: 87.0%; Average loss: 0.1056\n",
            "Iteration: 26095; Percent complete: 87.0%; Average loss: 0.1122\n",
            "Iteration: 26096; Percent complete: 87.0%; Average loss: 0.1649\n",
            "Iteration: 26097; Percent complete: 87.0%; Average loss: 0.1131\n",
            "Iteration: 26098; Percent complete: 87.0%; Average loss: 0.0920\n",
            "Iteration: 26099; Percent complete: 87.0%; Average loss: 0.1323\n",
            "Iteration: 26100; Percent complete: 87.0%; Average loss: 0.1532\n",
            "Iteration: 26101; Percent complete: 87.0%; Average loss: 0.0941\n",
            "Iteration: 26102; Percent complete: 87.0%; Average loss: 0.1338\n",
            "Iteration: 26103; Percent complete: 87.0%; Average loss: 0.1520\n",
            "Iteration: 26104; Percent complete: 87.0%; Average loss: 0.1549\n",
            "Iteration: 26105; Percent complete: 87.0%; Average loss: 0.1144\n",
            "Iteration: 26106; Percent complete: 87.0%; Average loss: 0.1277\n",
            "Iteration: 26107; Percent complete: 87.0%; Average loss: 0.1556\n",
            "Iteration: 26108; Percent complete: 87.0%; Average loss: 0.1276\n",
            "Iteration: 26109; Percent complete: 87.0%; Average loss: 0.1033\n",
            "Iteration: 26110; Percent complete: 87.0%; Average loss: 0.1204\n",
            "Iteration: 26111; Percent complete: 87.0%; Average loss: 0.1615\n",
            "Iteration: 26112; Percent complete: 87.0%; Average loss: 0.1019\n",
            "Iteration: 26113; Percent complete: 87.0%; Average loss: 0.0993\n",
            "Iteration: 26114; Percent complete: 87.0%; Average loss: 0.1272\n",
            "Iteration: 26115; Percent complete: 87.1%; Average loss: 0.1334\n",
            "Iteration: 26116; Percent complete: 87.1%; Average loss: 0.1228\n",
            "Iteration: 26117; Percent complete: 87.1%; Average loss: 0.1056\n",
            "Iteration: 26118; Percent complete: 87.1%; Average loss: 0.1513\n",
            "Iteration: 26119; Percent complete: 87.1%; Average loss: 0.1448\n",
            "Iteration: 26120; Percent complete: 87.1%; Average loss: 0.1367\n",
            "Iteration: 26121; Percent complete: 87.1%; Average loss: 0.1227\n",
            "Iteration: 26122; Percent complete: 87.1%; Average loss: 0.1185\n",
            "Iteration: 26123; Percent complete: 87.1%; Average loss: 0.1607\n",
            "Iteration: 26124; Percent complete: 87.1%; Average loss: 0.1522\n",
            "Iteration: 26125; Percent complete: 87.1%; Average loss: 0.1234\n",
            "Iteration: 26126; Percent complete: 87.1%; Average loss: 0.1246\n",
            "Iteration: 26127; Percent complete: 87.1%; Average loss: 0.1610\n",
            "Iteration: 26128; Percent complete: 87.1%; Average loss: 0.1070\n",
            "Iteration: 26129; Percent complete: 87.1%; Average loss: 0.1038\n",
            "Iteration: 26130; Percent complete: 87.1%; Average loss: 0.1014\n",
            "Iteration: 26131; Percent complete: 87.1%; Average loss: 0.0906\n",
            "Iteration: 26132; Percent complete: 87.1%; Average loss: 0.1638\n",
            "Iteration: 26133; Percent complete: 87.1%; Average loss: 0.1017\n",
            "Iteration: 26134; Percent complete: 87.1%; Average loss: 0.1099\n",
            "Iteration: 26135; Percent complete: 87.1%; Average loss: 0.0953\n",
            "Iteration: 26136; Percent complete: 87.1%; Average loss: 0.1790\n",
            "Iteration: 26137; Percent complete: 87.1%; Average loss: 0.0650\n",
            "Iteration: 26138; Percent complete: 87.1%; Average loss: 0.1049\n",
            "Iteration: 26139; Percent complete: 87.1%; Average loss: 0.1933\n",
            "Iteration: 26140; Percent complete: 87.1%; Average loss: 0.1313\n",
            "Iteration: 26141; Percent complete: 87.1%; Average loss: 0.1595\n",
            "Iteration: 26142; Percent complete: 87.1%; Average loss: 0.1038\n",
            "Iteration: 26143; Percent complete: 87.1%; Average loss: 0.0951\n",
            "Iteration: 26144; Percent complete: 87.1%; Average loss: 0.1253\n",
            "Iteration: 26145; Percent complete: 87.2%; Average loss: 0.1102\n",
            "Iteration: 26146; Percent complete: 87.2%; Average loss: 0.1146\n",
            "Iteration: 26147; Percent complete: 87.2%; Average loss: 0.1034\n",
            "Iteration: 26148; Percent complete: 87.2%; Average loss: 0.0959\n",
            "Iteration: 26149; Percent complete: 87.2%; Average loss: 0.1207\n",
            "Iteration: 26150; Percent complete: 87.2%; Average loss: 0.1307\n",
            "Iteration: 26151; Percent complete: 87.2%; Average loss: 0.1506\n",
            "Iteration: 26152; Percent complete: 87.2%; Average loss: 0.1201\n",
            "Iteration: 26153; Percent complete: 87.2%; Average loss: 0.1098\n",
            "Iteration: 26154; Percent complete: 87.2%; Average loss: 0.1054\n",
            "Iteration: 26155; Percent complete: 87.2%; Average loss: 0.1339\n",
            "Iteration: 26156; Percent complete: 87.2%; Average loss: 0.1147\n",
            "Iteration: 26157; Percent complete: 87.2%; Average loss: 0.1409\n",
            "Iteration: 26158; Percent complete: 87.2%; Average loss: 0.1586\n",
            "Iteration: 26159; Percent complete: 87.2%; Average loss: 0.1817\n",
            "Iteration: 26160; Percent complete: 87.2%; Average loss: 0.1660\n",
            "Iteration: 26161; Percent complete: 87.2%; Average loss: 0.1069\n",
            "Iteration: 26162; Percent complete: 87.2%; Average loss: 0.1107\n",
            "Iteration: 26163; Percent complete: 87.2%; Average loss: 0.1263\n",
            "Iteration: 26164; Percent complete: 87.2%; Average loss: 0.1150\n",
            "Iteration: 26165; Percent complete: 87.2%; Average loss: 0.1213\n",
            "Iteration: 26166; Percent complete: 87.2%; Average loss: 0.1167\n",
            "Iteration: 26167; Percent complete: 87.2%; Average loss: 0.0976\n",
            "Iteration: 26168; Percent complete: 87.2%; Average loss: 0.1544\n",
            "Iteration: 26169; Percent complete: 87.2%; Average loss: 0.1154\n",
            "Iteration: 26170; Percent complete: 87.2%; Average loss: 0.1062\n",
            "Iteration: 26171; Percent complete: 87.2%; Average loss: 0.1292\n",
            "Iteration: 26172; Percent complete: 87.2%; Average loss: 0.1338\n",
            "Iteration: 26173; Percent complete: 87.2%; Average loss: 0.1063\n",
            "Iteration: 26174; Percent complete: 87.2%; Average loss: 0.1296\n",
            "Iteration: 26175; Percent complete: 87.2%; Average loss: 0.1703\n",
            "Iteration: 26176; Percent complete: 87.3%; Average loss: 0.1214\n",
            "Iteration: 26177; Percent complete: 87.3%; Average loss: 0.1118\n",
            "Iteration: 26178; Percent complete: 87.3%; Average loss: 0.0899\n",
            "Iteration: 26179; Percent complete: 87.3%; Average loss: 0.1450\n",
            "Iteration: 26180; Percent complete: 87.3%; Average loss: 0.1379\n",
            "Iteration: 26181; Percent complete: 87.3%; Average loss: 0.1328\n",
            "Iteration: 26182; Percent complete: 87.3%; Average loss: 0.0995\n",
            "Iteration: 26183; Percent complete: 87.3%; Average loss: 0.0923\n",
            "Iteration: 26184; Percent complete: 87.3%; Average loss: 0.1244\n",
            "Iteration: 26185; Percent complete: 87.3%; Average loss: 0.1363\n",
            "Iteration: 26186; Percent complete: 87.3%; Average loss: 0.1235\n",
            "Iteration: 26187; Percent complete: 87.3%; Average loss: 0.1619\n",
            "Iteration: 26188; Percent complete: 87.3%; Average loss: 0.1634\n",
            "Iteration: 26189; Percent complete: 87.3%; Average loss: 0.1254\n",
            "Iteration: 26190; Percent complete: 87.3%; Average loss: 0.1200\n",
            "Iteration: 26191; Percent complete: 87.3%; Average loss: 0.1123\n",
            "Iteration: 26192; Percent complete: 87.3%; Average loss: 0.1205\n",
            "Iteration: 26193; Percent complete: 87.3%; Average loss: 0.1592\n",
            "Iteration: 26194; Percent complete: 87.3%; Average loss: 0.1461\n",
            "Iteration: 26195; Percent complete: 87.3%; Average loss: 0.1385\n",
            "Iteration: 26196; Percent complete: 87.3%; Average loss: 0.1763\n",
            "Iteration: 26197; Percent complete: 87.3%; Average loss: 0.1983\n",
            "Iteration: 26198; Percent complete: 87.3%; Average loss: 0.1654\n",
            "Iteration: 26199; Percent complete: 87.3%; Average loss: 0.1098\n",
            "Iteration: 26200; Percent complete: 87.3%; Average loss: 0.0742\n",
            "Iteration: 26201; Percent complete: 87.3%; Average loss: 0.1438\n",
            "Iteration: 26202; Percent complete: 87.3%; Average loss: 0.1433\n",
            "Iteration: 26203; Percent complete: 87.3%; Average loss: 0.0889\n",
            "Iteration: 26204; Percent complete: 87.3%; Average loss: 0.1979\n",
            "Iteration: 26205; Percent complete: 87.4%; Average loss: 0.1512\n",
            "Iteration: 26206; Percent complete: 87.4%; Average loss: 0.1213\n",
            "Iteration: 26207; Percent complete: 87.4%; Average loss: 0.0991\n",
            "Iteration: 26208; Percent complete: 87.4%; Average loss: 0.1507\n",
            "Iteration: 26209; Percent complete: 87.4%; Average loss: 0.1142\n",
            "Iteration: 26210; Percent complete: 87.4%; Average loss: 0.1218\n",
            "Iteration: 26211; Percent complete: 87.4%; Average loss: 0.1280\n",
            "Iteration: 26212; Percent complete: 87.4%; Average loss: 0.1731\n",
            "Iteration: 26213; Percent complete: 87.4%; Average loss: 0.1098\n",
            "Iteration: 26214; Percent complete: 87.4%; Average loss: 0.1570\n",
            "Iteration: 26215; Percent complete: 87.4%; Average loss: 0.1278\n",
            "Iteration: 26216; Percent complete: 87.4%; Average loss: 0.1375\n",
            "Iteration: 26217; Percent complete: 87.4%; Average loss: 0.1048\n",
            "Iteration: 26218; Percent complete: 87.4%; Average loss: 0.0862\n",
            "Iteration: 26219; Percent complete: 87.4%; Average loss: 0.0944\n",
            "Iteration: 26220; Percent complete: 87.4%; Average loss: 0.1155\n",
            "Iteration: 26221; Percent complete: 87.4%; Average loss: 0.0942\n",
            "Iteration: 26222; Percent complete: 87.4%; Average loss: 0.1322\n",
            "Iteration: 26223; Percent complete: 87.4%; Average loss: 0.1282\n",
            "Iteration: 26224; Percent complete: 87.4%; Average loss: 0.1238\n",
            "Iteration: 26225; Percent complete: 87.4%; Average loss: 0.1525\n",
            "Iteration: 26226; Percent complete: 87.4%; Average loss: 0.0892\n",
            "Iteration: 26227; Percent complete: 87.4%; Average loss: 0.1329\n",
            "Iteration: 26228; Percent complete: 87.4%; Average loss: 0.0915\n",
            "Iteration: 26229; Percent complete: 87.4%; Average loss: 0.1172\n",
            "Iteration: 26230; Percent complete: 87.4%; Average loss: 0.1018\n",
            "Iteration: 26231; Percent complete: 87.4%; Average loss: 0.1128\n",
            "Iteration: 26232; Percent complete: 87.4%; Average loss: 0.0991\n",
            "Iteration: 26233; Percent complete: 87.4%; Average loss: 0.1402\n",
            "Iteration: 26234; Percent complete: 87.4%; Average loss: 0.1158\n",
            "Iteration: 26235; Percent complete: 87.5%; Average loss: 0.1277\n",
            "Iteration: 26236; Percent complete: 87.5%; Average loss: 0.1314\n",
            "Iteration: 26237; Percent complete: 87.5%; Average loss: 0.1536\n",
            "Iteration: 26238; Percent complete: 87.5%; Average loss: 0.1267\n",
            "Iteration: 26239; Percent complete: 87.5%; Average loss: 0.1082\n",
            "Iteration: 26240; Percent complete: 87.5%; Average loss: 0.1151\n",
            "Iteration: 26241; Percent complete: 87.5%; Average loss: 0.1682\n",
            "Iteration: 26242; Percent complete: 87.5%; Average loss: 0.1732\n",
            "Iteration: 26243; Percent complete: 87.5%; Average loss: 0.1190\n",
            "Iteration: 26244; Percent complete: 87.5%; Average loss: 0.1237\n",
            "Iteration: 26245; Percent complete: 87.5%; Average loss: 0.1312\n",
            "Iteration: 26246; Percent complete: 87.5%; Average loss: 0.1292\n",
            "Iteration: 26247; Percent complete: 87.5%; Average loss: 0.1501\n",
            "Iteration: 26248; Percent complete: 87.5%; Average loss: 0.1115\n",
            "Iteration: 26249; Percent complete: 87.5%; Average loss: 0.1499\n",
            "Iteration: 26250; Percent complete: 87.5%; Average loss: 0.1472\n",
            "Iteration: 26251; Percent complete: 87.5%; Average loss: 0.1122\n",
            "Iteration: 26252; Percent complete: 87.5%; Average loss: 0.1182\n",
            "Iteration: 26253; Percent complete: 87.5%; Average loss: 0.1520\n",
            "Iteration: 26254; Percent complete: 87.5%; Average loss: 0.1506\n",
            "Iteration: 26255; Percent complete: 87.5%; Average loss: 0.1068\n",
            "Iteration: 26256; Percent complete: 87.5%; Average loss: 0.1297\n",
            "Iteration: 26257; Percent complete: 87.5%; Average loss: 0.1177\n",
            "Iteration: 26258; Percent complete: 87.5%; Average loss: 0.1182\n",
            "Iteration: 26259; Percent complete: 87.5%; Average loss: 0.1041\n",
            "Iteration: 26260; Percent complete: 87.5%; Average loss: 0.1836\n",
            "Iteration: 26261; Percent complete: 87.5%; Average loss: 0.1576\n",
            "Iteration: 26262; Percent complete: 87.5%; Average loss: 0.0918\n",
            "Iteration: 26263; Percent complete: 87.5%; Average loss: 0.1066\n",
            "Iteration: 26264; Percent complete: 87.5%; Average loss: 0.1054\n",
            "Iteration: 26265; Percent complete: 87.5%; Average loss: 0.1438\n",
            "Iteration: 26266; Percent complete: 87.6%; Average loss: 0.1322\n",
            "Iteration: 26267; Percent complete: 87.6%; Average loss: 0.1130\n",
            "Iteration: 26268; Percent complete: 87.6%; Average loss: 0.1217\n",
            "Iteration: 26269; Percent complete: 87.6%; Average loss: 0.1394\n",
            "Iteration: 26270; Percent complete: 87.6%; Average loss: 0.1267\n",
            "Iteration: 26271; Percent complete: 87.6%; Average loss: 0.1200\n",
            "Iteration: 26272; Percent complete: 87.6%; Average loss: 0.1064\n",
            "Iteration: 26273; Percent complete: 87.6%; Average loss: 0.0941\n",
            "Iteration: 26274; Percent complete: 87.6%; Average loss: 0.1390\n",
            "Iteration: 26275; Percent complete: 87.6%; Average loss: 0.1537\n",
            "Iteration: 26276; Percent complete: 87.6%; Average loss: 0.1302\n",
            "Iteration: 26277; Percent complete: 87.6%; Average loss: 0.1255\n",
            "Iteration: 26278; Percent complete: 87.6%; Average loss: 0.1154\n",
            "Iteration: 26279; Percent complete: 87.6%; Average loss: 0.1062\n",
            "Iteration: 26280; Percent complete: 87.6%; Average loss: 0.1195\n",
            "Iteration: 26281; Percent complete: 87.6%; Average loss: 0.0882\n",
            "Iteration: 26282; Percent complete: 87.6%; Average loss: 0.1640\n",
            "Iteration: 26283; Percent complete: 87.6%; Average loss: 0.1586\n",
            "Iteration: 26284; Percent complete: 87.6%; Average loss: 0.1502\n",
            "Iteration: 26285; Percent complete: 87.6%; Average loss: 0.1273\n",
            "Iteration: 26286; Percent complete: 87.6%; Average loss: 0.1039\n",
            "Iteration: 26287; Percent complete: 87.6%; Average loss: 0.1094\n",
            "Iteration: 26288; Percent complete: 87.6%; Average loss: 0.1181\n",
            "Iteration: 26289; Percent complete: 87.6%; Average loss: 0.1227\n",
            "Iteration: 26290; Percent complete: 87.6%; Average loss: 0.0777\n",
            "Iteration: 26291; Percent complete: 87.6%; Average loss: 0.0909\n",
            "Iteration: 26292; Percent complete: 87.6%; Average loss: 0.1339\n",
            "Iteration: 26293; Percent complete: 87.6%; Average loss: 0.1601\n",
            "Iteration: 26294; Percent complete: 87.6%; Average loss: 0.2132\n",
            "Iteration: 26295; Percent complete: 87.6%; Average loss: 0.1132\n",
            "Iteration: 26296; Percent complete: 87.7%; Average loss: 0.1577\n",
            "Iteration: 26297; Percent complete: 87.7%; Average loss: 0.1035\n",
            "Iteration: 26298; Percent complete: 87.7%; Average loss: 0.1045\n",
            "Iteration: 26299; Percent complete: 87.7%; Average loss: 0.1251\n",
            "Iteration: 26300; Percent complete: 87.7%; Average loss: 0.1187\n",
            "Iteration: 26301; Percent complete: 87.7%; Average loss: 0.1305\n",
            "Iteration: 26302; Percent complete: 87.7%; Average loss: 0.1396\n",
            "Iteration: 26303; Percent complete: 87.7%; Average loss: 0.1235\n",
            "Iteration: 26304; Percent complete: 87.7%; Average loss: 0.1394\n",
            "Iteration: 26305; Percent complete: 87.7%; Average loss: 0.1147\n",
            "Iteration: 26306; Percent complete: 87.7%; Average loss: 0.1085\n",
            "Iteration: 26307; Percent complete: 87.7%; Average loss: 0.1786\n",
            "Iteration: 26308; Percent complete: 87.7%; Average loss: 0.1292\n",
            "Iteration: 26309; Percent complete: 87.7%; Average loss: 0.1363\n",
            "Iteration: 26310; Percent complete: 87.7%; Average loss: 0.1025\n",
            "Iteration: 26311; Percent complete: 87.7%; Average loss: 0.0590\n",
            "Iteration: 26312; Percent complete: 87.7%; Average loss: 0.0764\n",
            "Iteration: 26313; Percent complete: 87.7%; Average loss: 0.1342\n",
            "Iteration: 26314; Percent complete: 87.7%; Average loss: 0.1199\n",
            "Iteration: 26315; Percent complete: 87.7%; Average loss: 0.1315\n",
            "Iteration: 26316; Percent complete: 87.7%; Average loss: 0.1013\n",
            "Iteration: 26317; Percent complete: 87.7%; Average loss: 0.1199\n",
            "Iteration: 26318; Percent complete: 87.7%; Average loss: 0.1297\n",
            "Iteration: 26319; Percent complete: 87.7%; Average loss: 0.1300\n",
            "Iteration: 26320; Percent complete: 87.7%; Average loss: 0.1222\n",
            "Iteration: 26321; Percent complete: 87.7%; Average loss: 0.1317\n",
            "Iteration: 26322; Percent complete: 87.7%; Average loss: 0.1211\n",
            "Iteration: 26323; Percent complete: 87.7%; Average loss: 0.1280\n",
            "Iteration: 26324; Percent complete: 87.7%; Average loss: 0.1265\n",
            "Iteration: 26325; Percent complete: 87.8%; Average loss: 0.1432\n",
            "Iteration: 26326; Percent complete: 87.8%; Average loss: 0.1353\n",
            "Iteration: 26327; Percent complete: 87.8%; Average loss: 0.0943\n",
            "Iteration: 26328; Percent complete: 87.8%; Average loss: 0.1116\n",
            "Iteration: 26329; Percent complete: 87.8%; Average loss: 0.1183\n",
            "Iteration: 26330; Percent complete: 87.8%; Average loss: 0.1317\n",
            "Iteration: 26331; Percent complete: 87.8%; Average loss: 0.0896\n",
            "Iteration: 26332; Percent complete: 87.8%; Average loss: 0.1762\n",
            "Iteration: 26333; Percent complete: 87.8%; Average loss: 0.0978\n",
            "Iteration: 26334; Percent complete: 87.8%; Average loss: 0.1667\n",
            "Iteration: 26335; Percent complete: 87.8%; Average loss: 0.0752\n",
            "Iteration: 26336; Percent complete: 87.8%; Average loss: 0.1251\n",
            "Iteration: 26337; Percent complete: 87.8%; Average loss: 0.1748\n",
            "Iteration: 26338; Percent complete: 87.8%; Average loss: 0.1661\n",
            "Iteration: 26339; Percent complete: 87.8%; Average loss: 0.1361\n",
            "Iteration: 26340; Percent complete: 87.8%; Average loss: 0.1617\n",
            "Iteration: 26341; Percent complete: 87.8%; Average loss: 0.1070\n",
            "Iteration: 26342; Percent complete: 87.8%; Average loss: 0.1198\n",
            "Iteration: 26343; Percent complete: 87.8%; Average loss: 0.1325\n",
            "Iteration: 26344; Percent complete: 87.8%; Average loss: 0.1430\n",
            "Iteration: 26345; Percent complete: 87.8%; Average loss: 0.1314\n",
            "Iteration: 26346; Percent complete: 87.8%; Average loss: 0.0968\n",
            "Iteration: 26347; Percent complete: 87.8%; Average loss: 0.1252\n",
            "Iteration: 26348; Percent complete: 87.8%; Average loss: 0.0956\n",
            "Iteration: 26349; Percent complete: 87.8%; Average loss: 0.1099\n",
            "Iteration: 26350; Percent complete: 87.8%; Average loss: 0.1403\n",
            "Iteration: 26351; Percent complete: 87.8%; Average loss: 0.0868\n",
            "Iteration: 26352; Percent complete: 87.8%; Average loss: 0.1548\n",
            "Iteration: 26353; Percent complete: 87.8%; Average loss: 0.1017\n",
            "Iteration: 26354; Percent complete: 87.8%; Average loss: 0.1498\n",
            "Iteration: 26355; Percent complete: 87.8%; Average loss: 0.1061\n",
            "Iteration: 26356; Percent complete: 87.9%; Average loss: 0.1164\n",
            "Iteration: 26357; Percent complete: 87.9%; Average loss: 0.1530\n",
            "Iteration: 26358; Percent complete: 87.9%; Average loss: 0.1448\n",
            "Iteration: 26359; Percent complete: 87.9%; Average loss: 0.1019\n",
            "Iteration: 26360; Percent complete: 87.9%; Average loss: 0.1520\n",
            "Iteration: 26361; Percent complete: 87.9%; Average loss: 0.1252\n",
            "Iteration: 26362; Percent complete: 87.9%; Average loss: 0.0993\n",
            "Iteration: 26363; Percent complete: 87.9%; Average loss: 0.1066\n",
            "Iteration: 26364; Percent complete: 87.9%; Average loss: 0.1072\n",
            "Iteration: 26365; Percent complete: 87.9%; Average loss: 0.1100\n",
            "Iteration: 26366; Percent complete: 87.9%; Average loss: 0.1458\n",
            "Iteration: 26367; Percent complete: 87.9%; Average loss: 0.1350\n",
            "Iteration: 26368; Percent complete: 87.9%; Average loss: 0.1107\n",
            "Iteration: 26369; Percent complete: 87.9%; Average loss: 0.0947\n",
            "Iteration: 26370; Percent complete: 87.9%; Average loss: 0.0840\n",
            "Iteration: 26371; Percent complete: 87.9%; Average loss: 0.1308\n",
            "Iteration: 26372; Percent complete: 87.9%; Average loss: 0.1362\n",
            "Iteration: 26373; Percent complete: 87.9%; Average loss: 0.1407\n",
            "Iteration: 26374; Percent complete: 87.9%; Average loss: 0.1456\n",
            "Iteration: 26375; Percent complete: 87.9%; Average loss: 0.0762\n",
            "Iteration: 26376; Percent complete: 87.9%; Average loss: 0.0964\n",
            "Iteration: 26377; Percent complete: 87.9%; Average loss: 0.1178\n",
            "Iteration: 26378; Percent complete: 87.9%; Average loss: 0.1702\n",
            "Iteration: 26379; Percent complete: 87.9%; Average loss: 0.1690\n",
            "Iteration: 26380; Percent complete: 87.9%; Average loss: 0.1385\n",
            "Iteration: 26381; Percent complete: 87.9%; Average loss: 0.1397\n",
            "Iteration: 26382; Percent complete: 87.9%; Average loss: 0.2092\n",
            "Iteration: 26383; Percent complete: 87.9%; Average loss: 0.1003\n",
            "Iteration: 26384; Percent complete: 87.9%; Average loss: 0.1354\n",
            "Iteration: 26385; Percent complete: 87.9%; Average loss: 0.1290\n",
            "Iteration: 26386; Percent complete: 88.0%; Average loss: 0.1311\n",
            "Iteration: 26387; Percent complete: 88.0%; Average loss: 0.1138\n",
            "Iteration: 26388; Percent complete: 88.0%; Average loss: 0.1821\n",
            "Iteration: 26389; Percent complete: 88.0%; Average loss: 0.1203\n",
            "Iteration: 26390; Percent complete: 88.0%; Average loss: 0.1053\n",
            "Iteration: 26391; Percent complete: 88.0%; Average loss: 0.1231\n",
            "Iteration: 26392; Percent complete: 88.0%; Average loss: 0.0948\n",
            "Iteration: 26393; Percent complete: 88.0%; Average loss: 0.1210\n",
            "Iteration: 26394; Percent complete: 88.0%; Average loss: 0.0916\n",
            "Iteration: 26395; Percent complete: 88.0%; Average loss: 0.0803\n",
            "Iteration: 26396; Percent complete: 88.0%; Average loss: 0.1293\n",
            "Iteration: 26397; Percent complete: 88.0%; Average loss: 0.0984\n",
            "Iteration: 26398; Percent complete: 88.0%; Average loss: 0.1092\n",
            "Iteration: 26399; Percent complete: 88.0%; Average loss: 0.0820\n",
            "Iteration: 26400; Percent complete: 88.0%; Average loss: 0.1385\n",
            "Iteration: 26401; Percent complete: 88.0%; Average loss: 0.0833\n",
            "Iteration: 26402; Percent complete: 88.0%; Average loss: 0.1228\n",
            "Iteration: 26403; Percent complete: 88.0%; Average loss: 0.1381\n",
            "Iteration: 26404; Percent complete: 88.0%; Average loss: 0.1292\n",
            "Iteration: 26405; Percent complete: 88.0%; Average loss: 0.1161\n",
            "Iteration: 26406; Percent complete: 88.0%; Average loss: 0.1824\n",
            "Iteration: 26407; Percent complete: 88.0%; Average loss: 0.1178\n",
            "Iteration: 26408; Percent complete: 88.0%; Average loss: 0.1318\n",
            "Iteration: 26409; Percent complete: 88.0%; Average loss: 0.1263\n",
            "Iteration: 26410; Percent complete: 88.0%; Average loss: 0.0795\n",
            "Iteration: 26411; Percent complete: 88.0%; Average loss: 0.1188\n",
            "Iteration: 26412; Percent complete: 88.0%; Average loss: 0.1239\n",
            "Iteration: 26413; Percent complete: 88.0%; Average loss: 0.1254\n",
            "Iteration: 26414; Percent complete: 88.0%; Average loss: 0.1242\n",
            "Iteration: 26415; Percent complete: 88.0%; Average loss: 0.0739\n",
            "Iteration: 26416; Percent complete: 88.1%; Average loss: 0.1165\n",
            "Iteration: 26417; Percent complete: 88.1%; Average loss: 0.1278\n",
            "Iteration: 26418; Percent complete: 88.1%; Average loss: 0.1243\n",
            "Iteration: 26419; Percent complete: 88.1%; Average loss: 0.1270\n",
            "Iteration: 26420; Percent complete: 88.1%; Average loss: 0.0909\n",
            "Iteration: 26421; Percent complete: 88.1%; Average loss: 0.0720\n",
            "Iteration: 26422; Percent complete: 88.1%; Average loss: 0.1070\n",
            "Iteration: 26423; Percent complete: 88.1%; Average loss: 0.1245\n",
            "Iteration: 26424; Percent complete: 88.1%; Average loss: 0.1412\n",
            "Iteration: 26425; Percent complete: 88.1%; Average loss: 0.1327\n",
            "Iteration: 26426; Percent complete: 88.1%; Average loss: 0.0975\n",
            "Iteration: 26427; Percent complete: 88.1%; Average loss: 0.0954\n",
            "Iteration: 26428; Percent complete: 88.1%; Average loss: 0.0943\n",
            "Iteration: 26429; Percent complete: 88.1%; Average loss: 0.1277\n",
            "Iteration: 26430; Percent complete: 88.1%; Average loss: 0.0972\n",
            "Iteration: 26431; Percent complete: 88.1%; Average loss: 0.1156\n",
            "Iteration: 26432; Percent complete: 88.1%; Average loss: 0.1405\n",
            "Iteration: 26433; Percent complete: 88.1%; Average loss: 0.1028\n",
            "Iteration: 26434; Percent complete: 88.1%; Average loss: 0.1380\n",
            "Iteration: 26435; Percent complete: 88.1%; Average loss: 0.1503\n",
            "Iteration: 26436; Percent complete: 88.1%; Average loss: 0.1578\n",
            "Iteration: 26437; Percent complete: 88.1%; Average loss: 0.1364\n",
            "Iteration: 26438; Percent complete: 88.1%; Average loss: 0.0799\n",
            "Iteration: 26439; Percent complete: 88.1%; Average loss: 0.1215\n",
            "Iteration: 26440; Percent complete: 88.1%; Average loss: 0.0955\n",
            "Iteration: 26441; Percent complete: 88.1%; Average loss: 0.1162\n",
            "Iteration: 26442; Percent complete: 88.1%; Average loss: 0.1285\n",
            "Iteration: 26443; Percent complete: 88.1%; Average loss: 0.1475\n",
            "Iteration: 26444; Percent complete: 88.1%; Average loss: 0.1539\n",
            "Iteration: 26445; Percent complete: 88.1%; Average loss: 0.1369\n",
            "Iteration: 26446; Percent complete: 88.2%; Average loss: 0.0964\n",
            "Iteration: 26447; Percent complete: 88.2%; Average loss: 0.1174\n",
            "Iteration: 26448; Percent complete: 88.2%; Average loss: 0.1305\n",
            "Iteration: 26449; Percent complete: 88.2%; Average loss: 0.1079\n",
            "Iteration: 26450; Percent complete: 88.2%; Average loss: 0.1205\n",
            "Iteration: 26451; Percent complete: 88.2%; Average loss: 0.1329\n",
            "Iteration: 26452; Percent complete: 88.2%; Average loss: 0.1815\n",
            "Iteration: 26453; Percent complete: 88.2%; Average loss: 0.1054\n",
            "Iteration: 26454; Percent complete: 88.2%; Average loss: 0.1585\n",
            "Iteration: 26455; Percent complete: 88.2%; Average loss: 0.0798\n",
            "Iteration: 26456; Percent complete: 88.2%; Average loss: 0.1486\n",
            "Iteration: 26457; Percent complete: 88.2%; Average loss: 0.1158\n",
            "Iteration: 26458; Percent complete: 88.2%; Average loss: 0.1500\n",
            "Iteration: 26459; Percent complete: 88.2%; Average loss: 0.1324\n",
            "Iteration: 26460; Percent complete: 88.2%; Average loss: 0.1450\n",
            "Iteration: 26461; Percent complete: 88.2%; Average loss: 0.1062\n",
            "Iteration: 26462; Percent complete: 88.2%; Average loss: 0.1050\n",
            "Iteration: 26463; Percent complete: 88.2%; Average loss: 0.1147\n",
            "Iteration: 26464; Percent complete: 88.2%; Average loss: 0.1284\n",
            "Iteration: 26465; Percent complete: 88.2%; Average loss: 0.1113\n",
            "Iteration: 26466; Percent complete: 88.2%; Average loss: 0.0925\n",
            "Iteration: 26467; Percent complete: 88.2%; Average loss: 0.1450\n",
            "Iteration: 26468; Percent complete: 88.2%; Average loss: 0.0950\n",
            "Iteration: 26469; Percent complete: 88.2%; Average loss: 0.1075\n",
            "Iteration: 26470; Percent complete: 88.2%; Average loss: 0.0888\n",
            "Iteration: 26471; Percent complete: 88.2%; Average loss: 0.1333\n",
            "Iteration: 26472; Percent complete: 88.2%; Average loss: 0.0924\n",
            "Iteration: 26473; Percent complete: 88.2%; Average loss: 0.0812\n",
            "Iteration: 26474; Percent complete: 88.2%; Average loss: 0.1057\n",
            "Iteration: 26475; Percent complete: 88.2%; Average loss: 0.1401\n",
            "Iteration: 26476; Percent complete: 88.3%; Average loss: 0.1148\n",
            "Iteration: 26477; Percent complete: 88.3%; Average loss: 0.1354\n",
            "Iteration: 26478; Percent complete: 88.3%; Average loss: 0.1331\n",
            "Iteration: 26479; Percent complete: 88.3%; Average loss: 0.1674\n",
            "Iteration: 26480; Percent complete: 88.3%; Average loss: 0.0729\n",
            "Iteration: 26481; Percent complete: 88.3%; Average loss: 0.1193\n",
            "Iteration: 26482; Percent complete: 88.3%; Average loss: 0.1120\n",
            "Iteration: 26483; Percent complete: 88.3%; Average loss: 0.1636\n",
            "Iteration: 26484; Percent complete: 88.3%; Average loss: 0.1187\n",
            "Iteration: 26485; Percent complete: 88.3%; Average loss: 0.0968\n",
            "Iteration: 26486; Percent complete: 88.3%; Average loss: 0.1374\n",
            "Iteration: 26487; Percent complete: 88.3%; Average loss: 0.1316\n",
            "Iteration: 26488; Percent complete: 88.3%; Average loss: 0.1510\n",
            "Iteration: 26489; Percent complete: 88.3%; Average loss: 0.1497\n",
            "Iteration: 26490; Percent complete: 88.3%; Average loss: 0.1013\n",
            "Iteration: 26491; Percent complete: 88.3%; Average loss: 0.1203\n",
            "Iteration: 26492; Percent complete: 88.3%; Average loss: 0.1191\n",
            "Iteration: 26493; Percent complete: 88.3%; Average loss: 0.1337\n",
            "Iteration: 26494; Percent complete: 88.3%; Average loss: 0.1052\n",
            "Iteration: 26495; Percent complete: 88.3%; Average loss: 0.1280\n",
            "Iteration: 26496; Percent complete: 88.3%; Average loss: 0.1265\n",
            "Iteration: 26497; Percent complete: 88.3%; Average loss: 0.1364\n",
            "Iteration: 26498; Percent complete: 88.3%; Average loss: 0.1284\n",
            "Iteration: 26499; Percent complete: 88.3%; Average loss: 0.1209\n",
            "Iteration: 26500; Percent complete: 88.3%; Average loss: 0.1067\n",
            "Iteration: 26501; Percent complete: 88.3%; Average loss: 0.1280\n",
            "Iteration: 26502; Percent complete: 88.3%; Average loss: 0.1164\n",
            "Iteration: 26503; Percent complete: 88.3%; Average loss: 0.1447\n",
            "Iteration: 26504; Percent complete: 88.3%; Average loss: 0.1527\n",
            "Iteration: 26505; Percent complete: 88.3%; Average loss: 0.1200\n",
            "Iteration: 26506; Percent complete: 88.4%; Average loss: 0.1127\n",
            "Iteration: 26507; Percent complete: 88.4%; Average loss: 0.1457\n",
            "Iteration: 26508; Percent complete: 88.4%; Average loss: 0.1412\n",
            "Iteration: 26509; Percent complete: 88.4%; Average loss: 0.1281\n",
            "Iteration: 26510; Percent complete: 88.4%; Average loss: 0.0750\n",
            "Iteration: 26511; Percent complete: 88.4%; Average loss: 0.1644\n",
            "Iteration: 26512; Percent complete: 88.4%; Average loss: 0.1092\n",
            "Iteration: 26513; Percent complete: 88.4%; Average loss: 0.1044\n",
            "Iteration: 26514; Percent complete: 88.4%; Average loss: 0.1384\n",
            "Iteration: 26515; Percent complete: 88.4%; Average loss: 0.1228\n",
            "Iteration: 26516; Percent complete: 88.4%; Average loss: 0.1277\n",
            "Iteration: 26517; Percent complete: 88.4%; Average loss: 0.1644\n",
            "Iteration: 26518; Percent complete: 88.4%; Average loss: 0.0651\n",
            "Iteration: 26519; Percent complete: 88.4%; Average loss: 0.1205\n",
            "Iteration: 26520; Percent complete: 88.4%; Average loss: 0.1103\n",
            "Iteration: 26521; Percent complete: 88.4%; Average loss: 0.1039\n",
            "Iteration: 26522; Percent complete: 88.4%; Average loss: 0.1353\n",
            "Iteration: 26523; Percent complete: 88.4%; Average loss: 0.0924\n",
            "Iteration: 26524; Percent complete: 88.4%; Average loss: 0.1402\n",
            "Iteration: 26525; Percent complete: 88.4%; Average loss: 0.1435\n",
            "Iteration: 26526; Percent complete: 88.4%; Average loss: 0.1595\n",
            "Iteration: 26527; Percent complete: 88.4%; Average loss: 0.1549\n",
            "Iteration: 26528; Percent complete: 88.4%; Average loss: 0.1295\n",
            "Iteration: 26529; Percent complete: 88.4%; Average loss: 0.1017\n",
            "Iteration: 26530; Percent complete: 88.4%; Average loss: 0.1154\n",
            "Iteration: 26531; Percent complete: 88.4%; Average loss: 0.1438\n",
            "Iteration: 26532; Percent complete: 88.4%; Average loss: 0.1189\n",
            "Iteration: 26533; Percent complete: 88.4%; Average loss: 0.1053\n",
            "Iteration: 26534; Percent complete: 88.4%; Average loss: 0.1002\n",
            "Iteration: 26535; Percent complete: 88.4%; Average loss: 0.1294\n",
            "Iteration: 26536; Percent complete: 88.5%; Average loss: 0.1224\n",
            "Iteration: 26537; Percent complete: 88.5%; Average loss: 0.2112\n",
            "Iteration: 26538; Percent complete: 88.5%; Average loss: 0.1165\n",
            "Iteration: 26539; Percent complete: 88.5%; Average loss: 0.1071\n",
            "Iteration: 26540; Percent complete: 88.5%; Average loss: 0.1572\n",
            "Iteration: 26541; Percent complete: 88.5%; Average loss: 0.1181\n",
            "Iteration: 26542; Percent complete: 88.5%; Average loss: 0.1192\n",
            "Iteration: 26543; Percent complete: 88.5%; Average loss: 0.0740\n",
            "Iteration: 26544; Percent complete: 88.5%; Average loss: 0.1175\n",
            "Iteration: 26545; Percent complete: 88.5%; Average loss: 0.1474\n",
            "Iteration: 26546; Percent complete: 88.5%; Average loss: 0.1035\n",
            "Iteration: 26547; Percent complete: 88.5%; Average loss: 0.1391\n",
            "Iteration: 26548; Percent complete: 88.5%; Average loss: 0.1629\n",
            "Iteration: 26549; Percent complete: 88.5%; Average loss: 0.1365\n",
            "Iteration: 26550; Percent complete: 88.5%; Average loss: 0.0960\n",
            "Iteration: 26551; Percent complete: 88.5%; Average loss: 0.0991\n",
            "Iteration: 26552; Percent complete: 88.5%; Average loss: 0.1405\n",
            "Iteration: 26553; Percent complete: 88.5%; Average loss: 0.1465\n",
            "Iteration: 26554; Percent complete: 88.5%; Average loss: 0.0955\n",
            "Iteration: 26555; Percent complete: 88.5%; Average loss: 0.0891\n",
            "Iteration: 26556; Percent complete: 88.5%; Average loss: 0.1108\n",
            "Iteration: 26557; Percent complete: 88.5%; Average loss: 0.1075\n",
            "Iteration: 26558; Percent complete: 88.5%; Average loss: 0.1457\n",
            "Iteration: 26559; Percent complete: 88.5%; Average loss: 0.1129\n",
            "Iteration: 26560; Percent complete: 88.5%; Average loss: 0.1634\n",
            "Iteration: 26561; Percent complete: 88.5%; Average loss: 0.0853\n",
            "Iteration: 26562; Percent complete: 88.5%; Average loss: 0.0988\n",
            "Iteration: 26563; Percent complete: 88.5%; Average loss: 0.1337\n",
            "Iteration: 26564; Percent complete: 88.5%; Average loss: 0.0965\n",
            "Iteration: 26565; Percent complete: 88.5%; Average loss: 0.1006\n",
            "Iteration: 26566; Percent complete: 88.6%; Average loss: 0.1814\n",
            "Iteration: 26567; Percent complete: 88.6%; Average loss: 0.1251\n",
            "Iteration: 26568; Percent complete: 88.6%; Average loss: 0.1267\n",
            "Iteration: 26569; Percent complete: 88.6%; Average loss: 0.1361\n",
            "Iteration: 26570; Percent complete: 88.6%; Average loss: 0.1209\n",
            "Iteration: 26571; Percent complete: 88.6%; Average loss: 0.1950\n",
            "Iteration: 26572; Percent complete: 88.6%; Average loss: 0.1699\n",
            "Iteration: 26573; Percent complete: 88.6%; Average loss: 0.0995\n",
            "Iteration: 26574; Percent complete: 88.6%; Average loss: 0.1257\n",
            "Iteration: 26575; Percent complete: 88.6%; Average loss: 0.1105\n",
            "Iteration: 26576; Percent complete: 88.6%; Average loss: 0.1697\n",
            "Iteration: 26577; Percent complete: 88.6%; Average loss: 0.0778\n",
            "Iteration: 26578; Percent complete: 88.6%; Average loss: 0.1635\n",
            "Iteration: 26579; Percent complete: 88.6%; Average loss: 0.1110\n",
            "Iteration: 26580; Percent complete: 88.6%; Average loss: 0.1182\n",
            "Iteration: 26581; Percent complete: 88.6%; Average loss: 0.1730\n",
            "Iteration: 26582; Percent complete: 88.6%; Average loss: 0.1723\n",
            "Iteration: 26583; Percent complete: 88.6%; Average loss: 0.1135\n",
            "Iteration: 26584; Percent complete: 88.6%; Average loss: 0.1695\n",
            "Iteration: 26585; Percent complete: 88.6%; Average loss: 0.0833\n",
            "Iteration: 26586; Percent complete: 88.6%; Average loss: 0.1396\n",
            "Iteration: 26587; Percent complete: 88.6%; Average loss: 0.1351\n",
            "Iteration: 26588; Percent complete: 88.6%; Average loss: 0.1080\n",
            "Iteration: 26589; Percent complete: 88.6%; Average loss: 0.1079\n",
            "Iteration: 26590; Percent complete: 88.6%; Average loss: 0.1542\n",
            "Iteration: 26591; Percent complete: 88.6%; Average loss: 0.1620\n",
            "Iteration: 26592; Percent complete: 88.6%; Average loss: 0.1320\n",
            "Iteration: 26593; Percent complete: 88.6%; Average loss: 0.1561\n",
            "Iteration: 26594; Percent complete: 88.6%; Average loss: 0.1481\n",
            "Iteration: 26595; Percent complete: 88.6%; Average loss: 0.1491\n",
            "Iteration: 26596; Percent complete: 88.7%; Average loss: 0.1296\n",
            "Iteration: 26597; Percent complete: 88.7%; Average loss: 0.0849\n",
            "Iteration: 26598; Percent complete: 88.7%; Average loss: 0.1430\n",
            "Iteration: 26599; Percent complete: 88.7%; Average loss: 0.1539\n",
            "Iteration: 26600; Percent complete: 88.7%; Average loss: 0.1052\n",
            "Iteration: 26601; Percent complete: 88.7%; Average loss: 0.1298\n",
            "Iteration: 26602; Percent complete: 88.7%; Average loss: 0.1467\n",
            "Iteration: 26603; Percent complete: 88.7%; Average loss: 0.0936\n",
            "Iteration: 26604; Percent complete: 88.7%; Average loss: 0.0994\n",
            "Iteration: 26605; Percent complete: 88.7%; Average loss: 0.1325\n",
            "Iteration: 26606; Percent complete: 88.7%; Average loss: 0.1147\n",
            "Iteration: 26607; Percent complete: 88.7%; Average loss: 0.1038\n",
            "Iteration: 26608; Percent complete: 88.7%; Average loss: 0.1707\n",
            "Iteration: 26609; Percent complete: 88.7%; Average loss: 0.1166\n",
            "Iteration: 26610; Percent complete: 88.7%; Average loss: 0.1562\n",
            "Iteration: 26611; Percent complete: 88.7%; Average loss: 0.1696\n",
            "Iteration: 26612; Percent complete: 88.7%; Average loss: 0.1390\n",
            "Iteration: 26613; Percent complete: 88.7%; Average loss: 0.1424\n",
            "Iteration: 26614; Percent complete: 88.7%; Average loss: 0.1351\n",
            "Iteration: 26615; Percent complete: 88.7%; Average loss: 0.1463\n",
            "Iteration: 26616; Percent complete: 88.7%; Average loss: 0.2013\n",
            "Iteration: 26617; Percent complete: 88.7%; Average loss: 0.1545\n",
            "Iteration: 26618; Percent complete: 88.7%; Average loss: 0.1057\n",
            "Iteration: 26619; Percent complete: 88.7%; Average loss: 0.0969\n",
            "Iteration: 26620; Percent complete: 88.7%; Average loss: 0.1368\n",
            "Iteration: 26621; Percent complete: 88.7%; Average loss: 0.1328\n",
            "Iteration: 26622; Percent complete: 88.7%; Average loss: 0.0666\n",
            "Iteration: 26623; Percent complete: 88.7%; Average loss: 0.1398\n",
            "Iteration: 26624; Percent complete: 88.7%; Average loss: 0.1186\n",
            "Iteration: 26625; Percent complete: 88.8%; Average loss: 0.1222\n",
            "Iteration: 26626; Percent complete: 88.8%; Average loss: 0.1335\n",
            "Iteration: 26627; Percent complete: 88.8%; Average loss: 0.0993\n",
            "Iteration: 26628; Percent complete: 88.8%; Average loss: 0.1299\n",
            "Iteration: 26629; Percent complete: 88.8%; Average loss: 0.1127\n",
            "Iteration: 26630; Percent complete: 88.8%; Average loss: 0.1093\n",
            "Iteration: 26631; Percent complete: 88.8%; Average loss: 0.1157\n",
            "Iteration: 26632; Percent complete: 88.8%; Average loss: 0.1098\n",
            "Iteration: 26633; Percent complete: 88.8%; Average loss: 0.1727\n",
            "Iteration: 26634; Percent complete: 88.8%; Average loss: 0.1314\n",
            "Iteration: 26635; Percent complete: 88.8%; Average loss: 0.1587\n",
            "Iteration: 26636; Percent complete: 88.8%; Average loss: 0.1014\n",
            "Iteration: 26637; Percent complete: 88.8%; Average loss: 0.1041\n",
            "Iteration: 26638; Percent complete: 88.8%; Average loss: 0.1614\n",
            "Iteration: 26639; Percent complete: 88.8%; Average loss: 0.1224\n",
            "Iteration: 26640; Percent complete: 88.8%; Average loss: 0.1268\n",
            "Iteration: 26641; Percent complete: 88.8%; Average loss: 0.1146\n",
            "Iteration: 26642; Percent complete: 88.8%; Average loss: 0.1101\n",
            "Iteration: 26643; Percent complete: 88.8%; Average loss: 0.1751\n",
            "Iteration: 26644; Percent complete: 88.8%; Average loss: 0.1524\n",
            "Iteration: 26645; Percent complete: 88.8%; Average loss: 0.1917\n",
            "Iteration: 26646; Percent complete: 88.8%; Average loss: 0.1016\n",
            "Iteration: 26647; Percent complete: 88.8%; Average loss: 0.1005\n",
            "Iteration: 26648; Percent complete: 88.8%; Average loss: 0.1190\n",
            "Iteration: 26649; Percent complete: 88.8%; Average loss: 0.1275\n",
            "Iteration: 26650; Percent complete: 88.8%; Average loss: 0.1579\n",
            "Iteration: 26651; Percent complete: 88.8%; Average loss: 0.1507\n",
            "Iteration: 26652; Percent complete: 88.8%; Average loss: 0.1372\n",
            "Iteration: 26653; Percent complete: 88.8%; Average loss: 0.1079\n",
            "Iteration: 26654; Percent complete: 88.8%; Average loss: 0.1381\n",
            "Iteration: 26655; Percent complete: 88.8%; Average loss: 0.1121\n",
            "Iteration: 26656; Percent complete: 88.9%; Average loss: 0.1203\n",
            "Iteration: 26657; Percent complete: 88.9%; Average loss: 0.1483\n",
            "Iteration: 26658; Percent complete: 88.9%; Average loss: 0.1217\n",
            "Iteration: 26659; Percent complete: 88.9%; Average loss: 0.1111\n",
            "Iteration: 26660; Percent complete: 88.9%; Average loss: 0.1235\n",
            "Iteration: 26661; Percent complete: 88.9%; Average loss: 0.1336\n",
            "Iteration: 26662; Percent complete: 88.9%; Average loss: 0.1126\n",
            "Iteration: 26663; Percent complete: 88.9%; Average loss: 0.1101\n",
            "Iteration: 26664; Percent complete: 88.9%; Average loss: 0.1206\n",
            "Iteration: 26665; Percent complete: 88.9%; Average loss: 0.1507\n",
            "Iteration: 26666; Percent complete: 88.9%; Average loss: 0.1141\n",
            "Iteration: 26667; Percent complete: 88.9%; Average loss: 0.1223\n",
            "Iteration: 26668; Percent complete: 88.9%; Average loss: 0.1247\n",
            "Iteration: 26669; Percent complete: 88.9%; Average loss: 0.0969\n",
            "Iteration: 26670; Percent complete: 88.9%; Average loss: 0.1241\n",
            "Iteration: 26671; Percent complete: 88.9%; Average loss: 0.1422\n",
            "Iteration: 26672; Percent complete: 88.9%; Average loss: 0.1145\n",
            "Iteration: 26673; Percent complete: 88.9%; Average loss: 0.0998\n",
            "Iteration: 26674; Percent complete: 88.9%; Average loss: 0.1221\n",
            "Iteration: 26675; Percent complete: 88.9%; Average loss: 0.1332\n",
            "Iteration: 26676; Percent complete: 88.9%; Average loss: 0.1187\n",
            "Iteration: 26677; Percent complete: 88.9%; Average loss: 0.1919\n",
            "Iteration: 26678; Percent complete: 88.9%; Average loss: 0.1064\n",
            "Iteration: 26679; Percent complete: 88.9%; Average loss: 0.1181\n",
            "Iteration: 26680; Percent complete: 88.9%; Average loss: 0.1211\n",
            "Iteration: 26681; Percent complete: 88.9%; Average loss: 0.1190\n",
            "Iteration: 26682; Percent complete: 88.9%; Average loss: 0.1475\n",
            "Iteration: 26683; Percent complete: 88.9%; Average loss: 0.1709\n",
            "Iteration: 26684; Percent complete: 88.9%; Average loss: 0.1570\n",
            "Iteration: 26685; Percent complete: 88.9%; Average loss: 0.0923\n",
            "Iteration: 26686; Percent complete: 89.0%; Average loss: 0.1049\n",
            "Iteration: 26687; Percent complete: 89.0%; Average loss: 0.1257\n",
            "Iteration: 26688; Percent complete: 89.0%; Average loss: 0.1413\n",
            "Iteration: 26689; Percent complete: 89.0%; Average loss: 0.1197\n",
            "Iteration: 26690; Percent complete: 89.0%; Average loss: 0.1255\n",
            "Iteration: 26691; Percent complete: 89.0%; Average loss: 0.1587\n",
            "Iteration: 26692; Percent complete: 89.0%; Average loss: 0.1143\n",
            "Iteration: 26693; Percent complete: 89.0%; Average loss: 0.1388\n",
            "Iteration: 26694; Percent complete: 89.0%; Average loss: 0.0929\n",
            "Iteration: 26695; Percent complete: 89.0%; Average loss: 0.1197\n",
            "Iteration: 26696; Percent complete: 89.0%; Average loss: 0.1369\n",
            "Iteration: 26697; Percent complete: 89.0%; Average loss: 0.1194\n",
            "Iteration: 26698; Percent complete: 89.0%; Average loss: 0.0893\n",
            "Iteration: 26699; Percent complete: 89.0%; Average loss: 0.1133\n",
            "Iteration: 26700; Percent complete: 89.0%; Average loss: 0.1617\n",
            "Iteration: 26701; Percent complete: 89.0%; Average loss: 0.1103\n",
            "Iteration: 26702; Percent complete: 89.0%; Average loss: 0.1160\n",
            "Iteration: 26703; Percent complete: 89.0%; Average loss: 0.0971\n",
            "Iteration: 26704; Percent complete: 89.0%; Average loss: 0.0991\n",
            "Iteration: 26705; Percent complete: 89.0%; Average loss: 0.0968\n",
            "Iteration: 26706; Percent complete: 89.0%; Average loss: 0.0816\n",
            "Iteration: 26707; Percent complete: 89.0%; Average loss: 0.1573\n",
            "Iteration: 26708; Percent complete: 89.0%; Average loss: 0.1217\n",
            "Iteration: 26709; Percent complete: 89.0%; Average loss: 0.0736\n",
            "Iteration: 26710; Percent complete: 89.0%; Average loss: 0.1685\n",
            "Iteration: 26711; Percent complete: 89.0%; Average loss: 0.1457\n",
            "Iteration: 26712; Percent complete: 89.0%; Average loss: 0.1339\n",
            "Iteration: 26713; Percent complete: 89.0%; Average loss: 0.1064\n",
            "Iteration: 26714; Percent complete: 89.0%; Average loss: 0.1431\n",
            "Iteration: 26715; Percent complete: 89.0%; Average loss: 0.1115\n",
            "Iteration: 26716; Percent complete: 89.1%; Average loss: 0.1531\n",
            "Iteration: 26717; Percent complete: 89.1%; Average loss: 0.1419\n",
            "Iteration: 26718; Percent complete: 89.1%; Average loss: 0.1339\n",
            "Iteration: 26719; Percent complete: 89.1%; Average loss: 0.0786\n",
            "Iteration: 26720; Percent complete: 89.1%; Average loss: 0.1180\n",
            "Iteration: 26721; Percent complete: 89.1%; Average loss: 0.1345\n",
            "Iteration: 26722; Percent complete: 89.1%; Average loss: 0.1359\n",
            "Iteration: 26723; Percent complete: 89.1%; Average loss: 0.1114\n",
            "Iteration: 26724; Percent complete: 89.1%; Average loss: 0.1803\n",
            "Iteration: 26725; Percent complete: 89.1%; Average loss: 0.1281\n",
            "Iteration: 26726; Percent complete: 89.1%; Average loss: 0.1155\n",
            "Iteration: 26727; Percent complete: 89.1%; Average loss: 0.1169\n",
            "Iteration: 26728; Percent complete: 89.1%; Average loss: 0.0940\n",
            "Iteration: 26729; Percent complete: 89.1%; Average loss: 0.1663\n",
            "Iteration: 26730; Percent complete: 89.1%; Average loss: 0.1268\n",
            "Iteration: 26731; Percent complete: 89.1%; Average loss: 0.1084\n",
            "Iteration: 26732; Percent complete: 89.1%; Average loss: 0.1543\n",
            "Iteration: 26733; Percent complete: 89.1%; Average loss: 0.1229\n",
            "Iteration: 26734; Percent complete: 89.1%; Average loss: 0.1346\n",
            "Iteration: 26735; Percent complete: 89.1%; Average loss: 0.1232\n",
            "Iteration: 26736; Percent complete: 89.1%; Average loss: 0.1298\n",
            "Iteration: 26737; Percent complete: 89.1%; Average loss: 0.1141\n",
            "Iteration: 26738; Percent complete: 89.1%; Average loss: 0.1132\n",
            "Iteration: 26739; Percent complete: 89.1%; Average loss: 0.0886\n",
            "Iteration: 26740; Percent complete: 89.1%; Average loss: 0.0910\n",
            "Iteration: 26741; Percent complete: 89.1%; Average loss: 0.1707\n",
            "Iteration: 26742; Percent complete: 89.1%; Average loss: 0.1254\n",
            "Iteration: 26743; Percent complete: 89.1%; Average loss: 0.1413\n",
            "Iteration: 26744; Percent complete: 89.1%; Average loss: 0.1104\n",
            "Iteration: 26745; Percent complete: 89.1%; Average loss: 0.1376\n",
            "Iteration: 26746; Percent complete: 89.2%; Average loss: 0.1240\n",
            "Iteration: 26747; Percent complete: 89.2%; Average loss: 0.0692\n",
            "Iteration: 26748; Percent complete: 89.2%; Average loss: 0.1137\n",
            "Iteration: 26749; Percent complete: 89.2%; Average loss: 0.1188\n",
            "Iteration: 26750; Percent complete: 89.2%; Average loss: 0.1836\n",
            "Iteration: 26751; Percent complete: 89.2%; Average loss: 0.1169\n",
            "Iteration: 26752; Percent complete: 89.2%; Average loss: 0.1243\n",
            "Iteration: 26753; Percent complete: 89.2%; Average loss: 0.0803\n",
            "Iteration: 26754; Percent complete: 89.2%; Average loss: 0.2303\n",
            "Iteration: 26755; Percent complete: 89.2%; Average loss: 0.1059\n",
            "Iteration: 26756; Percent complete: 89.2%; Average loss: 0.1225\n",
            "Iteration: 26757; Percent complete: 89.2%; Average loss: 0.1239\n",
            "Iteration: 26758; Percent complete: 89.2%; Average loss: 0.1223\n",
            "Iteration: 26759; Percent complete: 89.2%; Average loss: 0.1123\n",
            "Iteration: 26760; Percent complete: 89.2%; Average loss: 0.1113\n",
            "Iteration: 26761; Percent complete: 89.2%; Average loss: 0.1185\n",
            "Iteration: 26762; Percent complete: 89.2%; Average loss: 0.1280\n",
            "Iteration: 26763; Percent complete: 89.2%; Average loss: 0.0881\n",
            "Iteration: 26764; Percent complete: 89.2%; Average loss: 0.1024\n",
            "Iteration: 26765; Percent complete: 89.2%; Average loss: 0.1216\n",
            "Iteration: 26766; Percent complete: 89.2%; Average loss: 0.1441\n",
            "Iteration: 26767; Percent complete: 89.2%; Average loss: 0.1266\n",
            "Iteration: 26768; Percent complete: 89.2%; Average loss: 0.1454\n",
            "Iteration: 26769; Percent complete: 89.2%; Average loss: 0.1273\n",
            "Iteration: 26770; Percent complete: 89.2%; Average loss: 0.0940\n",
            "Iteration: 26771; Percent complete: 89.2%; Average loss: 0.1118\n",
            "Iteration: 26772; Percent complete: 89.2%; Average loss: 0.1790\n",
            "Iteration: 26773; Percent complete: 89.2%; Average loss: 0.1359\n",
            "Iteration: 26774; Percent complete: 89.2%; Average loss: 0.1394\n",
            "Iteration: 26775; Percent complete: 89.2%; Average loss: 0.1321\n",
            "Iteration: 26776; Percent complete: 89.3%; Average loss: 0.1301\n",
            "Iteration: 26777; Percent complete: 89.3%; Average loss: 0.1195\n",
            "Iteration: 26778; Percent complete: 89.3%; Average loss: 0.1374\n",
            "Iteration: 26779; Percent complete: 89.3%; Average loss: 0.1130\n",
            "Iteration: 26780; Percent complete: 89.3%; Average loss: 0.1104\n",
            "Iteration: 26781; Percent complete: 89.3%; Average loss: 0.1039\n",
            "Iteration: 26782; Percent complete: 89.3%; Average loss: 0.1302\n",
            "Iteration: 26783; Percent complete: 89.3%; Average loss: 0.1342\n",
            "Iteration: 26784; Percent complete: 89.3%; Average loss: 0.1337\n",
            "Iteration: 26785; Percent complete: 89.3%; Average loss: 0.1125\n",
            "Iteration: 26786; Percent complete: 89.3%; Average loss: 0.1405\n",
            "Iteration: 26787; Percent complete: 89.3%; Average loss: 0.0931\n",
            "Iteration: 26788; Percent complete: 89.3%; Average loss: 0.1341\n",
            "Iteration: 26789; Percent complete: 89.3%; Average loss: 0.1355\n",
            "Iteration: 26790; Percent complete: 89.3%; Average loss: 0.1332\n",
            "Iteration: 26791; Percent complete: 89.3%; Average loss: 0.0928\n",
            "Iteration: 26792; Percent complete: 89.3%; Average loss: 0.1348\n",
            "Iteration: 26793; Percent complete: 89.3%; Average loss: 0.1339\n",
            "Iteration: 26794; Percent complete: 89.3%; Average loss: 0.1084\n",
            "Iteration: 26795; Percent complete: 89.3%; Average loss: 0.1764\n",
            "Iteration: 26796; Percent complete: 89.3%; Average loss: 0.1175\n",
            "Iteration: 26797; Percent complete: 89.3%; Average loss: 0.1087\n",
            "Iteration: 26798; Percent complete: 89.3%; Average loss: 0.1252\n",
            "Iteration: 26799; Percent complete: 89.3%; Average loss: 0.1464\n",
            "Iteration: 26800; Percent complete: 89.3%; Average loss: 0.1309\n",
            "Iteration: 26801; Percent complete: 89.3%; Average loss: 0.1202\n",
            "Iteration: 26802; Percent complete: 89.3%; Average loss: 0.1190\n",
            "Iteration: 26803; Percent complete: 89.3%; Average loss: 0.1028\n",
            "Iteration: 26804; Percent complete: 89.3%; Average loss: 0.1232\n",
            "Iteration: 26805; Percent complete: 89.3%; Average loss: 0.1153\n",
            "Iteration: 26806; Percent complete: 89.4%; Average loss: 0.1388\n",
            "Iteration: 26807; Percent complete: 89.4%; Average loss: 0.0931\n",
            "Iteration: 26808; Percent complete: 89.4%; Average loss: 0.1406\n",
            "Iteration: 26809; Percent complete: 89.4%; Average loss: 0.1177\n",
            "Iteration: 26810; Percent complete: 89.4%; Average loss: 0.1335\n",
            "Iteration: 26811; Percent complete: 89.4%; Average loss: 0.1258\n",
            "Iteration: 26812; Percent complete: 89.4%; Average loss: 0.1267\n",
            "Iteration: 26813; Percent complete: 89.4%; Average loss: 0.0913\n",
            "Iteration: 26814; Percent complete: 89.4%; Average loss: 0.1382\n",
            "Iteration: 26815; Percent complete: 89.4%; Average loss: 0.1092\n",
            "Iteration: 26816; Percent complete: 89.4%; Average loss: 0.1293\n",
            "Iteration: 26817; Percent complete: 89.4%; Average loss: 0.1682\n",
            "Iteration: 26818; Percent complete: 89.4%; Average loss: 0.1766\n",
            "Iteration: 26819; Percent complete: 89.4%; Average loss: 0.0854\n",
            "Iteration: 26820; Percent complete: 89.4%; Average loss: 0.1244\n",
            "Iteration: 26821; Percent complete: 89.4%; Average loss: 0.1330\n",
            "Iteration: 26822; Percent complete: 89.4%; Average loss: 0.1279\n",
            "Iteration: 26823; Percent complete: 89.4%; Average loss: 0.1261\n",
            "Iteration: 26824; Percent complete: 89.4%; Average loss: 0.1074\n",
            "Iteration: 26825; Percent complete: 89.4%; Average loss: 0.0909\n",
            "Iteration: 26826; Percent complete: 89.4%; Average loss: 0.1973\n",
            "Iteration: 26827; Percent complete: 89.4%; Average loss: 0.1348\n",
            "Iteration: 26828; Percent complete: 89.4%; Average loss: 0.1062\n",
            "Iteration: 26829; Percent complete: 89.4%; Average loss: 0.0705\n",
            "Iteration: 26830; Percent complete: 89.4%; Average loss: 0.0988\n",
            "Iteration: 26831; Percent complete: 89.4%; Average loss: 0.1099\n",
            "Iteration: 26832; Percent complete: 89.4%; Average loss: 0.1130\n",
            "Iteration: 26833; Percent complete: 89.4%; Average loss: 0.1437\n",
            "Iteration: 26834; Percent complete: 89.4%; Average loss: 0.1221\n",
            "Iteration: 26835; Percent complete: 89.5%; Average loss: 0.2001\n",
            "Iteration: 26836; Percent complete: 89.5%; Average loss: 0.1216\n",
            "Iteration: 26837; Percent complete: 89.5%; Average loss: 0.1195\n",
            "Iteration: 26838; Percent complete: 89.5%; Average loss: 0.1197\n",
            "Iteration: 26839; Percent complete: 89.5%; Average loss: 0.1265\n",
            "Iteration: 26840; Percent complete: 89.5%; Average loss: 0.1275\n",
            "Iteration: 26841; Percent complete: 89.5%; Average loss: 0.0834\n",
            "Iteration: 26842; Percent complete: 89.5%; Average loss: 0.1389\n",
            "Iteration: 26843; Percent complete: 89.5%; Average loss: 0.1505\n",
            "Iteration: 26844; Percent complete: 89.5%; Average loss: 0.1427\n",
            "Iteration: 26845; Percent complete: 89.5%; Average loss: 0.1062\n",
            "Iteration: 26846; Percent complete: 89.5%; Average loss: 0.1874\n",
            "Iteration: 26847; Percent complete: 89.5%; Average loss: 0.1111\n",
            "Iteration: 26848; Percent complete: 89.5%; Average loss: 0.1468\n",
            "Iteration: 26849; Percent complete: 89.5%; Average loss: 0.0763\n",
            "Iteration: 26850; Percent complete: 89.5%; Average loss: 0.1427\n",
            "Iteration: 26851; Percent complete: 89.5%; Average loss: 0.1043\n",
            "Iteration: 26852; Percent complete: 89.5%; Average loss: 0.1406\n",
            "Iteration: 26853; Percent complete: 89.5%; Average loss: 0.1402\n",
            "Iteration: 26854; Percent complete: 89.5%; Average loss: 0.1141\n",
            "Iteration: 26855; Percent complete: 89.5%; Average loss: 0.1212\n",
            "Iteration: 26856; Percent complete: 89.5%; Average loss: 0.1053\n",
            "Iteration: 26857; Percent complete: 89.5%; Average loss: 0.1495\n",
            "Iteration: 26858; Percent complete: 89.5%; Average loss: 0.1021\n",
            "Iteration: 26859; Percent complete: 89.5%; Average loss: 0.1106\n",
            "Iteration: 26860; Percent complete: 89.5%; Average loss: 0.1433\n",
            "Iteration: 26861; Percent complete: 89.5%; Average loss: 0.1147\n",
            "Iteration: 26862; Percent complete: 89.5%; Average loss: 0.0938\n",
            "Iteration: 26863; Percent complete: 89.5%; Average loss: 0.1127\n",
            "Iteration: 26864; Percent complete: 89.5%; Average loss: 0.1280\n",
            "Iteration: 26865; Percent complete: 89.5%; Average loss: 0.0862\n",
            "Iteration: 26866; Percent complete: 89.6%; Average loss: 0.0863\n",
            "Iteration: 26867; Percent complete: 89.6%; Average loss: 0.1074\n",
            "Iteration: 26868; Percent complete: 89.6%; Average loss: 0.0973\n",
            "Iteration: 26869; Percent complete: 89.6%; Average loss: 0.1042\n",
            "Iteration: 26870; Percent complete: 89.6%; Average loss: 0.0915\n",
            "Iteration: 26871; Percent complete: 89.6%; Average loss: 0.1501\n",
            "Iteration: 26872; Percent complete: 89.6%; Average loss: 0.1157\n",
            "Iteration: 26873; Percent complete: 89.6%; Average loss: 0.1719\n",
            "Iteration: 26874; Percent complete: 89.6%; Average loss: 0.1001\n",
            "Iteration: 26875; Percent complete: 89.6%; Average loss: 0.1616\n",
            "Iteration: 26876; Percent complete: 89.6%; Average loss: 0.0926\n",
            "Iteration: 26877; Percent complete: 89.6%; Average loss: 0.1027\n",
            "Iteration: 26878; Percent complete: 89.6%; Average loss: 0.1090\n",
            "Iteration: 26879; Percent complete: 89.6%; Average loss: 0.1016\n",
            "Iteration: 26880; Percent complete: 89.6%; Average loss: 0.1459\n",
            "Iteration: 26881; Percent complete: 89.6%; Average loss: 0.0903\n",
            "Iteration: 26882; Percent complete: 89.6%; Average loss: 0.1481\n",
            "Iteration: 26883; Percent complete: 89.6%; Average loss: 0.1260\n",
            "Iteration: 26884; Percent complete: 89.6%; Average loss: 0.1436\n",
            "Iteration: 26885; Percent complete: 89.6%; Average loss: 0.1096\n",
            "Iteration: 26886; Percent complete: 89.6%; Average loss: 0.1962\n",
            "Iteration: 26887; Percent complete: 89.6%; Average loss: 0.0975\n",
            "Iteration: 26888; Percent complete: 89.6%; Average loss: 0.1008\n",
            "Iteration: 26889; Percent complete: 89.6%; Average loss: 0.0971\n",
            "Iteration: 26890; Percent complete: 89.6%; Average loss: 0.1093\n",
            "Iteration: 26891; Percent complete: 89.6%; Average loss: 0.0906\n",
            "Iteration: 26892; Percent complete: 89.6%; Average loss: 0.0981\n",
            "Iteration: 26893; Percent complete: 89.6%; Average loss: 0.0880\n",
            "Iteration: 26894; Percent complete: 89.6%; Average loss: 0.1371\n",
            "Iteration: 26895; Percent complete: 89.6%; Average loss: 0.1304\n",
            "Iteration: 26896; Percent complete: 89.7%; Average loss: 0.1262\n",
            "Iteration: 26897; Percent complete: 89.7%; Average loss: 0.1372\n",
            "Iteration: 26898; Percent complete: 89.7%; Average loss: 0.1301\n",
            "Iteration: 26899; Percent complete: 89.7%; Average loss: 0.1259\n",
            "Iteration: 26900; Percent complete: 89.7%; Average loss: 0.1105\n",
            "Iteration: 26901; Percent complete: 89.7%; Average loss: 0.1306\n",
            "Iteration: 26902; Percent complete: 89.7%; Average loss: 0.1099\n",
            "Iteration: 26903; Percent complete: 89.7%; Average loss: 0.1012\n",
            "Iteration: 26904; Percent complete: 89.7%; Average loss: 0.1424\n",
            "Iteration: 26905; Percent complete: 89.7%; Average loss: 0.1040\n",
            "Iteration: 26906; Percent complete: 89.7%; Average loss: 0.1005\n",
            "Iteration: 26907; Percent complete: 89.7%; Average loss: 0.1866\n",
            "Iteration: 26908; Percent complete: 89.7%; Average loss: 0.1094\n",
            "Iteration: 26909; Percent complete: 89.7%; Average loss: 0.1122\n",
            "Iteration: 26910; Percent complete: 89.7%; Average loss: 0.1386\n",
            "Iteration: 26911; Percent complete: 89.7%; Average loss: 0.1113\n",
            "Iteration: 26912; Percent complete: 89.7%; Average loss: 0.0958\n",
            "Iteration: 26913; Percent complete: 89.7%; Average loss: 0.0935\n",
            "Iteration: 26914; Percent complete: 89.7%; Average loss: 0.1250\n",
            "Iteration: 26915; Percent complete: 89.7%; Average loss: 0.1100\n",
            "Iteration: 26916; Percent complete: 89.7%; Average loss: 0.0849\n",
            "Iteration: 26917; Percent complete: 89.7%; Average loss: 0.1512\n",
            "Iteration: 26918; Percent complete: 89.7%; Average loss: 0.1533\n",
            "Iteration: 26919; Percent complete: 89.7%; Average loss: 0.1336\n",
            "Iteration: 26920; Percent complete: 89.7%; Average loss: 0.1514\n",
            "Iteration: 26921; Percent complete: 89.7%; Average loss: 0.1546\n",
            "Iteration: 26922; Percent complete: 89.7%; Average loss: 0.1598\n",
            "Iteration: 26923; Percent complete: 89.7%; Average loss: 0.1432\n",
            "Iteration: 26924; Percent complete: 89.7%; Average loss: 0.1365\n",
            "Iteration: 26925; Percent complete: 89.8%; Average loss: 0.1466\n",
            "Iteration: 26926; Percent complete: 89.8%; Average loss: 0.0935\n",
            "Iteration: 26927; Percent complete: 89.8%; Average loss: 0.1719\n",
            "Iteration: 26928; Percent complete: 89.8%; Average loss: 0.1278\n",
            "Iteration: 26929; Percent complete: 89.8%; Average loss: 0.1377\n",
            "Iteration: 26930; Percent complete: 89.8%; Average loss: 0.1484\n",
            "Iteration: 26931; Percent complete: 89.8%; Average loss: 0.1511\n",
            "Iteration: 26932; Percent complete: 89.8%; Average loss: 0.1396\n",
            "Iteration: 26933; Percent complete: 89.8%; Average loss: 0.1152\n",
            "Iteration: 26934; Percent complete: 89.8%; Average loss: 0.1054\n",
            "Iteration: 26935; Percent complete: 89.8%; Average loss: 0.1633\n",
            "Iteration: 26936; Percent complete: 89.8%; Average loss: 0.1040\n",
            "Iteration: 26937; Percent complete: 89.8%; Average loss: 0.1554\n",
            "Iteration: 26938; Percent complete: 89.8%; Average loss: 0.1552\n",
            "Iteration: 26939; Percent complete: 89.8%; Average loss: 0.1990\n",
            "Iteration: 26940; Percent complete: 89.8%; Average loss: 0.1035\n",
            "Iteration: 26941; Percent complete: 89.8%; Average loss: 0.1266\n",
            "Iteration: 26942; Percent complete: 89.8%; Average loss: 0.1045\n",
            "Iteration: 26943; Percent complete: 89.8%; Average loss: 0.1157\n",
            "Iteration: 26944; Percent complete: 89.8%; Average loss: 0.1307\n",
            "Iteration: 26945; Percent complete: 89.8%; Average loss: 0.1287\n",
            "Iteration: 26946; Percent complete: 89.8%; Average loss: 0.1316\n",
            "Iteration: 26947; Percent complete: 89.8%; Average loss: 0.1067\n",
            "Iteration: 26948; Percent complete: 89.8%; Average loss: 0.1317\n",
            "Iteration: 26949; Percent complete: 89.8%; Average loss: 0.1261\n",
            "Iteration: 26950; Percent complete: 89.8%; Average loss: 0.1792\n",
            "Iteration: 26951; Percent complete: 89.8%; Average loss: 0.0973\n",
            "Iteration: 26952; Percent complete: 89.8%; Average loss: 0.1324\n",
            "Iteration: 26953; Percent complete: 89.8%; Average loss: 0.1135\n",
            "Iteration: 26954; Percent complete: 89.8%; Average loss: 0.1321\n",
            "Iteration: 26955; Percent complete: 89.8%; Average loss: 0.1225\n",
            "Iteration: 26956; Percent complete: 89.9%; Average loss: 0.2194\n",
            "Iteration: 26957; Percent complete: 89.9%; Average loss: 0.1004\n",
            "Iteration: 26958; Percent complete: 89.9%; Average loss: 0.1126\n",
            "Iteration: 26959; Percent complete: 89.9%; Average loss: 0.1134\n",
            "Iteration: 26960; Percent complete: 89.9%; Average loss: 0.1720\n",
            "Iteration: 26961; Percent complete: 89.9%; Average loss: 0.1085\n",
            "Iteration: 26962; Percent complete: 89.9%; Average loss: 0.1352\n",
            "Iteration: 26963; Percent complete: 89.9%; Average loss: 0.1398\n",
            "Iteration: 26964; Percent complete: 89.9%; Average loss: 0.1033\n",
            "Iteration: 26965; Percent complete: 89.9%; Average loss: 0.1029\n",
            "Iteration: 26966; Percent complete: 89.9%; Average loss: 0.0924\n",
            "Iteration: 26967; Percent complete: 89.9%; Average loss: 0.1032\n",
            "Iteration: 26968; Percent complete: 89.9%; Average loss: 0.1403\n",
            "Iteration: 26969; Percent complete: 89.9%; Average loss: 0.1301\n",
            "Iteration: 26970; Percent complete: 89.9%; Average loss: 0.1764\n",
            "Iteration: 26971; Percent complete: 89.9%; Average loss: 0.1521\n",
            "Iteration: 26972; Percent complete: 89.9%; Average loss: 0.1087\n",
            "Iteration: 26973; Percent complete: 89.9%; Average loss: 0.1045\n",
            "Iteration: 26974; Percent complete: 89.9%; Average loss: 0.1110\n",
            "Iteration: 26975; Percent complete: 89.9%; Average loss: 0.1592\n",
            "Iteration: 26976; Percent complete: 89.9%; Average loss: 0.1395\n",
            "Iteration: 26977; Percent complete: 89.9%; Average loss: 0.0970\n",
            "Iteration: 26978; Percent complete: 89.9%; Average loss: 0.1036\n",
            "Iteration: 26979; Percent complete: 89.9%; Average loss: 0.1089\n",
            "Iteration: 26980; Percent complete: 89.9%; Average loss: 0.1380\n",
            "Iteration: 26981; Percent complete: 89.9%; Average loss: 0.1413\n",
            "Iteration: 26982; Percent complete: 89.9%; Average loss: 0.1151\n",
            "Iteration: 26983; Percent complete: 89.9%; Average loss: 0.1330\n",
            "Iteration: 26984; Percent complete: 89.9%; Average loss: 0.1198\n",
            "Iteration: 26985; Percent complete: 90.0%; Average loss: 0.1169\n",
            "Iteration: 26986; Percent complete: 90.0%; Average loss: 0.1377\n",
            "Iteration: 26987; Percent complete: 90.0%; Average loss: 0.2177\n",
            "Iteration: 26988; Percent complete: 90.0%; Average loss: 0.1039\n",
            "Iteration: 26989; Percent complete: 90.0%; Average loss: 0.1249\n",
            "Iteration: 26990; Percent complete: 90.0%; Average loss: 0.1436\n",
            "Iteration: 26991; Percent complete: 90.0%; Average loss: 0.1589\n",
            "Iteration: 26992; Percent complete: 90.0%; Average loss: 0.1125\n",
            "Iteration: 26993; Percent complete: 90.0%; Average loss: 0.0918\n",
            "Iteration: 26994; Percent complete: 90.0%; Average loss: 0.1527\n",
            "Iteration: 26995; Percent complete: 90.0%; Average loss: 0.1386\n",
            "Iteration: 26996; Percent complete: 90.0%; Average loss: 0.1514\n",
            "Iteration: 26997; Percent complete: 90.0%; Average loss: 0.1569\n",
            "Iteration: 26998; Percent complete: 90.0%; Average loss: 0.1314\n",
            "Iteration: 26999; Percent complete: 90.0%; Average loss: 0.1521\n",
            "Iteration: 27000; Percent complete: 90.0%; Average loss: 0.1408\n",
            "Iteration: 27001; Percent complete: 90.0%; Average loss: 0.1369\n",
            "Iteration: 27002; Percent complete: 90.0%; Average loss: 0.1172\n",
            "Iteration: 27003; Percent complete: 90.0%; Average loss: 0.1196\n",
            "Iteration: 27004; Percent complete: 90.0%; Average loss: 0.1530\n",
            "Iteration: 27005; Percent complete: 90.0%; Average loss: 0.0806\n",
            "Iteration: 27006; Percent complete: 90.0%; Average loss: 0.1404\n",
            "Iteration: 27007; Percent complete: 90.0%; Average loss: 0.1507\n",
            "Iteration: 27008; Percent complete: 90.0%; Average loss: 0.1372\n",
            "Iteration: 27009; Percent complete: 90.0%; Average loss: 0.1742\n",
            "Iteration: 27010; Percent complete: 90.0%; Average loss: 0.1138\n",
            "Iteration: 27011; Percent complete: 90.0%; Average loss: 0.1353\n",
            "Iteration: 27012; Percent complete: 90.0%; Average loss: 0.1170\n",
            "Iteration: 27013; Percent complete: 90.0%; Average loss: 0.1137\n",
            "Iteration: 27014; Percent complete: 90.0%; Average loss: 0.1241\n",
            "Iteration: 27015; Percent complete: 90.0%; Average loss: 0.1320\n",
            "Iteration: 27016; Percent complete: 90.1%; Average loss: 0.1090\n",
            "Iteration: 27017; Percent complete: 90.1%; Average loss: 0.1229\n",
            "Iteration: 27018; Percent complete: 90.1%; Average loss: 0.1291\n",
            "Iteration: 27019; Percent complete: 90.1%; Average loss: 0.0828\n",
            "Iteration: 27020; Percent complete: 90.1%; Average loss: 0.1111\n",
            "Iteration: 27021; Percent complete: 90.1%; Average loss: 0.1824\n",
            "Iteration: 27022; Percent complete: 90.1%; Average loss: 0.1356\n",
            "Iteration: 27023; Percent complete: 90.1%; Average loss: 0.1581\n",
            "Iteration: 27024; Percent complete: 90.1%; Average loss: 0.1227\n",
            "Iteration: 27025; Percent complete: 90.1%; Average loss: 0.1358\n",
            "Iteration: 27026; Percent complete: 90.1%; Average loss: 0.0855\n",
            "Iteration: 27027; Percent complete: 90.1%; Average loss: 0.1020\n",
            "Iteration: 27028; Percent complete: 90.1%; Average loss: 0.1247\n",
            "Iteration: 27029; Percent complete: 90.1%; Average loss: 0.1226\n",
            "Iteration: 27030; Percent complete: 90.1%; Average loss: 0.1130\n",
            "Iteration: 27031; Percent complete: 90.1%; Average loss: 0.1321\n",
            "Iteration: 27032; Percent complete: 90.1%; Average loss: 0.1746\n",
            "Iteration: 27033; Percent complete: 90.1%; Average loss: 0.0837\n",
            "Iteration: 27034; Percent complete: 90.1%; Average loss: 0.1856\n",
            "Iteration: 27035; Percent complete: 90.1%; Average loss: 0.1144\n",
            "Iteration: 27036; Percent complete: 90.1%; Average loss: 0.2015\n",
            "Iteration: 27037; Percent complete: 90.1%; Average loss: 0.1484\n",
            "Iteration: 27038; Percent complete: 90.1%; Average loss: 0.1594\n",
            "Iteration: 27039; Percent complete: 90.1%; Average loss: 0.1241\n",
            "Iteration: 27040; Percent complete: 90.1%; Average loss: 0.1130\n",
            "Iteration: 27041; Percent complete: 90.1%; Average loss: 0.1562\n",
            "Iteration: 27042; Percent complete: 90.1%; Average loss: 0.1112\n",
            "Iteration: 27043; Percent complete: 90.1%; Average loss: 0.1667\n",
            "Iteration: 27044; Percent complete: 90.1%; Average loss: 0.1224\n",
            "Iteration: 27045; Percent complete: 90.1%; Average loss: 0.1206\n",
            "Iteration: 27046; Percent complete: 90.2%; Average loss: 0.1335\n",
            "Iteration: 27047; Percent complete: 90.2%; Average loss: 0.1161\n",
            "Iteration: 27048; Percent complete: 90.2%; Average loss: 0.2024\n",
            "Iteration: 27049; Percent complete: 90.2%; Average loss: 0.1815\n",
            "Iteration: 27050; Percent complete: 90.2%; Average loss: 0.1254\n",
            "Iteration: 27051; Percent complete: 90.2%; Average loss: 0.1195\n",
            "Iteration: 27052; Percent complete: 90.2%; Average loss: 0.1052\n",
            "Iteration: 27053; Percent complete: 90.2%; Average loss: 0.1046\n",
            "Iteration: 27054; Percent complete: 90.2%; Average loss: 0.0652\n",
            "Iteration: 27055; Percent complete: 90.2%; Average loss: 0.1531\n",
            "Iteration: 27056; Percent complete: 90.2%; Average loss: 0.1774\n",
            "Iteration: 27057; Percent complete: 90.2%; Average loss: 0.1178\n",
            "Iteration: 27058; Percent complete: 90.2%; Average loss: 0.1199\n",
            "Iteration: 27059; Percent complete: 90.2%; Average loss: 0.1363\n",
            "Iteration: 27060; Percent complete: 90.2%; Average loss: 0.1211\n",
            "Iteration: 27061; Percent complete: 90.2%; Average loss: 0.1605\n",
            "Iteration: 27062; Percent complete: 90.2%; Average loss: 0.1340\n",
            "Iteration: 27063; Percent complete: 90.2%; Average loss: 0.1850\n",
            "Iteration: 27064; Percent complete: 90.2%; Average loss: 0.1341\n",
            "Iteration: 27065; Percent complete: 90.2%; Average loss: 0.1272\n",
            "Iteration: 27066; Percent complete: 90.2%; Average loss: 0.1192\n",
            "Iteration: 27067; Percent complete: 90.2%; Average loss: 0.1141\n",
            "Iteration: 27068; Percent complete: 90.2%; Average loss: 0.0899\n",
            "Iteration: 27069; Percent complete: 90.2%; Average loss: 0.1444\n",
            "Iteration: 27070; Percent complete: 90.2%; Average loss: 0.1139\n",
            "Iteration: 27071; Percent complete: 90.2%; Average loss: 0.1636\n",
            "Iteration: 27072; Percent complete: 90.2%; Average loss: 0.1253\n",
            "Iteration: 27073; Percent complete: 90.2%; Average loss: 0.1095\n",
            "Iteration: 27074; Percent complete: 90.2%; Average loss: 0.1643\n",
            "Iteration: 27075; Percent complete: 90.2%; Average loss: 0.1320\n",
            "Iteration: 27076; Percent complete: 90.3%; Average loss: 0.1167\n",
            "Iteration: 27077; Percent complete: 90.3%; Average loss: 0.1150\n",
            "Iteration: 27078; Percent complete: 90.3%; Average loss: 0.1335\n",
            "Iteration: 27079; Percent complete: 90.3%; Average loss: 0.1174\n",
            "Iteration: 27080; Percent complete: 90.3%; Average loss: 0.1223\n",
            "Iteration: 27081; Percent complete: 90.3%; Average loss: 0.1346\n",
            "Iteration: 27082; Percent complete: 90.3%; Average loss: 0.1613\n",
            "Iteration: 27083; Percent complete: 90.3%; Average loss: 0.1968\n",
            "Iteration: 27084; Percent complete: 90.3%; Average loss: 0.1539\n",
            "Iteration: 27085; Percent complete: 90.3%; Average loss: 0.1565\n",
            "Iteration: 27086; Percent complete: 90.3%; Average loss: 0.1415\n",
            "Iteration: 27087; Percent complete: 90.3%; Average loss: 0.1603\n",
            "Iteration: 27088; Percent complete: 90.3%; Average loss: 0.0733\n",
            "Iteration: 27089; Percent complete: 90.3%; Average loss: 0.1424\n",
            "Iteration: 27090; Percent complete: 90.3%; Average loss: 0.1057\n",
            "Iteration: 27091; Percent complete: 90.3%; Average loss: 0.1532\n",
            "Iteration: 27092; Percent complete: 90.3%; Average loss: 0.1085\n",
            "Iteration: 27093; Percent complete: 90.3%; Average loss: 0.1450\n",
            "Iteration: 27094; Percent complete: 90.3%; Average loss: 0.1672\n",
            "Iteration: 27095; Percent complete: 90.3%; Average loss: 0.0816\n",
            "Iteration: 27096; Percent complete: 90.3%; Average loss: 0.1380\n",
            "Iteration: 27097; Percent complete: 90.3%; Average loss: 0.1079\n",
            "Iteration: 27098; Percent complete: 90.3%; Average loss: 0.1257\n",
            "Iteration: 27099; Percent complete: 90.3%; Average loss: 0.1445\n",
            "Iteration: 27100; Percent complete: 90.3%; Average loss: 0.1101\n",
            "Iteration: 27101; Percent complete: 90.3%; Average loss: 0.1100\n",
            "Iteration: 27102; Percent complete: 90.3%; Average loss: 0.1139\n",
            "Iteration: 27103; Percent complete: 90.3%; Average loss: 0.1676\n",
            "Iteration: 27104; Percent complete: 90.3%; Average loss: 0.1091\n",
            "Iteration: 27105; Percent complete: 90.3%; Average loss: 0.1111\n",
            "Iteration: 27106; Percent complete: 90.4%; Average loss: 0.1028\n",
            "Iteration: 27107; Percent complete: 90.4%; Average loss: 0.1263\n",
            "Iteration: 27108; Percent complete: 90.4%; Average loss: 0.0933\n",
            "Iteration: 27109; Percent complete: 90.4%; Average loss: 0.1115\n",
            "Iteration: 27110; Percent complete: 90.4%; Average loss: 0.0962\n",
            "Iteration: 27111; Percent complete: 90.4%; Average loss: 0.1394\n",
            "Iteration: 27112; Percent complete: 90.4%; Average loss: 0.1892\n",
            "Iteration: 27113; Percent complete: 90.4%; Average loss: 0.1329\n",
            "Iteration: 27114; Percent complete: 90.4%; Average loss: 0.0956\n",
            "Iteration: 27115; Percent complete: 90.4%; Average loss: 0.1284\n",
            "Iteration: 27116; Percent complete: 90.4%; Average loss: 0.1223\n",
            "Iteration: 27117; Percent complete: 90.4%; Average loss: 0.1766\n",
            "Iteration: 27118; Percent complete: 90.4%; Average loss: 0.1685\n",
            "Iteration: 27119; Percent complete: 90.4%; Average loss: 0.0922\n",
            "Iteration: 27120; Percent complete: 90.4%; Average loss: 0.1388\n",
            "Iteration: 27121; Percent complete: 90.4%; Average loss: 0.1621\n",
            "Iteration: 27122; Percent complete: 90.4%; Average loss: 0.1358\n",
            "Iteration: 27123; Percent complete: 90.4%; Average loss: 0.0901\n",
            "Iteration: 27124; Percent complete: 90.4%; Average loss: 0.1482\n",
            "Iteration: 27125; Percent complete: 90.4%; Average loss: 0.1500\n",
            "Iteration: 27126; Percent complete: 90.4%; Average loss: 0.1157\n",
            "Iteration: 27127; Percent complete: 90.4%; Average loss: 0.1454\n",
            "Iteration: 27128; Percent complete: 90.4%; Average loss: 0.1018\n",
            "Iteration: 27129; Percent complete: 90.4%; Average loss: 0.1151\n",
            "Iteration: 27130; Percent complete: 90.4%; Average loss: 0.1141\n",
            "Iteration: 27131; Percent complete: 90.4%; Average loss: 0.1841\n",
            "Iteration: 27132; Percent complete: 90.4%; Average loss: 0.1500\n",
            "Iteration: 27133; Percent complete: 90.4%; Average loss: 0.1697\n",
            "Iteration: 27134; Percent complete: 90.4%; Average loss: 0.1397\n",
            "Iteration: 27135; Percent complete: 90.5%; Average loss: 0.1557\n",
            "Iteration: 27136; Percent complete: 90.5%; Average loss: 0.1164\n",
            "Iteration: 27137; Percent complete: 90.5%; Average loss: 0.1346\n",
            "Iteration: 27138; Percent complete: 90.5%; Average loss: 0.0752\n",
            "Iteration: 27139; Percent complete: 90.5%; Average loss: 0.1220\n",
            "Iteration: 27140; Percent complete: 90.5%; Average loss: 0.1121\n",
            "Iteration: 27141; Percent complete: 90.5%; Average loss: 0.1129\n",
            "Iteration: 27142; Percent complete: 90.5%; Average loss: 0.0797\n",
            "Iteration: 27143; Percent complete: 90.5%; Average loss: 0.1134\n",
            "Iteration: 27144; Percent complete: 90.5%; Average loss: 0.1251\n",
            "Iteration: 27145; Percent complete: 90.5%; Average loss: 0.1128\n",
            "Iteration: 27146; Percent complete: 90.5%; Average loss: 0.1266\n",
            "Iteration: 27147; Percent complete: 90.5%; Average loss: 0.1448\n",
            "Iteration: 27148; Percent complete: 90.5%; Average loss: 0.1463\n",
            "Iteration: 27149; Percent complete: 90.5%; Average loss: 0.1243\n",
            "Iteration: 27150; Percent complete: 90.5%; Average loss: 0.1082\n",
            "Iteration: 27151; Percent complete: 90.5%; Average loss: 0.0959\n",
            "Iteration: 27152; Percent complete: 90.5%; Average loss: 0.1924\n",
            "Iteration: 27153; Percent complete: 90.5%; Average loss: 0.1016\n",
            "Iteration: 27154; Percent complete: 90.5%; Average loss: 0.1105\n",
            "Iteration: 27155; Percent complete: 90.5%; Average loss: 0.1723\n",
            "Iteration: 27156; Percent complete: 90.5%; Average loss: 0.1403\n",
            "Iteration: 27157; Percent complete: 90.5%; Average loss: 0.1422\n",
            "Iteration: 27158; Percent complete: 90.5%; Average loss: 0.1347\n",
            "Iteration: 27159; Percent complete: 90.5%; Average loss: 0.1223\n",
            "Iteration: 27160; Percent complete: 90.5%; Average loss: 0.1577\n",
            "Iteration: 27161; Percent complete: 90.5%; Average loss: 0.1248\n",
            "Iteration: 27162; Percent complete: 90.5%; Average loss: 0.1140\n",
            "Iteration: 27163; Percent complete: 90.5%; Average loss: 0.1030\n",
            "Iteration: 27164; Percent complete: 90.5%; Average loss: 0.1177\n",
            "Iteration: 27165; Percent complete: 90.5%; Average loss: 0.1119\n",
            "Iteration: 27166; Percent complete: 90.6%; Average loss: 0.1579\n",
            "Iteration: 27167; Percent complete: 90.6%; Average loss: 0.1379\n",
            "Iteration: 27168; Percent complete: 90.6%; Average loss: 0.1198\n",
            "Iteration: 27169; Percent complete: 90.6%; Average loss: 0.1094\n",
            "Iteration: 27170; Percent complete: 90.6%; Average loss: 0.1413\n",
            "Iteration: 27171; Percent complete: 90.6%; Average loss: 0.1362\n",
            "Iteration: 27172; Percent complete: 90.6%; Average loss: 0.1124\n",
            "Iteration: 27173; Percent complete: 90.6%; Average loss: 0.1413\n",
            "Iteration: 27174; Percent complete: 90.6%; Average loss: 0.1275\n",
            "Iteration: 27175; Percent complete: 90.6%; Average loss: 0.1471\n",
            "Iteration: 27176; Percent complete: 90.6%; Average loss: 0.1035\n",
            "Iteration: 27177; Percent complete: 90.6%; Average loss: 0.1589\n",
            "Iteration: 27178; Percent complete: 90.6%; Average loss: 0.0785\n",
            "Iteration: 27179; Percent complete: 90.6%; Average loss: 0.0878\n",
            "Iteration: 27180; Percent complete: 90.6%; Average loss: 0.1228\n",
            "Iteration: 27181; Percent complete: 90.6%; Average loss: 0.0939\n",
            "Iteration: 27182; Percent complete: 90.6%; Average loss: 0.1432\n",
            "Iteration: 27183; Percent complete: 90.6%; Average loss: 0.1083\n",
            "Iteration: 27184; Percent complete: 90.6%; Average loss: 0.1621\n",
            "Iteration: 27185; Percent complete: 90.6%; Average loss: 0.0984\n",
            "Iteration: 27186; Percent complete: 90.6%; Average loss: 0.1158\n",
            "Iteration: 27187; Percent complete: 90.6%; Average loss: 0.1312\n",
            "Iteration: 27188; Percent complete: 90.6%; Average loss: 0.1106\n",
            "Iteration: 27189; Percent complete: 90.6%; Average loss: 0.0960\n",
            "Iteration: 27190; Percent complete: 90.6%; Average loss: 0.1211\n",
            "Iteration: 27191; Percent complete: 90.6%; Average loss: 0.1520\n",
            "Iteration: 27192; Percent complete: 90.6%; Average loss: 0.1109\n",
            "Iteration: 27193; Percent complete: 90.6%; Average loss: 0.1481\n",
            "Iteration: 27194; Percent complete: 90.6%; Average loss: 0.0909\n",
            "Iteration: 27195; Percent complete: 90.6%; Average loss: 0.1456\n",
            "Iteration: 27196; Percent complete: 90.7%; Average loss: 0.1003\n",
            "Iteration: 27197; Percent complete: 90.7%; Average loss: 0.1167\n",
            "Iteration: 27198; Percent complete: 90.7%; Average loss: 0.1388\n",
            "Iteration: 27199; Percent complete: 90.7%; Average loss: 0.1488\n",
            "Iteration: 27200; Percent complete: 90.7%; Average loss: 0.1135\n",
            "Iteration: 27201; Percent complete: 90.7%; Average loss: 0.0941\n",
            "Iteration: 27202; Percent complete: 90.7%; Average loss: 0.0926\n",
            "Iteration: 27203; Percent complete: 90.7%; Average loss: 0.1130\n",
            "Iteration: 27204; Percent complete: 90.7%; Average loss: 0.0994\n",
            "Iteration: 27205; Percent complete: 90.7%; Average loss: 0.1381\n",
            "Iteration: 27206; Percent complete: 90.7%; Average loss: 0.1146\n",
            "Iteration: 27207; Percent complete: 90.7%; Average loss: 0.1238\n",
            "Iteration: 27208; Percent complete: 90.7%; Average loss: 0.1283\n",
            "Iteration: 27209; Percent complete: 90.7%; Average loss: 0.1058\n",
            "Iteration: 27210; Percent complete: 90.7%; Average loss: 0.1511\n",
            "Iteration: 27211; Percent complete: 90.7%; Average loss: 0.0944\n",
            "Iteration: 27212; Percent complete: 90.7%; Average loss: 0.1799\n",
            "Iteration: 27213; Percent complete: 90.7%; Average loss: 0.0872\n",
            "Iteration: 27214; Percent complete: 90.7%; Average loss: 0.1637\n",
            "Iteration: 27215; Percent complete: 90.7%; Average loss: 0.1232\n",
            "Iteration: 27216; Percent complete: 90.7%; Average loss: 0.1135\n",
            "Iteration: 27217; Percent complete: 90.7%; Average loss: 0.0967\n",
            "Iteration: 27218; Percent complete: 90.7%; Average loss: 0.1339\n",
            "Iteration: 27219; Percent complete: 90.7%; Average loss: 0.1836\n",
            "Iteration: 27220; Percent complete: 90.7%; Average loss: 0.1807\n",
            "Iteration: 27221; Percent complete: 90.7%; Average loss: 0.1429\n",
            "Iteration: 27222; Percent complete: 90.7%; Average loss: 0.1393\n",
            "Iteration: 27223; Percent complete: 90.7%; Average loss: 0.1589\n",
            "Iteration: 27224; Percent complete: 90.7%; Average loss: 0.1895\n",
            "Iteration: 27225; Percent complete: 90.8%; Average loss: 0.1602\n",
            "Iteration: 27226; Percent complete: 90.8%; Average loss: 0.1328\n",
            "Iteration: 27227; Percent complete: 90.8%; Average loss: 0.0795\n",
            "Iteration: 27228; Percent complete: 90.8%; Average loss: 0.1064\n",
            "Iteration: 27229; Percent complete: 90.8%; Average loss: 0.0983\n",
            "Iteration: 27230; Percent complete: 90.8%; Average loss: 0.1225\n",
            "Iteration: 27231; Percent complete: 90.8%; Average loss: 0.1193\n",
            "Iteration: 27232; Percent complete: 90.8%; Average loss: 0.1675\n",
            "Iteration: 27233; Percent complete: 90.8%; Average loss: 0.1441\n",
            "Iteration: 27234; Percent complete: 90.8%; Average loss: 0.1331\n",
            "Iteration: 27235; Percent complete: 90.8%; Average loss: 0.1219\n",
            "Iteration: 27236; Percent complete: 90.8%; Average loss: 0.1403\n",
            "Iteration: 27237; Percent complete: 90.8%; Average loss: 0.1639\n",
            "Iteration: 27238; Percent complete: 90.8%; Average loss: 0.1178\n",
            "Iteration: 27239; Percent complete: 90.8%; Average loss: 0.1489\n",
            "Iteration: 27240; Percent complete: 90.8%; Average loss: 0.1289\n",
            "Iteration: 27241; Percent complete: 90.8%; Average loss: 0.0885\n",
            "Iteration: 27242; Percent complete: 90.8%; Average loss: 0.1449\n",
            "Iteration: 27243; Percent complete: 90.8%; Average loss: 0.1433\n",
            "Iteration: 27244; Percent complete: 90.8%; Average loss: 0.1260\n",
            "Iteration: 27245; Percent complete: 90.8%; Average loss: 0.1274\n",
            "Iteration: 27246; Percent complete: 90.8%; Average loss: 0.1027\n",
            "Iteration: 27247; Percent complete: 90.8%; Average loss: 0.0876\n",
            "Iteration: 27248; Percent complete: 90.8%; Average loss: 0.0777\n",
            "Iteration: 27249; Percent complete: 90.8%; Average loss: 0.1288\n",
            "Iteration: 27250; Percent complete: 90.8%; Average loss: 0.1047\n",
            "Iteration: 27251; Percent complete: 90.8%; Average loss: 0.1300\n",
            "Iteration: 27252; Percent complete: 90.8%; Average loss: 0.1517\n",
            "Iteration: 27253; Percent complete: 90.8%; Average loss: 0.1446\n",
            "Iteration: 27254; Percent complete: 90.8%; Average loss: 0.2080\n",
            "Iteration: 27255; Percent complete: 90.8%; Average loss: 0.1135\n",
            "Iteration: 27256; Percent complete: 90.9%; Average loss: 0.1270\n",
            "Iteration: 27257; Percent complete: 90.9%; Average loss: 0.1367\n",
            "Iteration: 27258; Percent complete: 90.9%; Average loss: 0.1160\n",
            "Iteration: 27259; Percent complete: 90.9%; Average loss: 0.1627\n",
            "Iteration: 27260; Percent complete: 90.9%; Average loss: 0.1503\n",
            "Iteration: 27261; Percent complete: 90.9%; Average loss: 0.1002\n",
            "Iteration: 27262; Percent complete: 90.9%; Average loss: 0.1201\n",
            "Iteration: 27263; Percent complete: 90.9%; Average loss: 0.1454\n",
            "Iteration: 27264; Percent complete: 90.9%; Average loss: 0.0732\n",
            "Iteration: 27265; Percent complete: 90.9%; Average loss: 0.1591\n",
            "Iteration: 27266; Percent complete: 90.9%; Average loss: 0.1313\n",
            "Iteration: 27267; Percent complete: 90.9%; Average loss: 0.1233\n",
            "Iteration: 27268; Percent complete: 90.9%; Average loss: 0.1405\n",
            "Iteration: 27269; Percent complete: 90.9%; Average loss: 0.1522\n",
            "Iteration: 27270; Percent complete: 90.9%; Average loss: 0.1194\n",
            "Iteration: 27271; Percent complete: 90.9%; Average loss: 0.1067\n",
            "Iteration: 27272; Percent complete: 90.9%; Average loss: 0.1336\n",
            "Iteration: 27273; Percent complete: 90.9%; Average loss: 0.0853\n",
            "Iteration: 27274; Percent complete: 90.9%; Average loss: 0.1497\n",
            "Iteration: 27275; Percent complete: 90.9%; Average loss: 0.1339\n",
            "Iteration: 27276; Percent complete: 90.9%; Average loss: 0.1333\n",
            "Iteration: 27277; Percent complete: 90.9%; Average loss: 0.1365\n",
            "Iteration: 27278; Percent complete: 90.9%; Average loss: 0.1670\n",
            "Iteration: 27279; Percent complete: 90.9%; Average loss: 0.1189\n",
            "Iteration: 27280; Percent complete: 90.9%; Average loss: 0.1179\n",
            "Iteration: 27281; Percent complete: 90.9%; Average loss: 0.1321\n",
            "Iteration: 27282; Percent complete: 90.9%; Average loss: 0.1160\n",
            "Iteration: 27283; Percent complete: 90.9%; Average loss: 0.1265\n",
            "Iteration: 27284; Percent complete: 90.9%; Average loss: 0.1202\n",
            "Iteration: 27285; Percent complete: 91.0%; Average loss: 0.0986\n",
            "Iteration: 27286; Percent complete: 91.0%; Average loss: 0.1239\n",
            "Iteration: 27287; Percent complete: 91.0%; Average loss: 0.1256\n",
            "Iteration: 27288; Percent complete: 91.0%; Average loss: 0.1260\n",
            "Iteration: 27289; Percent complete: 91.0%; Average loss: 0.0974\n",
            "Iteration: 27290; Percent complete: 91.0%; Average loss: 0.1225\n",
            "Iteration: 27291; Percent complete: 91.0%; Average loss: 0.1202\n",
            "Iteration: 27292; Percent complete: 91.0%; Average loss: 0.1492\n",
            "Iteration: 27293; Percent complete: 91.0%; Average loss: 0.1328\n",
            "Iteration: 27294; Percent complete: 91.0%; Average loss: 0.1318\n",
            "Iteration: 27295; Percent complete: 91.0%; Average loss: 0.1253\n",
            "Iteration: 27296; Percent complete: 91.0%; Average loss: 0.1267\n",
            "Iteration: 27297; Percent complete: 91.0%; Average loss: 0.1555\n",
            "Iteration: 27298; Percent complete: 91.0%; Average loss: 0.1396\n",
            "Iteration: 27299; Percent complete: 91.0%; Average loss: 0.1207\n",
            "Iteration: 27300; Percent complete: 91.0%; Average loss: 0.1677\n",
            "Iteration: 27301; Percent complete: 91.0%; Average loss: 0.1430\n",
            "Iteration: 27302; Percent complete: 91.0%; Average loss: 0.1785\n",
            "Iteration: 27303; Percent complete: 91.0%; Average loss: 0.1346\n",
            "Iteration: 27304; Percent complete: 91.0%; Average loss: 0.1354\n",
            "Iteration: 27305; Percent complete: 91.0%; Average loss: 0.1334\n",
            "Iteration: 27306; Percent complete: 91.0%; Average loss: 0.1323\n",
            "Iteration: 27307; Percent complete: 91.0%; Average loss: 0.1322\n",
            "Iteration: 27308; Percent complete: 91.0%; Average loss: 0.1132\n",
            "Iteration: 27309; Percent complete: 91.0%; Average loss: 0.1443\n",
            "Iteration: 27310; Percent complete: 91.0%; Average loss: 0.1216\n",
            "Iteration: 27311; Percent complete: 91.0%; Average loss: 0.1033\n",
            "Iteration: 27312; Percent complete: 91.0%; Average loss: 0.1210\n",
            "Iteration: 27313; Percent complete: 91.0%; Average loss: 0.1197\n",
            "Iteration: 27314; Percent complete: 91.0%; Average loss: 0.0933\n",
            "Iteration: 27315; Percent complete: 91.0%; Average loss: 0.1068\n",
            "Iteration: 27316; Percent complete: 91.1%; Average loss: 0.0724\n",
            "Iteration: 27317; Percent complete: 91.1%; Average loss: 0.1793\n",
            "Iteration: 27318; Percent complete: 91.1%; Average loss: 0.1335\n",
            "Iteration: 27319; Percent complete: 91.1%; Average loss: 0.1082\n",
            "Iteration: 27320; Percent complete: 91.1%; Average loss: 0.0778\n",
            "Iteration: 27321; Percent complete: 91.1%; Average loss: 0.1310\n",
            "Iteration: 27322; Percent complete: 91.1%; Average loss: 0.1317\n",
            "Iteration: 27323; Percent complete: 91.1%; Average loss: 0.1480\n",
            "Iteration: 27324; Percent complete: 91.1%; Average loss: 0.1880\n",
            "Iteration: 27325; Percent complete: 91.1%; Average loss: 0.1396\n",
            "Iteration: 27326; Percent complete: 91.1%; Average loss: 0.1040\n",
            "Iteration: 27327; Percent complete: 91.1%; Average loss: 0.1060\n",
            "Iteration: 27328; Percent complete: 91.1%; Average loss: 0.1285\n",
            "Iteration: 27329; Percent complete: 91.1%; Average loss: 0.1066\n",
            "Iteration: 27330; Percent complete: 91.1%; Average loss: 0.1189\n",
            "Iteration: 27331; Percent complete: 91.1%; Average loss: 0.1206\n",
            "Iteration: 27332; Percent complete: 91.1%; Average loss: 0.1185\n",
            "Iteration: 27333; Percent complete: 91.1%; Average loss: 0.1065\n",
            "Iteration: 27334; Percent complete: 91.1%; Average loss: 0.1483\n",
            "Iteration: 27335; Percent complete: 91.1%; Average loss: 0.1507\n",
            "Iteration: 27336; Percent complete: 91.1%; Average loss: 0.0991\n",
            "Iteration: 27337; Percent complete: 91.1%; Average loss: 0.1337\n",
            "Iteration: 27338; Percent complete: 91.1%; Average loss: 0.1456\n",
            "Iteration: 27339; Percent complete: 91.1%; Average loss: 0.1314\n",
            "Iteration: 27340; Percent complete: 91.1%; Average loss: 0.1379\n",
            "Iteration: 27341; Percent complete: 91.1%; Average loss: 0.1323\n",
            "Iteration: 27342; Percent complete: 91.1%; Average loss: 0.0850\n",
            "Iteration: 27343; Percent complete: 91.1%; Average loss: 0.1305\n",
            "Iteration: 27344; Percent complete: 91.1%; Average loss: 0.1123\n",
            "Iteration: 27345; Percent complete: 91.1%; Average loss: 0.1867\n",
            "Iteration: 27346; Percent complete: 91.2%; Average loss: 0.0893\n",
            "Iteration: 27347; Percent complete: 91.2%; Average loss: 0.1608\n",
            "Iteration: 27348; Percent complete: 91.2%; Average loss: 0.1469\n",
            "Iteration: 27349; Percent complete: 91.2%; Average loss: 0.1256\n",
            "Iteration: 27350; Percent complete: 91.2%; Average loss: 0.1020\n",
            "Iteration: 27351; Percent complete: 91.2%; Average loss: 0.1309\n",
            "Iteration: 27352; Percent complete: 91.2%; Average loss: 0.0958\n",
            "Iteration: 27353; Percent complete: 91.2%; Average loss: 0.1243\n",
            "Iteration: 27354; Percent complete: 91.2%; Average loss: 0.1180\n",
            "Iteration: 27355; Percent complete: 91.2%; Average loss: 0.1664\n",
            "Iteration: 27356; Percent complete: 91.2%; Average loss: 0.1390\n",
            "Iteration: 27357; Percent complete: 91.2%; Average loss: 0.1448\n",
            "Iteration: 27358; Percent complete: 91.2%; Average loss: 0.1035\n",
            "Iteration: 27359; Percent complete: 91.2%; Average loss: 0.1189\n",
            "Iteration: 27360; Percent complete: 91.2%; Average loss: 0.1721\n",
            "Iteration: 27361; Percent complete: 91.2%; Average loss: 0.1535\n",
            "Iteration: 27362; Percent complete: 91.2%; Average loss: 0.1225\n",
            "Iteration: 27363; Percent complete: 91.2%; Average loss: 0.1042\n",
            "Iteration: 27364; Percent complete: 91.2%; Average loss: 0.1140\n",
            "Iteration: 27365; Percent complete: 91.2%; Average loss: 0.0895\n",
            "Iteration: 27366; Percent complete: 91.2%; Average loss: 0.1562\n",
            "Iteration: 27367; Percent complete: 91.2%; Average loss: 0.1487\n",
            "Iteration: 27368; Percent complete: 91.2%; Average loss: 0.1299\n",
            "Iteration: 27369; Percent complete: 91.2%; Average loss: 0.1462\n",
            "Iteration: 27370; Percent complete: 91.2%; Average loss: 0.1094\n",
            "Iteration: 27371; Percent complete: 91.2%; Average loss: 0.1121\n",
            "Iteration: 27372; Percent complete: 91.2%; Average loss: 0.1329\n",
            "Iteration: 27373; Percent complete: 91.2%; Average loss: 0.0922\n",
            "Iteration: 27374; Percent complete: 91.2%; Average loss: 0.1092\n",
            "Iteration: 27375; Percent complete: 91.2%; Average loss: 0.1243\n",
            "Iteration: 27376; Percent complete: 91.3%; Average loss: 0.1154\n",
            "Iteration: 27377; Percent complete: 91.3%; Average loss: 0.0997\n",
            "Iteration: 27378; Percent complete: 91.3%; Average loss: 0.1223\n",
            "Iteration: 27379; Percent complete: 91.3%; Average loss: 0.1530\n",
            "Iteration: 27380; Percent complete: 91.3%; Average loss: 0.1514\n",
            "Iteration: 27381; Percent complete: 91.3%; Average loss: 0.1894\n",
            "Iteration: 27382; Percent complete: 91.3%; Average loss: 0.1344\n",
            "Iteration: 27383; Percent complete: 91.3%; Average loss: 0.1382\n",
            "Iteration: 27384; Percent complete: 91.3%; Average loss: 0.1265\n",
            "Iteration: 27385; Percent complete: 91.3%; Average loss: 0.1771\n",
            "Iteration: 27386; Percent complete: 91.3%; Average loss: 0.1174\n",
            "Iteration: 27387; Percent complete: 91.3%; Average loss: 0.1258\n",
            "Iteration: 27388; Percent complete: 91.3%; Average loss: 0.1062\n",
            "Iteration: 27389; Percent complete: 91.3%; Average loss: 0.1544\n",
            "Iteration: 27390; Percent complete: 91.3%; Average loss: 0.1050\n",
            "Iteration: 27391; Percent complete: 91.3%; Average loss: 0.1125\n",
            "Iteration: 27392; Percent complete: 91.3%; Average loss: 0.1186\n",
            "Iteration: 27393; Percent complete: 91.3%; Average loss: 0.1185\n",
            "Iteration: 27394; Percent complete: 91.3%; Average loss: 0.0951\n",
            "Iteration: 27395; Percent complete: 91.3%; Average loss: 0.1204\n",
            "Iteration: 27396; Percent complete: 91.3%; Average loss: 0.1189\n",
            "Iteration: 27397; Percent complete: 91.3%; Average loss: 0.1040\n",
            "Iteration: 27398; Percent complete: 91.3%; Average loss: 0.1571\n",
            "Iteration: 27399; Percent complete: 91.3%; Average loss: 0.1174\n",
            "Iteration: 27400; Percent complete: 91.3%; Average loss: 0.1282\n",
            "Iteration: 27401; Percent complete: 91.3%; Average loss: 0.1406\n",
            "Iteration: 27402; Percent complete: 91.3%; Average loss: 0.1354\n",
            "Iteration: 27403; Percent complete: 91.3%; Average loss: 0.1107\n",
            "Iteration: 27404; Percent complete: 91.3%; Average loss: 0.1243\n",
            "Iteration: 27405; Percent complete: 91.3%; Average loss: 0.1175\n",
            "Iteration: 27406; Percent complete: 91.4%; Average loss: 0.1427\n",
            "Iteration: 27407; Percent complete: 91.4%; Average loss: 0.1256\n",
            "Iteration: 27408; Percent complete: 91.4%; Average loss: 0.0977\n",
            "Iteration: 27409; Percent complete: 91.4%; Average loss: 0.1472\n",
            "Iteration: 27410; Percent complete: 91.4%; Average loss: 0.1539\n",
            "Iteration: 27411; Percent complete: 91.4%; Average loss: 0.0753\n",
            "Iteration: 27412; Percent complete: 91.4%; Average loss: 0.1149\n",
            "Iteration: 27413; Percent complete: 91.4%; Average loss: 0.1445\n",
            "Iteration: 27414; Percent complete: 91.4%; Average loss: 0.1439\n",
            "Iteration: 27415; Percent complete: 91.4%; Average loss: 0.1450\n",
            "Iteration: 27416; Percent complete: 91.4%; Average loss: 0.1243\n",
            "Iteration: 27417; Percent complete: 91.4%; Average loss: 0.1534\n",
            "Iteration: 27418; Percent complete: 91.4%; Average loss: 0.1118\n",
            "Iteration: 27419; Percent complete: 91.4%; Average loss: 0.1144\n",
            "Iteration: 27420; Percent complete: 91.4%; Average loss: 0.1419\n",
            "Iteration: 27421; Percent complete: 91.4%; Average loss: 0.0889\n",
            "Iteration: 27422; Percent complete: 91.4%; Average loss: 0.1122\n",
            "Iteration: 27423; Percent complete: 91.4%; Average loss: 0.1145\n",
            "Iteration: 27424; Percent complete: 91.4%; Average loss: 0.1215\n",
            "Iteration: 27425; Percent complete: 91.4%; Average loss: 0.1260\n",
            "Iteration: 27426; Percent complete: 91.4%; Average loss: 0.1128\n",
            "Iteration: 27427; Percent complete: 91.4%; Average loss: 0.1348\n",
            "Iteration: 27428; Percent complete: 91.4%; Average loss: 0.1156\n",
            "Iteration: 27429; Percent complete: 91.4%; Average loss: 0.1169\n",
            "Iteration: 27430; Percent complete: 91.4%; Average loss: 0.0942\n",
            "Iteration: 27431; Percent complete: 91.4%; Average loss: 0.1411\n",
            "Iteration: 27432; Percent complete: 91.4%; Average loss: 0.1094\n",
            "Iteration: 27433; Percent complete: 91.4%; Average loss: 0.1550\n",
            "Iteration: 27434; Percent complete: 91.4%; Average loss: 0.1677\n",
            "Iteration: 27435; Percent complete: 91.5%; Average loss: 0.1121\n",
            "Iteration: 27436; Percent complete: 91.5%; Average loss: 0.0831\n",
            "Iteration: 27437; Percent complete: 91.5%; Average loss: 0.1382\n",
            "Iteration: 27438; Percent complete: 91.5%; Average loss: 0.1114\n",
            "Iteration: 27439; Percent complete: 91.5%; Average loss: 0.1225\n",
            "Iteration: 27440; Percent complete: 91.5%; Average loss: 0.1451\n",
            "Iteration: 27441; Percent complete: 91.5%; Average loss: 0.1180\n",
            "Iteration: 27442; Percent complete: 91.5%; Average loss: 0.1418\n",
            "Iteration: 27443; Percent complete: 91.5%; Average loss: 0.1635\n",
            "Iteration: 27444; Percent complete: 91.5%; Average loss: 0.1450\n",
            "Iteration: 27445; Percent complete: 91.5%; Average loss: 0.0946\n",
            "Iteration: 27446; Percent complete: 91.5%; Average loss: 0.0965\n",
            "Iteration: 27447; Percent complete: 91.5%; Average loss: 0.1325\n",
            "Iteration: 27448; Percent complete: 91.5%; Average loss: 0.1102\n",
            "Iteration: 27449; Percent complete: 91.5%; Average loss: 0.1082\n",
            "Iteration: 27450; Percent complete: 91.5%; Average loss: 0.1362\n",
            "Iteration: 27451; Percent complete: 91.5%; Average loss: 0.1379\n",
            "Iteration: 27452; Percent complete: 91.5%; Average loss: 0.1650\n",
            "Iteration: 27453; Percent complete: 91.5%; Average loss: 0.1396\n",
            "Iteration: 27454; Percent complete: 91.5%; Average loss: 0.1111\n",
            "Iteration: 27455; Percent complete: 91.5%; Average loss: 0.1511\n",
            "Iteration: 27456; Percent complete: 91.5%; Average loss: 0.1489\n",
            "Iteration: 27457; Percent complete: 91.5%; Average loss: 0.1250\n",
            "Iteration: 27458; Percent complete: 91.5%; Average loss: 0.0897\n",
            "Iteration: 27459; Percent complete: 91.5%; Average loss: 0.0786\n",
            "Iteration: 27460; Percent complete: 91.5%; Average loss: 0.1427\n",
            "Iteration: 27461; Percent complete: 91.5%; Average loss: 0.1167\n",
            "Iteration: 27462; Percent complete: 91.5%; Average loss: 0.1539\n",
            "Iteration: 27463; Percent complete: 91.5%; Average loss: 0.1637\n",
            "Iteration: 27464; Percent complete: 91.5%; Average loss: 0.1419\n",
            "Iteration: 27465; Percent complete: 91.5%; Average loss: 0.0997\n",
            "Iteration: 27466; Percent complete: 91.6%; Average loss: 0.1268\n",
            "Iteration: 27467; Percent complete: 91.6%; Average loss: 0.1621\n",
            "Iteration: 27468; Percent complete: 91.6%; Average loss: 0.1115\n",
            "Iteration: 27469; Percent complete: 91.6%; Average loss: 0.1305\n",
            "Iteration: 27470; Percent complete: 91.6%; Average loss: 0.1268\n",
            "Iteration: 27471; Percent complete: 91.6%; Average loss: 0.1519\n",
            "Iteration: 27472; Percent complete: 91.6%; Average loss: 0.0808\n",
            "Iteration: 27473; Percent complete: 91.6%; Average loss: 0.1600\n",
            "Iteration: 27474; Percent complete: 91.6%; Average loss: 0.0784\n",
            "Iteration: 27475; Percent complete: 91.6%; Average loss: 0.0946\n",
            "Iteration: 27476; Percent complete: 91.6%; Average loss: 0.1369\n",
            "Iteration: 27477; Percent complete: 91.6%; Average loss: 0.0831\n",
            "Iteration: 27478; Percent complete: 91.6%; Average loss: 0.1115\n",
            "Iteration: 27479; Percent complete: 91.6%; Average loss: 0.1342\n",
            "Iteration: 27480; Percent complete: 91.6%; Average loss: 0.1097\n",
            "Iteration: 27481; Percent complete: 91.6%; Average loss: 0.1519\n",
            "Iteration: 27482; Percent complete: 91.6%; Average loss: 0.1128\n",
            "Iteration: 27483; Percent complete: 91.6%; Average loss: 0.0874\n",
            "Iteration: 27484; Percent complete: 91.6%; Average loss: 0.1533\n",
            "Iteration: 27485; Percent complete: 91.6%; Average loss: 0.1588\n",
            "Iteration: 27486; Percent complete: 91.6%; Average loss: 0.1067\n",
            "Iteration: 27487; Percent complete: 91.6%; Average loss: 0.1176\n",
            "Iteration: 27488; Percent complete: 91.6%; Average loss: 0.1218\n",
            "Iteration: 27489; Percent complete: 91.6%; Average loss: 0.1593\n",
            "Iteration: 27490; Percent complete: 91.6%; Average loss: 0.1583\n",
            "Iteration: 27491; Percent complete: 91.6%; Average loss: 0.1014\n",
            "Iteration: 27492; Percent complete: 91.6%; Average loss: 0.1168\n",
            "Iteration: 27493; Percent complete: 91.6%; Average loss: 0.1580\n",
            "Iteration: 27494; Percent complete: 91.6%; Average loss: 0.1047\n",
            "Iteration: 27495; Percent complete: 91.6%; Average loss: 0.1438\n",
            "Iteration: 27496; Percent complete: 91.7%; Average loss: 0.0928\n",
            "Iteration: 27497; Percent complete: 91.7%; Average loss: 0.2109\n",
            "Iteration: 27498; Percent complete: 91.7%; Average loss: 0.1836\n",
            "Iteration: 27499; Percent complete: 91.7%; Average loss: 0.0942\n",
            "Iteration: 27500; Percent complete: 91.7%; Average loss: 0.1297\n",
            "Iteration: 27501; Percent complete: 91.7%; Average loss: 0.1258\n",
            "Iteration: 27502; Percent complete: 91.7%; Average loss: 0.1080\n",
            "Iteration: 27503; Percent complete: 91.7%; Average loss: 0.1165\n",
            "Iteration: 27504; Percent complete: 91.7%; Average loss: 0.1399\n",
            "Iteration: 27505; Percent complete: 91.7%; Average loss: 0.1008\n",
            "Iteration: 27506; Percent complete: 91.7%; Average loss: 0.1025\n",
            "Iteration: 27507; Percent complete: 91.7%; Average loss: 0.1092\n",
            "Iteration: 27508; Percent complete: 91.7%; Average loss: 0.0938\n",
            "Iteration: 27509; Percent complete: 91.7%; Average loss: 0.1043\n",
            "Iteration: 27510; Percent complete: 91.7%; Average loss: 0.1086\n",
            "Iteration: 27511; Percent complete: 91.7%; Average loss: 0.1255\n",
            "Iteration: 27512; Percent complete: 91.7%; Average loss: 0.1084\n",
            "Iteration: 27513; Percent complete: 91.7%; Average loss: 0.1294\n",
            "Iteration: 27514; Percent complete: 91.7%; Average loss: 0.1145\n",
            "Iteration: 27515; Percent complete: 91.7%; Average loss: 0.0885\n",
            "Iteration: 27516; Percent complete: 91.7%; Average loss: 0.1200\n",
            "Iteration: 27517; Percent complete: 91.7%; Average loss: 0.1355\n",
            "Iteration: 27518; Percent complete: 91.7%; Average loss: 0.1202\n",
            "Iteration: 27519; Percent complete: 91.7%; Average loss: 0.1139\n",
            "Iteration: 27520; Percent complete: 91.7%; Average loss: 0.1413\n",
            "Iteration: 27521; Percent complete: 91.7%; Average loss: 0.1383\n",
            "Iteration: 27522; Percent complete: 91.7%; Average loss: 0.1320\n",
            "Iteration: 27523; Percent complete: 91.7%; Average loss: 0.1439\n",
            "Iteration: 27524; Percent complete: 91.7%; Average loss: 0.0953\n",
            "Iteration: 27525; Percent complete: 91.8%; Average loss: 0.0751\n",
            "Iteration: 27526; Percent complete: 91.8%; Average loss: 0.0965\n",
            "Iteration: 27527; Percent complete: 91.8%; Average loss: 0.1129\n",
            "Iteration: 27528; Percent complete: 91.8%; Average loss: 0.0951\n",
            "Iteration: 27529; Percent complete: 91.8%; Average loss: 0.1200\n",
            "Iteration: 27530; Percent complete: 91.8%; Average loss: 0.1423\n",
            "Iteration: 27531; Percent complete: 91.8%; Average loss: 0.1126\n",
            "Iteration: 27532; Percent complete: 91.8%; Average loss: 0.1245\n",
            "Iteration: 27533; Percent complete: 91.8%; Average loss: 0.1178\n",
            "Iteration: 27534; Percent complete: 91.8%; Average loss: 0.1339\n",
            "Iteration: 27535; Percent complete: 91.8%; Average loss: 0.0888\n",
            "Iteration: 27536; Percent complete: 91.8%; Average loss: 0.1639\n",
            "Iteration: 27537; Percent complete: 91.8%; Average loss: 0.1303\n",
            "Iteration: 27538; Percent complete: 91.8%; Average loss: 0.0979\n",
            "Iteration: 27539; Percent complete: 91.8%; Average loss: 0.1481\n",
            "Iteration: 27540; Percent complete: 91.8%; Average loss: 0.1714\n",
            "Iteration: 27541; Percent complete: 91.8%; Average loss: 0.1433\n",
            "Iteration: 27542; Percent complete: 91.8%; Average loss: 0.1102\n",
            "Iteration: 27543; Percent complete: 91.8%; Average loss: 0.0956\n",
            "Iteration: 27544; Percent complete: 91.8%; Average loss: 0.0991\n",
            "Iteration: 27545; Percent complete: 91.8%; Average loss: 0.0991\n",
            "Iteration: 27546; Percent complete: 91.8%; Average loss: 0.1690\n",
            "Iteration: 27547; Percent complete: 91.8%; Average loss: 0.1239\n",
            "Iteration: 27548; Percent complete: 91.8%; Average loss: 0.0955\n",
            "Iteration: 27549; Percent complete: 91.8%; Average loss: 0.1380\n",
            "Iteration: 27550; Percent complete: 91.8%; Average loss: 0.0922\n",
            "Iteration: 27551; Percent complete: 91.8%; Average loss: 0.0863\n",
            "Iteration: 27552; Percent complete: 91.8%; Average loss: 0.0966\n",
            "Iteration: 27553; Percent complete: 91.8%; Average loss: 0.1929\n",
            "Iteration: 27554; Percent complete: 91.8%; Average loss: 0.1689\n",
            "Iteration: 27555; Percent complete: 91.8%; Average loss: 0.1294\n",
            "Iteration: 27556; Percent complete: 91.9%; Average loss: 0.1240\n",
            "Iteration: 27557; Percent complete: 91.9%; Average loss: 0.1009\n",
            "Iteration: 27558; Percent complete: 91.9%; Average loss: 0.1112\n",
            "Iteration: 27559; Percent complete: 91.9%; Average loss: 0.1113\n",
            "Iteration: 27560; Percent complete: 91.9%; Average loss: 0.0940\n",
            "Iteration: 27561; Percent complete: 91.9%; Average loss: 0.1715\n",
            "Iteration: 27562; Percent complete: 91.9%; Average loss: 0.1260\n",
            "Iteration: 27563; Percent complete: 91.9%; Average loss: 0.1013\n",
            "Iteration: 27564; Percent complete: 91.9%; Average loss: 0.1863\n",
            "Iteration: 27565; Percent complete: 91.9%; Average loss: 0.1511\n",
            "Iteration: 27566; Percent complete: 91.9%; Average loss: 0.1249\n",
            "Iteration: 27567; Percent complete: 91.9%; Average loss: 0.1108\n",
            "Iteration: 27568; Percent complete: 91.9%; Average loss: 0.1256\n",
            "Iteration: 27569; Percent complete: 91.9%; Average loss: 0.0935\n",
            "Iteration: 27570; Percent complete: 91.9%; Average loss: 0.1611\n",
            "Iteration: 27571; Percent complete: 91.9%; Average loss: 0.1402\n",
            "Iteration: 27572; Percent complete: 91.9%; Average loss: 0.1229\n",
            "Iteration: 27573; Percent complete: 91.9%; Average loss: 0.1380\n",
            "Iteration: 27574; Percent complete: 91.9%; Average loss: 0.0949\n",
            "Iteration: 27575; Percent complete: 91.9%; Average loss: 0.1712\n",
            "Iteration: 27576; Percent complete: 91.9%; Average loss: 0.1380\n",
            "Iteration: 27577; Percent complete: 91.9%; Average loss: 0.1131\n",
            "Iteration: 27578; Percent complete: 91.9%; Average loss: 0.1026\n",
            "Iteration: 27579; Percent complete: 91.9%; Average loss: 0.1161\n",
            "Iteration: 27580; Percent complete: 91.9%; Average loss: 0.1766\n",
            "Iteration: 27581; Percent complete: 91.9%; Average loss: 0.1340\n",
            "Iteration: 27582; Percent complete: 91.9%; Average loss: 0.1164\n",
            "Iteration: 27583; Percent complete: 91.9%; Average loss: 0.1394\n",
            "Iteration: 27584; Percent complete: 91.9%; Average loss: 0.1383\n",
            "Iteration: 27585; Percent complete: 92.0%; Average loss: 0.1147\n",
            "Iteration: 27586; Percent complete: 92.0%; Average loss: 0.1132\n",
            "Iteration: 27587; Percent complete: 92.0%; Average loss: 0.1263\n",
            "Iteration: 27588; Percent complete: 92.0%; Average loss: 0.1508\n",
            "Iteration: 27589; Percent complete: 92.0%; Average loss: 0.0914\n",
            "Iteration: 27590; Percent complete: 92.0%; Average loss: 0.1562\n",
            "Iteration: 27591; Percent complete: 92.0%; Average loss: 0.1467\n",
            "Iteration: 27592; Percent complete: 92.0%; Average loss: 0.1132\n",
            "Iteration: 27593; Percent complete: 92.0%; Average loss: 0.0995\n",
            "Iteration: 27594; Percent complete: 92.0%; Average loss: 0.0858\n",
            "Iteration: 27595; Percent complete: 92.0%; Average loss: 0.0941\n",
            "Iteration: 27596; Percent complete: 92.0%; Average loss: 0.1076\n",
            "Iteration: 27597; Percent complete: 92.0%; Average loss: 0.1615\n",
            "Iteration: 27598; Percent complete: 92.0%; Average loss: 0.1100\n",
            "Iteration: 27599; Percent complete: 92.0%; Average loss: 0.1199\n",
            "Iteration: 27600; Percent complete: 92.0%; Average loss: 0.1344\n",
            "Iteration: 27601; Percent complete: 92.0%; Average loss: 0.0816\n",
            "Iteration: 27602; Percent complete: 92.0%; Average loss: 0.1101\n",
            "Iteration: 27603; Percent complete: 92.0%; Average loss: 0.1224\n",
            "Iteration: 27604; Percent complete: 92.0%; Average loss: 0.1777\n",
            "Iteration: 27605; Percent complete: 92.0%; Average loss: 0.1092\n",
            "Iteration: 27606; Percent complete: 92.0%; Average loss: 0.1570\n",
            "Iteration: 27607; Percent complete: 92.0%; Average loss: 0.1383\n",
            "Iteration: 27608; Percent complete: 92.0%; Average loss: 0.1230\n",
            "Iteration: 27609; Percent complete: 92.0%; Average loss: 0.1417\n",
            "Iteration: 27610; Percent complete: 92.0%; Average loss: 0.1341\n",
            "Iteration: 27611; Percent complete: 92.0%; Average loss: 0.1330\n",
            "Iteration: 27612; Percent complete: 92.0%; Average loss: 0.1113\n",
            "Iteration: 27613; Percent complete: 92.0%; Average loss: 0.0777\n",
            "Iteration: 27614; Percent complete: 92.0%; Average loss: 0.1305\n",
            "Iteration: 27615; Percent complete: 92.0%; Average loss: 0.1480\n",
            "Iteration: 27616; Percent complete: 92.1%; Average loss: 0.1186\n",
            "Iteration: 27617; Percent complete: 92.1%; Average loss: 0.1207\n",
            "Iteration: 27618; Percent complete: 92.1%; Average loss: 0.1164\n",
            "Iteration: 27619; Percent complete: 92.1%; Average loss: 0.1267\n",
            "Iteration: 27620; Percent complete: 92.1%; Average loss: 0.0756\n",
            "Iteration: 27621; Percent complete: 92.1%; Average loss: 0.1349\n",
            "Iteration: 27622; Percent complete: 92.1%; Average loss: 0.1916\n",
            "Iteration: 27623; Percent complete: 92.1%; Average loss: 0.1505\n",
            "Iteration: 27624; Percent complete: 92.1%; Average loss: 0.1381\n",
            "Iteration: 27625; Percent complete: 92.1%; Average loss: 0.1149\n",
            "Iteration: 27626; Percent complete: 92.1%; Average loss: 0.0980\n",
            "Iteration: 27627; Percent complete: 92.1%; Average loss: 0.1259\n",
            "Iteration: 27628; Percent complete: 92.1%; Average loss: 0.1604\n",
            "Iteration: 27629; Percent complete: 92.1%; Average loss: 0.1521\n",
            "Iteration: 27630; Percent complete: 92.1%; Average loss: 0.0940\n",
            "Iteration: 27631; Percent complete: 92.1%; Average loss: 0.1660\n",
            "Iteration: 27632; Percent complete: 92.1%; Average loss: 0.1050\n",
            "Iteration: 27633; Percent complete: 92.1%; Average loss: 0.1135\n",
            "Iteration: 27634; Percent complete: 92.1%; Average loss: 0.0934\n",
            "Iteration: 27635; Percent complete: 92.1%; Average loss: 0.1385\n",
            "Iteration: 27636; Percent complete: 92.1%; Average loss: 0.1013\n",
            "Iteration: 27637; Percent complete: 92.1%; Average loss: 0.2007\n",
            "Iteration: 27638; Percent complete: 92.1%; Average loss: 0.0791\n",
            "Iteration: 27639; Percent complete: 92.1%; Average loss: 0.1125\n",
            "Iteration: 27640; Percent complete: 92.1%; Average loss: 0.1931\n",
            "Iteration: 27641; Percent complete: 92.1%; Average loss: 0.1335\n",
            "Iteration: 27642; Percent complete: 92.1%; Average loss: 0.1234\n",
            "Iteration: 27643; Percent complete: 92.1%; Average loss: 0.1080\n",
            "Iteration: 27644; Percent complete: 92.1%; Average loss: 0.1024\n",
            "Iteration: 27645; Percent complete: 92.2%; Average loss: 0.1323\n",
            "Iteration: 27646; Percent complete: 92.2%; Average loss: 0.1128\n",
            "Iteration: 27647; Percent complete: 92.2%; Average loss: 0.1352\n",
            "Iteration: 27648; Percent complete: 92.2%; Average loss: 0.1420\n",
            "Iteration: 27649; Percent complete: 92.2%; Average loss: 0.1311\n",
            "Iteration: 27650; Percent complete: 92.2%; Average loss: 0.1063\n",
            "Iteration: 27651; Percent complete: 92.2%; Average loss: 0.1123\n",
            "Iteration: 27652; Percent complete: 92.2%; Average loss: 0.1131\n",
            "Iteration: 27653; Percent complete: 92.2%; Average loss: 0.0913\n",
            "Iteration: 27654; Percent complete: 92.2%; Average loss: 0.1204\n",
            "Iteration: 27655; Percent complete: 92.2%; Average loss: 0.1246\n",
            "Iteration: 27656; Percent complete: 92.2%; Average loss: 0.1123\n",
            "Iteration: 27657; Percent complete: 92.2%; Average loss: 0.1050\n",
            "Iteration: 27658; Percent complete: 92.2%; Average loss: 0.1641\n",
            "Iteration: 27659; Percent complete: 92.2%; Average loss: 0.0981\n",
            "Iteration: 27660; Percent complete: 92.2%; Average loss: 0.1510\n",
            "Iteration: 27661; Percent complete: 92.2%; Average loss: 0.1324\n",
            "Iteration: 27662; Percent complete: 92.2%; Average loss: 0.1264\n",
            "Iteration: 27663; Percent complete: 92.2%; Average loss: 0.1302\n",
            "Iteration: 27664; Percent complete: 92.2%; Average loss: 0.0916\n",
            "Iteration: 27665; Percent complete: 92.2%; Average loss: 0.1150\n",
            "Iteration: 27666; Percent complete: 92.2%; Average loss: 0.1341\n",
            "Iteration: 27667; Percent complete: 92.2%; Average loss: 0.1475\n",
            "Iteration: 27668; Percent complete: 92.2%; Average loss: 0.1341\n",
            "Iteration: 27669; Percent complete: 92.2%; Average loss: 0.1181\n",
            "Iteration: 27670; Percent complete: 92.2%; Average loss: 0.0903\n",
            "Iteration: 27671; Percent complete: 92.2%; Average loss: 0.1436\n",
            "Iteration: 27672; Percent complete: 92.2%; Average loss: 0.1566\n",
            "Iteration: 27673; Percent complete: 92.2%; Average loss: 0.1046\n",
            "Iteration: 27674; Percent complete: 92.2%; Average loss: 0.1140\n",
            "Iteration: 27675; Percent complete: 92.2%; Average loss: 0.1668\n",
            "Iteration: 27676; Percent complete: 92.3%; Average loss: 0.1237\n",
            "Iteration: 27677; Percent complete: 92.3%; Average loss: 0.1654\n",
            "Iteration: 27678; Percent complete: 92.3%; Average loss: 0.0978\n",
            "Iteration: 27679; Percent complete: 92.3%; Average loss: 0.1485\n",
            "Iteration: 27680; Percent complete: 92.3%; Average loss: 0.1170\n",
            "Iteration: 27681; Percent complete: 92.3%; Average loss: 0.1312\n",
            "Iteration: 27682; Percent complete: 92.3%; Average loss: 0.1435\n",
            "Iteration: 27683; Percent complete: 92.3%; Average loss: 0.1504\n",
            "Iteration: 27684; Percent complete: 92.3%; Average loss: 0.1331\n",
            "Iteration: 27685; Percent complete: 92.3%; Average loss: 0.1343\n",
            "Iteration: 27686; Percent complete: 92.3%; Average loss: 0.1387\n",
            "Iteration: 27687; Percent complete: 92.3%; Average loss: 0.1070\n",
            "Iteration: 27688; Percent complete: 92.3%; Average loss: 0.0881\n",
            "Iteration: 27689; Percent complete: 92.3%; Average loss: 0.0669\n",
            "Iteration: 27690; Percent complete: 92.3%; Average loss: 0.0999\n",
            "Iteration: 27691; Percent complete: 92.3%; Average loss: 0.1249\n",
            "Iteration: 27692; Percent complete: 92.3%; Average loss: 0.1359\n",
            "Iteration: 27693; Percent complete: 92.3%; Average loss: 0.1224\n",
            "Iteration: 27694; Percent complete: 92.3%; Average loss: 0.1316\n",
            "Iteration: 27695; Percent complete: 92.3%; Average loss: 0.1146\n",
            "Iteration: 27696; Percent complete: 92.3%; Average loss: 0.1302\n",
            "Iteration: 27697; Percent complete: 92.3%; Average loss: 0.1043\n",
            "Iteration: 27698; Percent complete: 92.3%; Average loss: 0.1236\n",
            "Iteration: 27699; Percent complete: 92.3%; Average loss: 0.1049\n",
            "Iteration: 27700; Percent complete: 92.3%; Average loss: 0.1537\n",
            "Iteration: 27701; Percent complete: 92.3%; Average loss: 0.1404\n",
            "Iteration: 27702; Percent complete: 92.3%; Average loss: 0.0729\n",
            "Iteration: 27703; Percent complete: 92.3%; Average loss: 0.1107\n",
            "Iteration: 27704; Percent complete: 92.3%; Average loss: 0.1180\n",
            "Iteration: 27705; Percent complete: 92.3%; Average loss: 0.0994\n",
            "Iteration: 27706; Percent complete: 92.4%; Average loss: 0.1741\n",
            "Iteration: 27707; Percent complete: 92.4%; Average loss: 0.1052\n",
            "Iteration: 27708; Percent complete: 92.4%; Average loss: 0.1437\n",
            "Iteration: 27709; Percent complete: 92.4%; Average loss: 0.1316\n",
            "Iteration: 27710; Percent complete: 92.4%; Average loss: 0.0887\n",
            "Iteration: 27711; Percent complete: 92.4%; Average loss: 0.1964\n",
            "Iteration: 27712; Percent complete: 92.4%; Average loss: 0.1112\n",
            "Iteration: 27713; Percent complete: 92.4%; Average loss: 0.1139\n",
            "Iteration: 27714; Percent complete: 92.4%; Average loss: 0.1150\n",
            "Iteration: 27715; Percent complete: 92.4%; Average loss: 0.0891\n",
            "Iteration: 27716; Percent complete: 92.4%; Average loss: 0.1512\n",
            "Iteration: 27717; Percent complete: 92.4%; Average loss: 0.1299\n",
            "Iteration: 27718; Percent complete: 92.4%; Average loss: 0.1272\n",
            "Iteration: 27719; Percent complete: 92.4%; Average loss: 0.1484\n",
            "Iteration: 27720; Percent complete: 92.4%; Average loss: 0.1542\n",
            "Iteration: 27721; Percent complete: 92.4%; Average loss: 0.1605\n",
            "Iteration: 27722; Percent complete: 92.4%; Average loss: 0.1220\n",
            "Iteration: 27723; Percent complete: 92.4%; Average loss: 0.1560\n",
            "Iteration: 27724; Percent complete: 92.4%; Average loss: 0.1408\n",
            "Iteration: 27725; Percent complete: 92.4%; Average loss: 0.1329\n",
            "Iteration: 27726; Percent complete: 92.4%; Average loss: 0.1449\n",
            "Iteration: 27727; Percent complete: 92.4%; Average loss: 0.1323\n",
            "Iteration: 27728; Percent complete: 92.4%; Average loss: 0.0944\n",
            "Iteration: 27729; Percent complete: 92.4%; Average loss: 0.1594\n",
            "Iteration: 27730; Percent complete: 92.4%; Average loss: 0.1438\n",
            "Iteration: 27731; Percent complete: 92.4%; Average loss: 0.1273\n",
            "Iteration: 27732; Percent complete: 92.4%; Average loss: 0.0985\n",
            "Iteration: 27733; Percent complete: 92.4%; Average loss: 0.0994\n",
            "Iteration: 27734; Percent complete: 92.4%; Average loss: 0.1040\n",
            "Iteration: 27735; Percent complete: 92.5%; Average loss: 0.1059\n",
            "Iteration: 27736; Percent complete: 92.5%; Average loss: 0.0871\n",
            "Iteration: 27737; Percent complete: 92.5%; Average loss: 0.1248\n",
            "Iteration: 27738; Percent complete: 92.5%; Average loss: 0.1489\n",
            "Iteration: 27739; Percent complete: 92.5%; Average loss: 0.1331\n",
            "Iteration: 27740; Percent complete: 92.5%; Average loss: 0.1249\n",
            "Iteration: 27741; Percent complete: 92.5%; Average loss: 0.0991\n",
            "Iteration: 27742; Percent complete: 92.5%; Average loss: 0.1322\n",
            "Iteration: 27743; Percent complete: 92.5%; Average loss: 0.1009\n",
            "Iteration: 27744; Percent complete: 92.5%; Average loss: 0.1362\n",
            "Iteration: 27745; Percent complete: 92.5%; Average loss: 0.1375\n",
            "Iteration: 27746; Percent complete: 92.5%; Average loss: 0.1605\n",
            "Iteration: 27747; Percent complete: 92.5%; Average loss: 0.0956\n",
            "Iteration: 27748; Percent complete: 92.5%; Average loss: 0.1526\n",
            "Iteration: 27749; Percent complete: 92.5%; Average loss: 0.1114\n",
            "Iteration: 27750; Percent complete: 92.5%; Average loss: 0.1531\n",
            "Iteration: 27751; Percent complete: 92.5%; Average loss: 0.1294\n",
            "Iteration: 27752; Percent complete: 92.5%; Average loss: 0.1486\n",
            "Iteration: 27753; Percent complete: 92.5%; Average loss: 0.1142\n",
            "Iteration: 27754; Percent complete: 92.5%; Average loss: 0.2003\n",
            "Iteration: 27755; Percent complete: 92.5%; Average loss: 0.0833\n",
            "Iteration: 27756; Percent complete: 92.5%; Average loss: 0.1056\n",
            "Iteration: 27757; Percent complete: 92.5%; Average loss: 0.1058\n",
            "Iteration: 27758; Percent complete: 92.5%; Average loss: 0.1402\n",
            "Iteration: 27759; Percent complete: 92.5%; Average loss: 0.1333\n",
            "Iteration: 27760; Percent complete: 92.5%; Average loss: 0.1083\n",
            "Iteration: 27761; Percent complete: 92.5%; Average loss: 0.0997\n",
            "Iteration: 27762; Percent complete: 92.5%; Average loss: 0.1616\n",
            "Iteration: 27763; Percent complete: 92.5%; Average loss: 0.1208\n",
            "Iteration: 27764; Percent complete: 92.5%; Average loss: 0.1040\n",
            "Iteration: 27765; Percent complete: 92.5%; Average loss: 0.0831\n",
            "Iteration: 27766; Percent complete: 92.6%; Average loss: 0.1392\n",
            "Iteration: 27767; Percent complete: 92.6%; Average loss: 0.1265\n",
            "Iteration: 27768; Percent complete: 92.6%; Average loss: 0.1263\n",
            "Iteration: 27769; Percent complete: 92.6%; Average loss: 0.1370\n",
            "Iteration: 27770; Percent complete: 92.6%; Average loss: 0.0850\n",
            "Iteration: 27771; Percent complete: 92.6%; Average loss: 0.1720\n",
            "Iteration: 27772; Percent complete: 92.6%; Average loss: 0.1152\n",
            "Iteration: 27773; Percent complete: 92.6%; Average loss: 0.1226\n",
            "Iteration: 27774; Percent complete: 92.6%; Average loss: 0.1902\n",
            "Iteration: 27775; Percent complete: 92.6%; Average loss: 0.0980\n",
            "Iteration: 27776; Percent complete: 92.6%; Average loss: 0.1151\n",
            "Iteration: 27777; Percent complete: 92.6%; Average loss: 0.0764\n",
            "Iteration: 27778; Percent complete: 92.6%; Average loss: 0.1369\n",
            "Iteration: 27779; Percent complete: 92.6%; Average loss: 0.1332\n",
            "Iteration: 27780; Percent complete: 92.6%; Average loss: 0.1748\n",
            "Iteration: 27781; Percent complete: 92.6%; Average loss: 0.1383\n",
            "Iteration: 27782; Percent complete: 92.6%; Average loss: 0.1016\n",
            "Iteration: 27783; Percent complete: 92.6%; Average loss: 0.0917\n",
            "Iteration: 27784; Percent complete: 92.6%; Average loss: 0.1301\n",
            "Iteration: 27785; Percent complete: 92.6%; Average loss: 0.1616\n",
            "Iteration: 27786; Percent complete: 92.6%; Average loss: 0.1341\n",
            "Iteration: 27787; Percent complete: 92.6%; Average loss: 0.1107\n",
            "Iteration: 27788; Percent complete: 92.6%; Average loss: 0.0801\n",
            "Iteration: 27789; Percent complete: 92.6%; Average loss: 0.1014\n",
            "Iteration: 27790; Percent complete: 92.6%; Average loss: 0.1857\n",
            "Iteration: 27791; Percent complete: 92.6%; Average loss: 0.1423\n",
            "Iteration: 27792; Percent complete: 92.6%; Average loss: 0.1130\n",
            "Iteration: 27793; Percent complete: 92.6%; Average loss: 0.1006\n",
            "Iteration: 27794; Percent complete: 92.6%; Average loss: 0.1259\n",
            "Iteration: 27795; Percent complete: 92.7%; Average loss: 0.1236\n",
            "Iteration: 27796; Percent complete: 92.7%; Average loss: 0.1206\n",
            "Iteration: 27797; Percent complete: 92.7%; Average loss: 0.1623\n",
            "Iteration: 27798; Percent complete: 92.7%; Average loss: 0.1369\n",
            "Iteration: 27799; Percent complete: 92.7%; Average loss: 0.1409\n",
            "Iteration: 27800; Percent complete: 92.7%; Average loss: 0.0961\n",
            "Iteration: 27801; Percent complete: 92.7%; Average loss: 0.0825\n",
            "Iteration: 27802; Percent complete: 92.7%; Average loss: 0.1291\n",
            "Iteration: 27803; Percent complete: 92.7%; Average loss: 0.0975\n",
            "Iteration: 27804; Percent complete: 92.7%; Average loss: 0.1420\n",
            "Iteration: 27805; Percent complete: 92.7%; Average loss: 0.1253\n",
            "Iteration: 27806; Percent complete: 92.7%; Average loss: 0.1019\n",
            "Iteration: 27807; Percent complete: 92.7%; Average loss: 0.1181\n",
            "Iteration: 27808; Percent complete: 92.7%; Average loss: 0.1778\n",
            "Iteration: 27809; Percent complete: 92.7%; Average loss: 0.1285\n",
            "Iteration: 27810; Percent complete: 92.7%; Average loss: 0.1177\n",
            "Iteration: 27811; Percent complete: 92.7%; Average loss: 0.1472\n",
            "Iteration: 27812; Percent complete: 92.7%; Average loss: 0.1233\n",
            "Iteration: 27813; Percent complete: 92.7%; Average loss: 0.1598\n",
            "Iteration: 27814; Percent complete: 92.7%; Average loss: 0.1400\n",
            "Iteration: 27815; Percent complete: 92.7%; Average loss: 0.0934\n",
            "Iteration: 27816; Percent complete: 92.7%; Average loss: 0.1404\n",
            "Iteration: 27817; Percent complete: 92.7%; Average loss: 0.1168\n",
            "Iteration: 27818; Percent complete: 92.7%; Average loss: 0.1468\n",
            "Iteration: 27819; Percent complete: 92.7%; Average loss: 0.1707\n",
            "Iteration: 27820; Percent complete: 92.7%; Average loss: 0.1053\n",
            "Iteration: 27821; Percent complete: 92.7%; Average loss: 0.1316\n",
            "Iteration: 27822; Percent complete: 92.7%; Average loss: 0.1735\n",
            "Iteration: 27823; Percent complete: 92.7%; Average loss: 0.1382\n",
            "Iteration: 27824; Percent complete: 92.7%; Average loss: 0.1147\n",
            "Iteration: 27825; Percent complete: 92.8%; Average loss: 0.0918\n",
            "Iteration: 27826; Percent complete: 92.8%; Average loss: 0.0750\n",
            "Iteration: 27827; Percent complete: 92.8%; Average loss: 0.0880\n",
            "Iteration: 27828; Percent complete: 92.8%; Average loss: 0.1156\n",
            "Iteration: 27829; Percent complete: 92.8%; Average loss: 0.0877\n",
            "Iteration: 27830; Percent complete: 92.8%; Average loss: 0.1128\n",
            "Iteration: 27831; Percent complete: 92.8%; Average loss: 0.0948\n",
            "Iteration: 27832; Percent complete: 92.8%; Average loss: 0.1123\n",
            "Iteration: 27833; Percent complete: 92.8%; Average loss: 0.1339\n",
            "Iteration: 27834; Percent complete: 92.8%; Average loss: 0.1357\n",
            "Iteration: 27835; Percent complete: 92.8%; Average loss: 0.1197\n",
            "Iteration: 27836; Percent complete: 92.8%; Average loss: 0.1475\n",
            "Iteration: 27837; Percent complete: 92.8%; Average loss: 0.1188\n",
            "Iteration: 27838; Percent complete: 92.8%; Average loss: 0.1164\n",
            "Iteration: 27839; Percent complete: 92.8%; Average loss: 0.1488\n",
            "Iteration: 27840; Percent complete: 92.8%; Average loss: 0.1304\n",
            "Iteration: 27841; Percent complete: 92.8%; Average loss: 0.1634\n",
            "Iteration: 27842; Percent complete: 92.8%; Average loss: 0.1155\n",
            "Iteration: 27843; Percent complete: 92.8%; Average loss: 0.1275\n",
            "Iteration: 27844; Percent complete: 92.8%; Average loss: 0.1170\n",
            "Iteration: 27845; Percent complete: 92.8%; Average loss: 0.1129\n",
            "Iteration: 27846; Percent complete: 92.8%; Average loss: 0.1211\n",
            "Iteration: 27847; Percent complete: 92.8%; Average loss: 0.1139\n",
            "Iteration: 27848; Percent complete: 92.8%; Average loss: 0.1214\n",
            "Iteration: 27849; Percent complete: 92.8%; Average loss: 0.1255\n",
            "Iteration: 27850; Percent complete: 92.8%; Average loss: 0.1373\n",
            "Iteration: 27851; Percent complete: 92.8%; Average loss: 0.1257\n",
            "Iteration: 27852; Percent complete: 92.8%; Average loss: 0.1368\n",
            "Iteration: 27853; Percent complete: 92.8%; Average loss: 0.1799\n",
            "Iteration: 27854; Percent complete: 92.8%; Average loss: 0.1380\n",
            "Iteration: 27855; Percent complete: 92.8%; Average loss: 0.0943\n",
            "Iteration: 27856; Percent complete: 92.9%; Average loss: 0.1166\n",
            "Iteration: 27857; Percent complete: 92.9%; Average loss: 0.1568\n",
            "Iteration: 27858; Percent complete: 92.9%; Average loss: 0.0996\n",
            "Iteration: 27859; Percent complete: 92.9%; Average loss: 0.1336\n",
            "Iteration: 27860; Percent complete: 92.9%; Average loss: 0.1297\n",
            "Iteration: 27861; Percent complete: 92.9%; Average loss: 0.1353\n",
            "Iteration: 27862; Percent complete: 92.9%; Average loss: 0.1353\n",
            "Iteration: 27863; Percent complete: 92.9%; Average loss: 0.1375\n",
            "Iteration: 27864; Percent complete: 92.9%; Average loss: 0.1598\n",
            "Iteration: 27865; Percent complete: 92.9%; Average loss: 0.0894\n",
            "Iteration: 27866; Percent complete: 92.9%; Average loss: 0.1076\n",
            "Iteration: 27867; Percent complete: 92.9%; Average loss: 0.1212\n",
            "Iteration: 27868; Percent complete: 92.9%; Average loss: 0.1189\n",
            "Iteration: 27869; Percent complete: 92.9%; Average loss: 0.1181\n",
            "Iteration: 27870; Percent complete: 92.9%; Average loss: 0.1100\n",
            "Iteration: 27871; Percent complete: 92.9%; Average loss: 0.1460\n",
            "Iteration: 27872; Percent complete: 92.9%; Average loss: 0.1534\n",
            "Iteration: 27873; Percent complete: 92.9%; Average loss: 0.1428\n",
            "Iteration: 27874; Percent complete: 92.9%; Average loss: 0.1090\n",
            "Iteration: 27875; Percent complete: 92.9%; Average loss: 0.0974\n",
            "Iteration: 27876; Percent complete: 92.9%; Average loss: 0.1390\n",
            "Iteration: 27877; Percent complete: 92.9%; Average loss: 0.0928\n",
            "Iteration: 27878; Percent complete: 92.9%; Average loss: 0.1004\n",
            "Iteration: 27879; Percent complete: 92.9%; Average loss: 0.2056\n",
            "Iteration: 27880; Percent complete: 92.9%; Average loss: 0.1314\n",
            "Iteration: 27881; Percent complete: 92.9%; Average loss: 0.1390\n",
            "Iteration: 27882; Percent complete: 92.9%; Average loss: 0.1302\n",
            "Iteration: 27883; Percent complete: 92.9%; Average loss: 0.1667\n",
            "Iteration: 27884; Percent complete: 92.9%; Average loss: 0.1207\n",
            "Iteration: 27885; Percent complete: 93.0%; Average loss: 0.1023\n",
            "Iteration: 27886; Percent complete: 93.0%; Average loss: 0.1257\n",
            "Iteration: 27887; Percent complete: 93.0%; Average loss: 0.1359\n",
            "Iteration: 27888; Percent complete: 93.0%; Average loss: 0.1437\n",
            "Iteration: 27889; Percent complete: 93.0%; Average loss: 0.1216\n",
            "Iteration: 27890; Percent complete: 93.0%; Average loss: 0.1367\n",
            "Iteration: 27891; Percent complete: 93.0%; Average loss: 0.1672\n",
            "Iteration: 27892; Percent complete: 93.0%; Average loss: 0.1220\n",
            "Iteration: 27893; Percent complete: 93.0%; Average loss: 0.0960\n",
            "Iteration: 27894; Percent complete: 93.0%; Average loss: 0.1352\n",
            "Iteration: 27895; Percent complete: 93.0%; Average loss: 0.1510\n",
            "Iteration: 27896; Percent complete: 93.0%; Average loss: 0.1756\n",
            "Iteration: 27897; Percent complete: 93.0%; Average loss: 0.1037\n",
            "Iteration: 27898; Percent complete: 93.0%; Average loss: 0.1535\n",
            "Iteration: 27899; Percent complete: 93.0%; Average loss: 0.1482\n",
            "Iteration: 27900; Percent complete: 93.0%; Average loss: 0.1252\n",
            "Iteration: 27901; Percent complete: 93.0%; Average loss: 0.1409\n",
            "Iteration: 27902; Percent complete: 93.0%; Average loss: 0.1078\n",
            "Iteration: 27903; Percent complete: 93.0%; Average loss: 0.1336\n",
            "Iteration: 27904; Percent complete: 93.0%; Average loss: 0.1419\n",
            "Iteration: 27905; Percent complete: 93.0%; Average loss: 0.1232\n",
            "Iteration: 27906; Percent complete: 93.0%; Average loss: 0.1423\n",
            "Iteration: 27907; Percent complete: 93.0%; Average loss: 0.0891\n",
            "Iteration: 27908; Percent complete: 93.0%; Average loss: 0.1119\n",
            "Iteration: 27909; Percent complete: 93.0%; Average loss: 0.1091\n",
            "Iteration: 27910; Percent complete: 93.0%; Average loss: 0.0901\n",
            "Iteration: 27911; Percent complete: 93.0%; Average loss: 0.1080\n",
            "Iteration: 27912; Percent complete: 93.0%; Average loss: 0.1553\n",
            "Iteration: 27913; Percent complete: 93.0%; Average loss: 0.1324\n",
            "Iteration: 27914; Percent complete: 93.0%; Average loss: 0.1366\n",
            "Iteration: 27915; Percent complete: 93.0%; Average loss: 0.1032\n",
            "Iteration: 27916; Percent complete: 93.1%; Average loss: 0.1320\n",
            "Iteration: 27917; Percent complete: 93.1%; Average loss: 0.1448\n",
            "Iteration: 27918; Percent complete: 93.1%; Average loss: 0.1250\n",
            "Iteration: 27919; Percent complete: 93.1%; Average loss: 0.1696\n",
            "Iteration: 27920; Percent complete: 93.1%; Average loss: 0.1288\n",
            "Iteration: 27921; Percent complete: 93.1%; Average loss: 0.0969\n",
            "Iteration: 27922; Percent complete: 93.1%; Average loss: 0.1193\n",
            "Iteration: 27923; Percent complete: 93.1%; Average loss: 0.0893\n",
            "Iteration: 27924; Percent complete: 93.1%; Average loss: 0.1815\n",
            "Iteration: 27925; Percent complete: 93.1%; Average loss: 0.0816\n",
            "Iteration: 27926; Percent complete: 93.1%; Average loss: 0.1066\n",
            "Iteration: 27927; Percent complete: 93.1%; Average loss: 0.1159\n",
            "Iteration: 27928; Percent complete: 93.1%; Average loss: 0.1160\n",
            "Iteration: 27929; Percent complete: 93.1%; Average loss: 0.1057\n",
            "Iteration: 27930; Percent complete: 93.1%; Average loss: 0.0799\n",
            "Iteration: 27931; Percent complete: 93.1%; Average loss: 0.1386\n",
            "Iteration: 27932; Percent complete: 93.1%; Average loss: 0.0892\n",
            "Iteration: 27933; Percent complete: 93.1%; Average loss: 0.1501\n",
            "Iteration: 27934; Percent complete: 93.1%; Average loss: 0.1073\n",
            "Iteration: 27935; Percent complete: 93.1%; Average loss: 0.1136\n",
            "Iteration: 27936; Percent complete: 93.1%; Average loss: 0.1087\n",
            "Iteration: 27937; Percent complete: 93.1%; Average loss: 0.1569\n",
            "Iteration: 27938; Percent complete: 93.1%; Average loss: 0.0923\n",
            "Iteration: 27939; Percent complete: 93.1%; Average loss: 0.0944\n",
            "Iteration: 27940; Percent complete: 93.1%; Average loss: 0.1275\n",
            "Iteration: 27941; Percent complete: 93.1%; Average loss: 0.1543\n",
            "Iteration: 27942; Percent complete: 93.1%; Average loss: 0.1299\n",
            "Iteration: 27943; Percent complete: 93.1%; Average loss: 0.1492\n",
            "Iteration: 27944; Percent complete: 93.1%; Average loss: 0.0903\n",
            "Iteration: 27945; Percent complete: 93.2%; Average loss: 0.1290\n",
            "Iteration: 27946; Percent complete: 93.2%; Average loss: 0.1493\n",
            "Iteration: 27947; Percent complete: 93.2%; Average loss: 0.1330\n",
            "Iteration: 27948; Percent complete: 93.2%; Average loss: 0.0936\n",
            "Iteration: 27949; Percent complete: 93.2%; Average loss: 0.1233\n",
            "Iteration: 27950; Percent complete: 93.2%; Average loss: 0.1264\n",
            "Iteration: 27951; Percent complete: 93.2%; Average loss: 0.0975\n",
            "Iteration: 27952; Percent complete: 93.2%; Average loss: 0.1218\n",
            "Iteration: 27953; Percent complete: 93.2%; Average loss: 0.1575\n",
            "Iteration: 27954; Percent complete: 93.2%; Average loss: 0.1501\n",
            "Iteration: 27955; Percent complete: 93.2%; Average loss: 0.1515\n",
            "Iteration: 27956; Percent complete: 93.2%; Average loss: 0.1147\n",
            "Iteration: 27957; Percent complete: 93.2%; Average loss: 0.1162\n",
            "Iteration: 27958; Percent complete: 93.2%; Average loss: 0.1287\n",
            "Iteration: 27959; Percent complete: 93.2%; Average loss: 0.0976\n",
            "Iteration: 27960; Percent complete: 93.2%; Average loss: 0.1054\n",
            "Iteration: 27961; Percent complete: 93.2%; Average loss: 0.1292\n",
            "Iteration: 27962; Percent complete: 93.2%; Average loss: 0.1435\n",
            "Iteration: 27963; Percent complete: 93.2%; Average loss: 0.0940\n",
            "Iteration: 27964; Percent complete: 93.2%; Average loss: 0.0982\n",
            "Iteration: 27965; Percent complete: 93.2%; Average loss: 0.1564\n",
            "Iteration: 27966; Percent complete: 93.2%; Average loss: 0.1353\n",
            "Iteration: 27967; Percent complete: 93.2%; Average loss: 0.1082\n",
            "Iteration: 27968; Percent complete: 93.2%; Average loss: 0.1153\n",
            "Iteration: 27969; Percent complete: 93.2%; Average loss: 0.0953\n",
            "Iteration: 27970; Percent complete: 93.2%; Average loss: 0.1087\n",
            "Iteration: 27971; Percent complete: 93.2%; Average loss: 0.1363\n",
            "Iteration: 27972; Percent complete: 93.2%; Average loss: 0.0983\n",
            "Iteration: 27973; Percent complete: 93.2%; Average loss: 0.1325\n",
            "Iteration: 27974; Percent complete: 93.2%; Average loss: 0.1107\n",
            "Iteration: 27975; Percent complete: 93.2%; Average loss: 0.1256\n",
            "Iteration: 27976; Percent complete: 93.3%; Average loss: 0.1233\n",
            "Iteration: 27977; Percent complete: 93.3%; Average loss: 0.1161\n",
            "Iteration: 27978; Percent complete: 93.3%; Average loss: 0.1506\n",
            "Iteration: 27979; Percent complete: 93.3%; Average loss: 0.1203\n",
            "Iteration: 27980; Percent complete: 93.3%; Average loss: 0.1055\n",
            "Iteration: 27981; Percent complete: 93.3%; Average loss: 0.1087\n",
            "Iteration: 27982; Percent complete: 93.3%; Average loss: 0.1244\n",
            "Iteration: 27983; Percent complete: 93.3%; Average loss: 0.1632\n",
            "Iteration: 27984; Percent complete: 93.3%; Average loss: 0.1595\n",
            "Iteration: 27985; Percent complete: 93.3%; Average loss: 0.0982\n",
            "Iteration: 27986; Percent complete: 93.3%; Average loss: 0.1251\n",
            "Iteration: 27987; Percent complete: 93.3%; Average loss: 0.1108\n",
            "Iteration: 27988; Percent complete: 93.3%; Average loss: 0.0851\n",
            "Iteration: 27989; Percent complete: 93.3%; Average loss: 0.1589\n",
            "Iteration: 27990; Percent complete: 93.3%; Average loss: 0.1607\n",
            "Iteration: 27991; Percent complete: 93.3%; Average loss: 0.0910\n",
            "Iteration: 27992; Percent complete: 93.3%; Average loss: 0.1622\n",
            "Iteration: 27993; Percent complete: 93.3%; Average loss: 0.1085\n",
            "Iteration: 27994; Percent complete: 93.3%; Average loss: 0.0880\n",
            "Iteration: 27995; Percent complete: 93.3%; Average loss: 0.1213\n",
            "Iteration: 27996; Percent complete: 93.3%; Average loss: 0.1165\n",
            "Iteration: 27997; Percent complete: 93.3%; Average loss: 0.1352\n",
            "Iteration: 27998; Percent complete: 93.3%; Average loss: 0.1098\n",
            "Iteration: 27999; Percent complete: 93.3%; Average loss: 0.0734\n",
            "Iteration: 28000; Percent complete: 93.3%; Average loss: 0.1745\n",
            "Iteration: 28001; Percent complete: 93.3%; Average loss: 0.1656\n",
            "Iteration: 28002; Percent complete: 93.3%; Average loss: 0.0892\n",
            "Iteration: 28003; Percent complete: 93.3%; Average loss: 0.1303\n",
            "Iteration: 28004; Percent complete: 93.3%; Average loss: 0.1038\n",
            "Iteration: 28005; Percent complete: 93.3%; Average loss: 0.0963\n",
            "Iteration: 28006; Percent complete: 93.4%; Average loss: 0.0863\n",
            "Iteration: 28007; Percent complete: 93.4%; Average loss: 0.1570\n",
            "Iteration: 28008; Percent complete: 93.4%; Average loss: 0.1009\n",
            "Iteration: 28009; Percent complete: 93.4%; Average loss: 0.1057\n",
            "Iteration: 28010; Percent complete: 93.4%; Average loss: 0.1507\n",
            "Iteration: 28011; Percent complete: 93.4%; Average loss: 0.1355\n",
            "Iteration: 28012; Percent complete: 93.4%; Average loss: 0.0954\n",
            "Iteration: 28013; Percent complete: 93.4%; Average loss: 0.0942\n",
            "Iteration: 28014; Percent complete: 93.4%; Average loss: 0.1164\n",
            "Iteration: 28015; Percent complete: 93.4%; Average loss: 0.1616\n",
            "Iteration: 28016; Percent complete: 93.4%; Average loss: 0.0742\n",
            "Iteration: 28017; Percent complete: 93.4%; Average loss: 0.1630\n",
            "Iteration: 28018; Percent complete: 93.4%; Average loss: 0.0778\n",
            "Iteration: 28019; Percent complete: 93.4%; Average loss: 0.1645\n",
            "Iteration: 28020; Percent complete: 93.4%; Average loss: 0.1044\n",
            "Iteration: 28021; Percent complete: 93.4%; Average loss: 0.1282\n",
            "Iteration: 28022; Percent complete: 93.4%; Average loss: 0.1123\n",
            "Iteration: 28023; Percent complete: 93.4%; Average loss: 0.1344\n",
            "Iteration: 28024; Percent complete: 93.4%; Average loss: 0.0937\n",
            "Iteration: 28025; Percent complete: 93.4%; Average loss: 0.1064\n",
            "Iteration: 28026; Percent complete: 93.4%; Average loss: 0.1229\n",
            "Iteration: 28027; Percent complete: 93.4%; Average loss: 0.1238\n",
            "Iteration: 28028; Percent complete: 93.4%; Average loss: 0.0952\n",
            "Iteration: 28029; Percent complete: 93.4%; Average loss: 0.1603\n",
            "Iteration: 28030; Percent complete: 93.4%; Average loss: 0.1207\n",
            "Iteration: 28031; Percent complete: 93.4%; Average loss: 0.1640\n",
            "Iteration: 28032; Percent complete: 93.4%; Average loss: 0.0929\n",
            "Iteration: 28033; Percent complete: 93.4%; Average loss: 0.1548\n",
            "Iteration: 28034; Percent complete: 93.4%; Average loss: 0.1170\n",
            "Iteration: 28035; Percent complete: 93.5%; Average loss: 0.1145\n",
            "Iteration: 28036; Percent complete: 93.5%; Average loss: 0.1087\n",
            "Iteration: 28037; Percent complete: 93.5%; Average loss: 0.2083\n",
            "Iteration: 28038; Percent complete: 93.5%; Average loss: 0.1765\n",
            "Iteration: 28039; Percent complete: 93.5%; Average loss: 0.1331\n",
            "Iteration: 28040; Percent complete: 93.5%; Average loss: 0.1445\n",
            "Iteration: 28041; Percent complete: 93.5%; Average loss: 0.1445\n",
            "Iteration: 28042; Percent complete: 93.5%; Average loss: 0.0934\n",
            "Iteration: 28043; Percent complete: 93.5%; Average loss: 0.1517\n",
            "Iteration: 28044; Percent complete: 93.5%; Average loss: 0.1211\n",
            "Iteration: 28045; Percent complete: 93.5%; Average loss: 0.1589\n",
            "Iteration: 28046; Percent complete: 93.5%; Average loss: 0.1047\n",
            "Iteration: 28047; Percent complete: 93.5%; Average loss: 0.1184\n",
            "Iteration: 28048; Percent complete: 93.5%; Average loss: 0.1353\n",
            "Iteration: 28049; Percent complete: 93.5%; Average loss: 0.0854\n",
            "Iteration: 28050; Percent complete: 93.5%; Average loss: 0.1613\n",
            "Iteration: 28051; Percent complete: 93.5%; Average loss: 0.1264\n",
            "Iteration: 28052; Percent complete: 93.5%; Average loss: 0.1242\n",
            "Iteration: 28053; Percent complete: 93.5%; Average loss: 0.1077\n",
            "Iteration: 28054; Percent complete: 93.5%; Average loss: 0.1333\n",
            "Iteration: 28055; Percent complete: 93.5%; Average loss: 0.1544\n",
            "Iteration: 28056; Percent complete: 93.5%; Average loss: 0.1280\n",
            "Iteration: 28057; Percent complete: 93.5%; Average loss: 0.1323\n",
            "Iteration: 28058; Percent complete: 93.5%; Average loss: 0.1404\n",
            "Iteration: 28059; Percent complete: 93.5%; Average loss: 0.0771\n",
            "Iteration: 28060; Percent complete: 93.5%; Average loss: 0.1100\n",
            "Iteration: 28061; Percent complete: 93.5%; Average loss: 0.1063\n",
            "Iteration: 28062; Percent complete: 93.5%; Average loss: 0.1275\n",
            "Iteration: 28063; Percent complete: 93.5%; Average loss: 0.1227\n",
            "Iteration: 28064; Percent complete: 93.5%; Average loss: 0.0991\n",
            "Iteration: 28065; Percent complete: 93.5%; Average loss: 0.0958\n",
            "Iteration: 28066; Percent complete: 93.6%; Average loss: 0.1174\n",
            "Iteration: 28067; Percent complete: 93.6%; Average loss: 0.1193\n",
            "Iteration: 28068; Percent complete: 93.6%; Average loss: 0.1366\n",
            "Iteration: 28069; Percent complete: 93.6%; Average loss: 0.1467\n",
            "Iteration: 28070; Percent complete: 93.6%; Average loss: 0.1338\n",
            "Iteration: 28071; Percent complete: 93.6%; Average loss: 0.1406\n",
            "Iteration: 28072; Percent complete: 93.6%; Average loss: 0.1267\n",
            "Iteration: 28073; Percent complete: 93.6%; Average loss: 0.1140\n",
            "Iteration: 28074; Percent complete: 93.6%; Average loss: 0.1301\n",
            "Iteration: 28075; Percent complete: 93.6%; Average loss: 0.0912\n",
            "Iteration: 28076; Percent complete: 93.6%; Average loss: 0.0828\n",
            "Iteration: 28077; Percent complete: 93.6%; Average loss: 0.1369\n",
            "Iteration: 28078; Percent complete: 93.6%; Average loss: 0.0876\n",
            "Iteration: 28079; Percent complete: 93.6%; Average loss: 0.1414\n",
            "Iteration: 28080; Percent complete: 93.6%; Average loss: 0.1488\n",
            "Iteration: 28081; Percent complete: 93.6%; Average loss: 0.1128\n",
            "Iteration: 28082; Percent complete: 93.6%; Average loss: 0.0642\n",
            "Iteration: 28083; Percent complete: 93.6%; Average loss: 0.1550\n",
            "Iteration: 28084; Percent complete: 93.6%; Average loss: 0.1216\n",
            "Iteration: 28085; Percent complete: 93.6%; Average loss: 0.1304\n",
            "Iteration: 28086; Percent complete: 93.6%; Average loss: 0.2182\n",
            "Iteration: 28087; Percent complete: 93.6%; Average loss: 0.1027\n",
            "Iteration: 28088; Percent complete: 93.6%; Average loss: 0.1380\n",
            "Iteration: 28089; Percent complete: 93.6%; Average loss: 0.0984\n",
            "Iteration: 28090; Percent complete: 93.6%; Average loss: 0.1541\n",
            "Iteration: 28091; Percent complete: 93.6%; Average loss: 0.1301\n",
            "Iteration: 28092; Percent complete: 93.6%; Average loss: 0.1573\n",
            "Iteration: 28093; Percent complete: 93.6%; Average loss: 0.1071\n",
            "Iteration: 28094; Percent complete: 93.6%; Average loss: 0.1717\n",
            "Iteration: 28095; Percent complete: 93.7%; Average loss: 0.1366\n",
            "Iteration: 28096; Percent complete: 93.7%; Average loss: 0.1200\n",
            "Iteration: 28097; Percent complete: 93.7%; Average loss: 0.1389\n",
            "Iteration: 28098; Percent complete: 93.7%; Average loss: 0.1717\n",
            "Iteration: 28099; Percent complete: 93.7%; Average loss: 0.1520\n",
            "Iteration: 28100; Percent complete: 93.7%; Average loss: 0.1595\n",
            "Iteration: 28101; Percent complete: 93.7%; Average loss: 0.1488\n",
            "Iteration: 28102; Percent complete: 93.7%; Average loss: 0.1241\n",
            "Iteration: 28103; Percent complete: 93.7%; Average loss: 0.0920\n",
            "Iteration: 28104; Percent complete: 93.7%; Average loss: 0.1507\n",
            "Iteration: 28105; Percent complete: 93.7%; Average loss: 0.1096\n",
            "Iteration: 28106; Percent complete: 93.7%; Average loss: 0.1341\n",
            "Iteration: 28107; Percent complete: 93.7%; Average loss: 0.1397\n",
            "Iteration: 28108; Percent complete: 93.7%; Average loss: 0.1582\n",
            "Iteration: 28109; Percent complete: 93.7%; Average loss: 0.1075\n",
            "Iteration: 28110; Percent complete: 93.7%; Average loss: 0.0969\n",
            "Iteration: 28111; Percent complete: 93.7%; Average loss: 0.0731\n",
            "Iteration: 28112; Percent complete: 93.7%; Average loss: 0.1222\n",
            "Iteration: 28113; Percent complete: 93.7%; Average loss: 0.1640\n",
            "Iteration: 28114; Percent complete: 93.7%; Average loss: 0.1014\n",
            "Iteration: 28115; Percent complete: 93.7%; Average loss: 0.1473\n",
            "Iteration: 28116; Percent complete: 93.7%; Average loss: 0.1634\n",
            "Iteration: 28117; Percent complete: 93.7%; Average loss: 0.1391\n",
            "Iteration: 28118; Percent complete: 93.7%; Average loss: 0.1085\n",
            "Iteration: 28119; Percent complete: 93.7%; Average loss: 0.0823\n",
            "Iteration: 28120; Percent complete: 93.7%; Average loss: 0.1458\n",
            "Iteration: 28121; Percent complete: 93.7%; Average loss: 0.1115\n",
            "Iteration: 28122; Percent complete: 93.7%; Average loss: 0.0836\n",
            "Iteration: 28123; Percent complete: 93.7%; Average loss: 0.1167\n",
            "Iteration: 28124; Percent complete: 93.7%; Average loss: 0.0763\n",
            "Iteration: 28125; Percent complete: 93.8%; Average loss: 0.1049\n",
            "Iteration: 28126; Percent complete: 93.8%; Average loss: 0.1051\n",
            "Iteration: 28127; Percent complete: 93.8%; Average loss: 0.1306\n",
            "Iteration: 28128; Percent complete: 93.8%; Average loss: 0.1387\n",
            "Iteration: 28129; Percent complete: 93.8%; Average loss: 0.0897\n",
            "Iteration: 28130; Percent complete: 93.8%; Average loss: 0.1365\n",
            "Iteration: 28131; Percent complete: 93.8%; Average loss: 0.1093\n",
            "Iteration: 28132; Percent complete: 93.8%; Average loss: 0.2018\n",
            "Iteration: 28133; Percent complete: 93.8%; Average loss: 0.0905\n",
            "Iteration: 28134; Percent complete: 93.8%; Average loss: 0.0802\n",
            "Iteration: 28135; Percent complete: 93.8%; Average loss: 0.1005\n",
            "Iteration: 28136; Percent complete: 93.8%; Average loss: 0.1299\n",
            "Iteration: 28137; Percent complete: 93.8%; Average loss: 0.0995\n",
            "Iteration: 28138; Percent complete: 93.8%; Average loss: 0.1380\n",
            "Iteration: 28139; Percent complete: 93.8%; Average loss: 0.1153\n",
            "Iteration: 28140; Percent complete: 93.8%; Average loss: 0.0982\n",
            "Iteration: 28141; Percent complete: 93.8%; Average loss: 0.1308\n",
            "Iteration: 28142; Percent complete: 93.8%; Average loss: 0.0862\n",
            "Iteration: 28143; Percent complete: 93.8%; Average loss: 0.1006\n",
            "Iteration: 28144; Percent complete: 93.8%; Average loss: 0.0962\n",
            "Iteration: 28145; Percent complete: 93.8%; Average loss: 0.0672\n",
            "Iteration: 28146; Percent complete: 93.8%; Average loss: 0.1331\n",
            "Iteration: 28147; Percent complete: 93.8%; Average loss: 0.1306\n",
            "Iteration: 28148; Percent complete: 93.8%; Average loss: 0.1754\n",
            "Iteration: 28149; Percent complete: 93.8%; Average loss: 0.1529\n",
            "Iteration: 28150; Percent complete: 93.8%; Average loss: 0.1780\n",
            "Iteration: 28151; Percent complete: 93.8%; Average loss: 0.1077\n",
            "Iteration: 28152; Percent complete: 93.8%; Average loss: 0.0996\n",
            "Iteration: 28153; Percent complete: 93.8%; Average loss: 0.1008\n",
            "Iteration: 28154; Percent complete: 93.8%; Average loss: 0.1229\n",
            "Iteration: 28155; Percent complete: 93.8%; Average loss: 0.1451\n",
            "Iteration: 28156; Percent complete: 93.9%; Average loss: 0.1015\n",
            "Iteration: 28157; Percent complete: 93.9%; Average loss: 0.1058\n",
            "Iteration: 28158; Percent complete: 93.9%; Average loss: 0.0982\n",
            "Iteration: 28159; Percent complete: 93.9%; Average loss: 0.1039\n",
            "Iteration: 28160; Percent complete: 93.9%; Average loss: 0.1512\n",
            "Iteration: 28161; Percent complete: 93.9%; Average loss: 0.1314\n",
            "Iteration: 28162; Percent complete: 93.9%; Average loss: 0.1331\n",
            "Iteration: 28163; Percent complete: 93.9%; Average loss: 0.1588\n",
            "Iteration: 28164; Percent complete: 93.9%; Average loss: 0.1748\n",
            "Iteration: 28165; Percent complete: 93.9%; Average loss: 0.1417\n",
            "Iteration: 28166; Percent complete: 93.9%; Average loss: 0.1384\n",
            "Iteration: 28167; Percent complete: 93.9%; Average loss: 0.1153\n",
            "Iteration: 28168; Percent complete: 93.9%; Average loss: 0.1187\n",
            "Iteration: 28169; Percent complete: 93.9%; Average loss: 0.1446\n",
            "Iteration: 28170; Percent complete: 93.9%; Average loss: 0.0780\n",
            "Iteration: 28171; Percent complete: 93.9%; Average loss: 0.1060\n",
            "Iteration: 28172; Percent complete: 93.9%; Average loss: 0.1305\n",
            "Iteration: 28173; Percent complete: 93.9%; Average loss: 0.1142\n",
            "Iteration: 28174; Percent complete: 93.9%; Average loss: 0.1171\n",
            "Iteration: 28175; Percent complete: 93.9%; Average loss: 0.0965\n",
            "Iteration: 28176; Percent complete: 93.9%; Average loss: 0.1217\n",
            "Iteration: 28177; Percent complete: 93.9%; Average loss: 0.1742\n",
            "Iteration: 28178; Percent complete: 93.9%; Average loss: 0.1037\n",
            "Iteration: 28179; Percent complete: 93.9%; Average loss: 0.1446\n",
            "Iteration: 28180; Percent complete: 93.9%; Average loss: 0.0869\n",
            "Iteration: 28181; Percent complete: 93.9%; Average loss: 0.1027\n",
            "Iteration: 28182; Percent complete: 93.9%; Average loss: 0.1001\n",
            "Iteration: 28183; Percent complete: 93.9%; Average loss: 0.1513\n",
            "Iteration: 28184; Percent complete: 93.9%; Average loss: 0.1690\n",
            "Iteration: 28185; Percent complete: 94.0%; Average loss: 0.1030\n",
            "Iteration: 28186; Percent complete: 94.0%; Average loss: 0.0910\n",
            "Iteration: 28187; Percent complete: 94.0%; Average loss: 0.1095\n",
            "Iteration: 28188; Percent complete: 94.0%; Average loss: 0.0899\n",
            "Iteration: 28189; Percent complete: 94.0%; Average loss: 0.1376\n",
            "Iteration: 28190; Percent complete: 94.0%; Average loss: 0.1402\n",
            "Iteration: 28191; Percent complete: 94.0%; Average loss: 0.1239\n",
            "Iteration: 28192; Percent complete: 94.0%; Average loss: 0.1417\n",
            "Iteration: 28193; Percent complete: 94.0%; Average loss: 0.1164\n",
            "Iteration: 28194; Percent complete: 94.0%; Average loss: 0.1757\n",
            "Iteration: 28195; Percent complete: 94.0%; Average loss: 0.1213\n",
            "Iteration: 28196; Percent complete: 94.0%; Average loss: 0.0986\n",
            "Iteration: 28197; Percent complete: 94.0%; Average loss: 0.1478\n",
            "Iteration: 28198; Percent complete: 94.0%; Average loss: 0.1476\n",
            "Iteration: 28199; Percent complete: 94.0%; Average loss: 0.1030\n",
            "Iteration: 28200; Percent complete: 94.0%; Average loss: 0.1563\n",
            "Iteration: 28201; Percent complete: 94.0%; Average loss: 0.1085\n",
            "Iteration: 28202; Percent complete: 94.0%; Average loss: 0.1581\n",
            "Iteration: 28203; Percent complete: 94.0%; Average loss: 0.1818\n",
            "Iteration: 28204; Percent complete: 94.0%; Average loss: 0.1532\n",
            "Iteration: 28205; Percent complete: 94.0%; Average loss: 0.1115\n",
            "Iteration: 28206; Percent complete: 94.0%; Average loss: 0.0724\n",
            "Iteration: 28207; Percent complete: 94.0%; Average loss: 0.0945\n",
            "Iteration: 28208; Percent complete: 94.0%; Average loss: 0.0890\n",
            "Iteration: 28209; Percent complete: 94.0%; Average loss: 0.0680\n",
            "Iteration: 28210; Percent complete: 94.0%; Average loss: 0.0975\n",
            "Iteration: 28211; Percent complete: 94.0%; Average loss: 0.0708\n",
            "Iteration: 28212; Percent complete: 94.0%; Average loss: 0.1081\n",
            "Iteration: 28213; Percent complete: 94.0%; Average loss: 0.1505\n",
            "Iteration: 28214; Percent complete: 94.0%; Average loss: 0.1008\n",
            "Iteration: 28215; Percent complete: 94.0%; Average loss: 0.0988\n",
            "Iteration: 28216; Percent complete: 94.1%; Average loss: 0.0912\n",
            "Iteration: 28217; Percent complete: 94.1%; Average loss: 0.1426\n",
            "Iteration: 28218; Percent complete: 94.1%; Average loss: 0.1155\n",
            "Iteration: 28219; Percent complete: 94.1%; Average loss: 0.0754\n",
            "Iteration: 28220; Percent complete: 94.1%; Average loss: 0.1216\n",
            "Iteration: 28221; Percent complete: 94.1%; Average loss: 0.1149\n",
            "Iteration: 28222; Percent complete: 94.1%; Average loss: 0.1521\n",
            "Iteration: 28223; Percent complete: 94.1%; Average loss: 0.1289\n",
            "Iteration: 28224; Percent complete: 94.1%; Average loss: 0.0910\n",
            "Iteration: 28225; Percent complete: 94.1%; Average loss: 0.1260\n",
            "Iteration: 28226; Percent complete: 94.1%; Average loss: 0.1215\n",
            "Iteration: 28227; Percent complete: 94.1%; Average loss: 0.1578\n",
            "Iteration: 28228; Percent complete: 94.1%; Average loss: 0.0777\n",
            "Iteration: 28229; Percent complete: 94.1%; Average loss: 0.1362\n",
            "Iteration: 28230; Percent complete: 94.1%; Average loss: 0.1254\n",
            "Iteration: 28231; Percent complete: 94.1%; Average loss: 0.1274\n",
            "Iteration: 28232; Percent complete: 94.1%; Average loss: 0.1238\n",
            "Iteration: 28233; Percent complete: 94.1%; Average loss: 0.1213\n",
            "Iteration: 28234; Percent complete: 94.1%; Average loss: 0.0597\n",
            "Iteration: 28235; Percent complete: 94.1%; Average loss: 0.1528\n",
            "Iteration: 28236; Percent complete: 94.1%; Average loss: 0.1029\n",
            "Iteration: 28237; Percent complete: 94.1%; Average loss: 0.1258\n",
            "Iteration: 28238; Percent complete: 94.1%; Average loss: 0.0980\n",
            "Iteration: 28239; Percent complete: 94.1%; Average loss: 0.1123\n",
            "Iteration: 28240; Percent complete: 94.1%; Average loss: 0.1432\n",
            "Iteration: 28241; Percent complete: 94.1%; Average loss: 0.1068\n",
            "Iteration: 28242; Percent complete: 94.1%; Average loss: 0.1203\n",
            "Iteration: 28243; Percent complete: 94.1%; Average loss: 0.1237\n",
            "Iteration: 28244; Percent complete: 94.1%; Average loss: 0.1810\n",
            "Iteration: 28245; Percent complete: 94.2%; Average loss: 0.1225\n",
            "Iteration: 28246; Percent complete: 94.2%; Average loss: 0.1346\n",
            "Iteration: 28247; Percent complete: 94.2%; Average loss: 0.1038\n",
            "Iteration: 28248; Percent complete: 94.2%; Average loss: 0.1366\n",
            "Iteration: 28249; Percent complete: 94.2%; Average loss: 0.1289\n",
            "Iteration: 28250; Percent complete: 94.2%; Average loss: 0.1245\n",
            "Iteration: 28251; Percent complete: 94.2%; Average loss: 0.1361\n",
            "Iteration: 28252; Percent complete: 94.2%; Average loss: 0.1311\n",
            "Iteration: 28253; Percent complete: 94.2%; Average loss: 0.1172\n",
            "Iteration: 28254; Percent complete: 94.2%; Average loss: 0.0552\n",
            "Iteration: 28255; Percent complete: 94.2%; Average loss: 0.1573\n",
            "Iteration: 28256; Percent complete: 94.2%; Average loss: 0.0823\n",
            "Iteration: 28257; Percent complete: 94.2%; Average loss: 0.1465\n",
            "Iteration: 28258; Percent complete: 94.2%; Average loss: 0.1056\n",
            "Iteration: 28259; Percent complete: 94.2%; Average loss: 0.1016\n",
            "Iteration: 28260; Percent complete: 94.2%; Average loss: 0.0978\n",
            "Iteration: 28261; Percent complete: 94.2%; Average loss: 0.1177\n",
            "Iteration: 28262; Percent complete: 94.2%; Average loss: 0.0927\n",
            "Iteration: 28263; Percent complete: 94.2%; Average loss: 0.1352\n",
            "Iteration: 28264; Percent complete: 94.2%; Average loss: 0.1585\n",
            "Iteration: 28265; Percent complete: 94.2%; Average loss: 0.0871\n",
            "Iteration: 28266; Percent complete: 94.2%; Average loss: 0.1277\n",
            "Iteration: 28267; Percent complete: 94.2%; Average loss: 0.1380\n",
            "Iteration: 28268; Percent complete: 94.2%; Average loss: 0.1498\n",
            "Iteration: 28269; Percent complete: 94.2%; Average loss: 0.1254\n",
            "Iteration: 28270; Percent complete: 94.2%; Average loss: 0.1122\n",
            "Iteration: 28271; Percent complete: 94.2%; Average loss: 0.1017\n",
            "Iteration: 28272; Percent complete: 94.2%; Average loss: 0.1421\n",
            "Iteration: 28273; Percent complete: 94.2%; Average loss: 0.1208\n",
            "Iteration: 28274; Percent complete: 94.2%; Average loss: 0.1214\n",
            "Iteration: 28275; Percent complete: 94.2%; Average loss: 0.0691\n",
            "Iteration: 28276; Percent complete: 94.3%; Average loss: 0.1269\n",
            "Iteration: 28277; Percent complete: 94.3%; Average loss: 0.0604\n",
            "Iteration: 28278; Percent complete: 94.3%; Average loss: 0.1508\n",
            "Iteration: 28279; Percent complete: 94.3%; Average loss: 0.1685\n",
            "Iteration: 28280; Percent complete: 94.3%; Average loss: 0.0913\n",
            "Iteration: 28281; Percent complete: 94.3%; Average loss: 0.1405\n",
            "Iteration: 28282; Percent complete: 94.3%; Average loss: 0.0833\n",
            "Iteration: 28283; Percent complete: 94.3%; Average loss: 0.1919\n",
            "Iteration: 28284; Percent complete: 94.3%; Average loss: 0.1522\n",
            "Iteration: 28285; Percent complete: 94.3%; Average loss: 0.1301\n",
            "Iteration: 28286; Percent complete: 94.3%; Average loss: 0.1476\n",
            "Iteration: 28287; Percent complete: 94.3%; Average loss: 0.0977\n",
            "Iteration: 28288; Percent complete: 94.3%; Average loss: 0.1590\n",
            "Iteration: 28289; Percent complete: 94.3%; Average loss: 0.1211\n",
            "Iteration: 28290; Percent complete: 94.3%; Average loss: 0.1535\n",
            "Iteration: 28291; Percent complete: 94.3%; Average loss: 0.1567\n",
            "Iteration: 28292; Percent complete: 94.3%; Average loss: 0.1472\n",
            "Iteration: 28293; Percent complete: 94.3%; Average loss: 0.1389\n",
            "Iteration: 28294; Percent complete: 94.3%; Average loss: 0.1075\n",
            "Iteration: 28295; Percent complete: 94.3%; Average loss: 0.0975\n",
            "Iteration: 28296; Percent complete: 94.3%; Average loss: 0.1189\n",
            "Iteration: 28297; Percent complete: 94.3%; Average loss: 0.1127\n",
            "Iteration: 28298; Percent complete: 94.3%; Average loss: 0.1003\n",
            "Iteration: 28299; Percent complete: 94.3%; Average loss: 0.1083\n",
            "Iteration: 28300; Percent complete: 94.3%; Average loss: 0.1329\n",
            "Iteration: 28301; Percent complete: 94.3%; Average loss: 0.1518\n",
            "Iteration: 28302; Percent complete: 94.3%; Average loss: 0.1674\n",
            "Iteration: 28303; Percent complete: 94.3%; Average loss: 0.1315\n",
            "Iteration: 28304; Percent complete: 94.3%; Average loss: 0.1481\n",
            "Iteration: 28305; Percent complete: 94.3%; Average loss: 0.0809\n",
            "Iteration: 28306; Percent complete: 94.4%; Average loss: 0.0875\n",
            "Iteration: 28307; Percent complete: 94.4%; Average loss: 0.1279\n",
            "Iteration: 28308; Percent complete: 94.4%; Average loss: 0.1413\n",
            "Iteration: 28309; Percent complete: 94.4%; Average loss: 0.1228\n",
            "Iteration: 28310; Percent complete: 94.4%; Average loss: 0.1211\n",
            "Iteration: 28311; Percent complete: 94.4%; Average loss: 0.0985\n",
            "Iteration: 28312; Percent complete: 94.4%; Average loss: 0.0999\n",
            "Iteration: 28313; Percent complete: 94.4%; Average loss: 0.1672\n",
            "Iteration: 28314; Percent complete: 94.4%; Average loss: 0.1219\n",
            "Iteration: 28315; Percent complete: 94.4%; Average loss: 0.1386\n",
            "Iteration: 28316; Percent complete: 94.4%; Average loss: 0.1068\n",
            "Iteration: 28317; Percent complete: 94.4%; Average loss: 0.1583\n",
            "Iteration: 28318; Percent complete: 94.4%; Average loss: 0.1061\n",
            "Iteration: 28319; Percent complete: 94.4%; Average loss: 0.0958\n",
            "Iteration: 28320; Percent complete: 94.4%; Average loss: 0.1761\n",
            "Iteration: 28321; Percent complete: 94.4%; Average loss: 0.1409\n",
            "Iteration: 28322; Percent complete: 94.4%; Average loss: 0.1010\n",
            "Iteration: 28323; Percent complete: 94.4%; Average loss: 0.1810\n",
            "Iteration: 28324; Percent complete: 94.4%; Average loss: 0.1212\n",
            "Iteration: 28325; Percent complete: 94.4%; Average loss: 0.1233\n",
            "Iteration: 28326; Percent complete: 94.4%; Average loss: 0.1020\n",
            "Iteration: 28327; Percent complete: 94.4%; Average loss: 0.0957\n",
            "Iteration: 28328; Percent complete: 94.4%; Average loss: 0.1033\n",
            "Iteration: 28329; Percent complete: 94.4%; Average loss: 0.1225\n",
            "Iteration: 28330; Percent complete: 94.4%; Average loss: 0.1648\n",
            "Iteration: 28331; Percent complete: 94.4%; Average loss: 0.1294\n",
            "Iteration: 28332; Percent complete: 94.4%; Average loss: 0.1435\n",
            "Iteration: 28333; Percent complete: 94.4%; Average loss: 0.1262\n",
            "Iteration: 28334; Percent complete: 94.4%; Average loss: 0.1461\n",
            "Iteration: 28335; Percent complete: 94.5%; Average loss: 0.1382\n",
            "Iteration: 28336; Percent complete: 94.5%; Average loss: 0.1263\n",
            "Iteration: 28337; Percent complete: 94.5%; Average loss: 0.1136\n",
            "Iteration: 28338; Percent complete: 94.5%; Average loss: 0.1325\n",
            "Iteration: 28339; Percent complete: 94.5%; Average loss: 0.1185\n",
            "Iteration: 28340; Percent complete: 94.5%; Average loss: 0.0964\n",
            "Iteration: 28341; Percent complete: 94.5%; Average loss: 0.1731\n",
            "Iteration: 28342; Percent complete: 94.5%; Average loss: 0.0864\n",
            "Iteration: 28343; Percent complete: 94.5%; Average loss: 0.1370\n",
            "Iteration: 28344; Percent complete: 94.5%; Average loss: 0.1439\n",
            "Iteration: 28345; Percent complete: 94.5%; Average loss: 0.1260\n",
            "Iteration: 28346; Percent complete: 94.5%; Average loss: 0.1078\n",
            "Iteration: 28347; Percent complete: 94.5%; Average loss: 0.1400\n",
            "Iteration: 28348; Percent complete: 94.5%; Average loss: 0.1466\n",
            "Iteration: 28349; Percent complete: 94.5%; Average loss: 0.0935\n",
            "Iteration: 28350; Percent complete: 94.5%; Average loss: 0.0916\n",
            "Iteration: 28351; Percent complete: 94.5%; Average loss: 0.0958\n",
            "Iteration: 28352; Percent complete: 94.5%; Average loss: 0.1460\n",
            "Iteration: 28353; Percent complete: 94.5%; Average loss: 0.1005\n",
            "Iteration: 28354; Percent complete: 94.5%; Average loss: 0.0997\n",
            "Iteration: 28355; Percent complete: 94.5%; Average loss: 0.1153\n",
            "Iteration: 28356; Percent complete: 94.5%; Average loss: 0.1452\n",
            "Iteration: 28357; Percent complete: 94.5%; Average loss: 0.0992\n",
            "Iteration: 28358; Percent complete: 94.5%; Average loss: 0.1228\n",
            "Iteration: 28359; Percent complete: 94.5%; Average loss: 0.1331\n",
            "Iteration: 28360; Percent complete: 94.5%; Average loss: 0.1691\n",
            "Iteration: 28361; Percent complete: 94.5%; Average loss: 0.1004\n",
            "Iteration: 28362; Percent complete: 94.5%; Average loss: 0.1118\n",
            "Iteration: 28363; Percent complete: 94.5%; Average loss: 0.1482\n",
            "Iteration: 28364; Percent complete: 94.5%; Average loss: 0.1068\n",
            "Iteration: 28365; Percent complete: 94.5%; Average loss: 0.0813\n",
            "Iteration: 28366; Percent complete: 94.6%; Average loss: 0.1321\n",
            "Iteration: 28367; Percent complete: 94.6%; Average loss: 0.1271\n",
            "Iteration: 28368; Percent complete: 94.6%; Average loss: 0.1115\n",
            "Iteration: 28369; Percent complete: 94.6%; Average loss: 0.1172\n",
            "Iteration: 28370; Percent complete: 94.6%; Average loss: 0.1134\n",
            "Iteration: 28371; Percent complete: 94.6%; Average loss: 0.1126\n",
            "Iteration: 28372; Percent complete: 94.6%; Average loss: 0.0889\n",
            "Iteration: 28373; Percent complete: 94.6%; Average loss: 0.1292\n",
            "Iteration: 28374; Percent complete: 94.6%; Average loss: 0.1381\n",
            "Iteration: 28375; Percent complete: 94.6%; Average loss: 0.1144\n",
            "Iteration: 28376; Percent complete: 94.6%; Average loss: 0.1557\n",
            "Iteration: 28377; Percent complete: 94.6%; Average loss: 0.0932\n",
            "Iteration: 28378; Percent complete: 94.6%; Average loss: 0.1352\n",
            "Iteration: 28379; Percent complete: 94.6%; Average loss: 0.0920\n",
            "Iteration: 28380; Percent complete: 94.6%; Average loss: 0.0911\n",
            "Iteration: 28381; Percent complete: 94.6%; Average loss: 0.0929\n",
            "Iteration: 28382; Percent complete: 94.6%; Average loss: 0.0769\n",
            "Iteration: 28383; Percent complete: 94.6%; Average loss: 0.1455\n",
            "Iteration: 28384; Percent complete: 94.6%; Average loss: 0.1217\n",
            "Iteration: 28385; Percent complete: 94.6%; Average loss: 0.1121\n",
            "Iteration: 28386; Percent complete: 94.6%; Average loss: 0.1092\n",
            "Iteration: 28387; Percent complete: 94.6%; Average loss: 0.1283\n",
            "Iteration: 28388; Percent complete: 94.6%; Average loss: 0.1451\n",
            "Iteration: 28389; Percent complete: 94.6%; Average loss: 0.1591\n",
            "Iteration: 28390; Percent complete: 94.6%; Average loss: 0.1329\n",
            "Iteration: 28391; Percent complete: 94.6%; Average loss: 0.1420\n",
            "Iteration: 28392; Percent complete: 94.6%; Average loss: 0.1817\n",
            "Iteration: 28393; Percent complete: 94.6%; Average loss: 0.1338\n",
            "Iteration: 28394; Percent complete: 94.6%; Average loss: 0.1352\n",
            "Iteration: 28395; Percent complete: 94.7%; Average loss: 0.1132\n",
            "Iteration: 28396; Percent complete: 94.7%; Average loss: 0.1309\n",
            "Iteration: 28397; Percent complete: 94.7%; Average loss: 0.1068\n",
            "Iteration: 28398; Percent complete: 94.7%; Average loss: 0.1113\n",
            "Iteration: 28399; Percent complete: 94.7%; Average loss: 0.1809\n",
            "Iteration: 28400; Percent complete: 94.7%; Average loss: 0.1110\n",
            "Iteration: 28401; Percent complete: 94.7%; Average loss: 0.0941\n",
            "Iteration: 28402; Percent complete: 94.7%; Average loss: 0.1451\n",
            "Iteration: 28403; Percent complete: 94.7%; Average loss: 0.1423\n",
            "Iteration: 28404; Percent complete: 94.7%; Average loss: 0.1079\n",
            "Iteration: 28405; Percent complete: 94.7%; Average loss: 0.1749\n",
            "Iteration: 28406; Percent complete: 94.7%; Average loss: 0.1587\n",
            "Iteration: 28407; Percent complete: 94.7%; Average loss: 0.0858\n",
            "Iteration: 28408; Percent complete: 94.7%; Average loss: 0.1125\n",
            "Iteration: 28409; Percent complete: 94.7%; Average loss: 0.0946\n",
            "Iteration: 28410; Percent complete: 94.7%; Average loss: 0.1409\n",
            "Iteration: 28411; Percent complete: 94.7%; Average loss: 0.1103\n",
            "Iteration: 28412; Percent complete: 94.7%; Average loss: 0.1293\n",
            "Iteration: 28413; Percent complete: 94.7%; Average loss: 0.1208\n",
            "Iteration: 28414; Percent complete: 94.7%; Average loss: 0.1351\n",
            "Iteration: 28415; Percent complete: 94.7%; Average loss: 0.0966\n",
            "Iteration: 28416; Percent complete: 94.7%; Average loss: 0.1466\n",
            "Iteration: 28417; Percent complete: 94.7%; Average loss: 0.1155\n",
            "Iteration: 28418; Percent complete: 94.7%; Average loss: 0.1375\n",
            "Iteration: 28419; Percent complete: 94.7%; Average loss: 0.1068\n",
            "Iteration: 28420; Percent complete: 94.7%; Average loss: 0.1069\n",
            "Iteration: 28421; Percent complete: 94.7%; Average loss: 0.0963\n",
            "Iteration: 28422; Percent complete: 94.7%; Average loss: 0.1180\n",
            "Iteration: 28423; Percent complete: 94.7%; Average loss: 0.0644\n",
            "Iteration: 28424; Percent complete: 94.7%; Average loss: 0.1147\n",
            "Iteration: 28425; Percent complete: 94.8%; Average loss: 0.1282\n",
            "Iteration: 28426; Percent complete: 94.8%; Average loss: 0.1109\n",
            "Iteration: 28427; Percent complete: 94.8%; Average loss: 0.1036\n",
            "Iteration: 28428; Percent complete: 94.8%; Average loss: 0.1473\n",
            "Iteration: 28429; Percent complete: 94.8%; Average loss: 0.1097\n",
            "Iteration: 28430; Percent complete: 94.8%; Average loss: 0.1374\n",
            "Iteration: 28431; Percent complete: 94.8%; Average loss: 0.1248\n",
            "Iteration: 28432; Percent complete: 94.8%; Average loss: 0.1039\n",
            "Iteration: 28433; Percent complete: 94.8%; Average loss: 0.0971\n",
            "Iteration: 28434; Percent complete: 94.8%; Average loss: 0.1236\n",
            "Iteration: 28435; Percent complete: 94.8%; Average loss: 0.1170\n",
            "Iteration: 28436; Percent complete: 94.8%; Average loss: 0.1186\n",
            "Iteration: 28437; Percent complete: 94.8%; Average loss: 0.1358\n",
            "Iteration: 28438; Percent complete: 94.8%; Average loss: 0.1170\n",
            "Iteration: 28439; Percent complete: 94.8%; Average loss: 0.1391\n",
            "Iteration: 28440; Percent complete: 94.8%; Average loss: 0.1588\n",
            "Iteration: 28441; Percent complete: 94.8%; Average loss: 0.1124\n",
            "Iteration: 28442; Percent complete: 94.8%; Average loss: 0.0848\n",
            "Iteration: 28443; Percent complete: 94.8%; Average loss: 0.1213\n",
            "Iteration: 28444; Percent complete: 94.8%; Average loss: 0.1465\n",
            "Iteration: 28445; Percent complete: 94.8%; Average loss: 0.1145\n",
            "Iteration: 28446; Percent complete: 94.8%; Average loss: 0.1230\n",
            "Iteration: 28447; Percent complete: 94.8%; Average loss: 0.1083\n",
            "Iteration: 28448; Percent complete: 94.8%; Average loss: 0.2026\n",
            "Iteration: 28449; Percent complete: 94.8%; Average loss: 0.1315\n",
            "Iteration: 28450; Percent complete: 94.8%; Average loss: 0.1517\n",
            "Iteration: 28451; Percent complete: 94.8%; Average loss: 0.0666\n",
            "Iteration: 28452; Percent complete: 94.8%; Average loss: 0.1307\n",
            "Iteration: 28453; Percent complete: 94.8%; Average loss: 0.1276\n",
            "Iteration: 28454; Percent complete: 94.8%; Average loss: 0.1162\n",
            "Iteration: 28455; Percent complete: 94.8%; Average loss: 0.1155\n",
            "Iteration: 28456; Percent complete: 94.9%; Average loss: 0.0969\n",
            "Iteration: 28457; Percent complete: 94.9%; Average loss: 0.1487\n",
            "Iteration: 28458; Percent complete: 94.9%; Average loss: 0.1046\n",
            "Iteration: 28459; Percent complete: 94.9%; Average loss: 0.1130\n",
            "Iteration: 28460; Percent complete: 94.9%; Average loss: 0.1008\n",
            "Iteration: 28461; Percent complete: 94.9%; Average loss: 0.1352\n",
            "Iteration: 28462; Percent complete: 94.9%; Average loss: 0.1569\n",
            "Iteration: 28463; Percent complete: 94.9%; Average loss: 0.1286\n",
            "Iteration: 28464; Percent complete: 94.9%; Average loss: 0.0725\n",
            "Iteration: 28465; Percent complete: 94.9%; Average loss: 0.0992\n",
            "Iteration: 28466; Percent complete: 94.9%; Average loss: 0.1617\n",
            "Iteration: 28467; Percent complete: 94.9%; Average loss: 0.1317\n",
            "Iteration: 28468; Percent complete: 94.9%; Average loss: 0.1632\n",
            "Iteration: 28469; Percent complete: 94.9%; Average loss: 0.1431\n",
            "Iteration: 28470; Percent complete: 94.9%; Average loss: 0.1575\n",
            "Iteration: 28471; Percent complete: 94.9%; Average loss: 0.2156\n",
            "Iteration: 28472; Percent complete: 94.9%; Average loss: 0.1328\n",
            "Iteration: 28473; Percent complete: 94.9%; Average loss: 0.2487\n",
            "Iteration: 28474; Percent complete: 94.9%; Average loss: 0.1349\n",
            "Iteration: 28475; Percent complete: 94.9%; Average loss: 0.1106\n",
            "Iteration: 28476; Percent complete: 94.9%; Average loss: 0.1465\n",
            "Iteration: 28477; Percent complete: 94.9%; Average loss: 0.1476\n",
            "Iteration: 28478; Percent complete: 94.9%; Average loss: 0.1023\n",
            "Iteration: 28479; Percent complete: 94.9%; Average loss: 0.1467\n",
            "Iteration: 28480; Percent complete: 94.9%; Average loss: 0.1392\n",
            "Iteration: 28481; Percent complete: 94.9%; Average loss: 0.1478\n",
            "Iteration: 28482; Percent complete: 94.9%; Average loss: 0.1629\n",
            "Iteration: 28483; Percent complete: 94.9%; Average loss: 0.1192\n",
            "Iteration: 28484; Percent complete: 94.9%; Average loss: 0.1428\n",
            "Iteration: 28485; Percent complete: 95.0%; Average loss: 0.1138\n",
            "Iteration: 28486; Percent complete: 95.0%; Average loss: 0.2044\n",
            "Iteration: 28487; Percent complete: 95.0%; Average loss: 0.1167\n",
            "Iteration: 28488; Percent complete: 95.0%; Average loss: 0.0677\n",
            "Iteration: 28489; Percent complete: 95.0%; Average loss: 0.1215\n",
            "Iteration: 28490; Percent complete: 95.0%; Average loss: 0.1309\n",
            "Iteration: 28491; Percent complete: 95.0%; Average loss: 0.1097\n",
            "Iteration: 28492; Percent complete: 95.0%; Average loss: 0.1108\n",
            "Iteration: 28493; Percent complete: 95.0%; Average loss: 0.1028\n",
            "Iteration: 28494; Percent complete: 95.0%; Average loss: 0.1159\n",
            "Iteration: 28495; Percent complete: 95.0%; Average loss: 0.1331\n",
            "Iteration: 28496; Percent complete: 95.0%; Average loss: 0.1325\n",
            "Iteration: 28497; Percent complete: 95.0%; Average loss: 0.1565\n",
            "Iteration: 28498; Percent complete: 95.0%; Average loss: 0.1287\n",
            "Iteration: 28499; Percent complete: 95.0%; Average loss: 0.1211\n",
            "Iteration: 28500; Percent complete: 95.0%; Average loss: 0.1345\n",
            "Iteration: 28501; Percent complete: 95.0%; Average loss: 0.0748\n",
            "Iteration: 28502; Percent complete: 95.0%; Average loss: 0.1246\n",
            "Iteration: 28503; Percent complete: 95.0%; Average loss: 0.1319\n",
            "Iteration: 28504; Percent complete: 95.0%; Average loss: 0.1274\n",
            "Iteration: 28505; Percent complete: 95.0%; Average loss: 0.1197\n",
            "Iteration: 28506; Percent complete: 95.0%; Average loss: 0.1319\n",
            "Iteration: 28507; Percent complete: 95.0%; Average loss: 0.1292\n",
            "Iteration: 28508; Percent complete: 95.0%; Average loss: 0.1431\n",
            "Iteration: 28509; Percent complete: 95.0%; Average loss: 0.1707\n",
            "Iteration: 28510; Percent complete: 95.0%; Average loss: 0.0981\n",
            "Iteration: 28511; Percent complete: 95.0%; Average loss: 0.0983\n",
            "Iteration: 28512; Percent complete: 95.0%; Average loss: 0.1386\n",
            "Iteration: 28513; Percent complete: 95.0%; Average loss: 0.0909\n",
            "Iteration: 28514; Percent complete: 95.0%; Average loss: 0.1074\n",
            "Iteration: 28515; Percent complete: 95.0%; Average loss: 0.1422\n",
            "Iteration: 28516; Percent complete: 95.1%; Average loss: 0.1227\n",
            "Iteration: 28517; Percent complete: 95.1%; Average loss: 0.1047\n",
            "Iteration: 28518; Percent complete: 95.1%; Average loss: 0.1525\n",
            "Iteration: 28519; Percent complete: 95.1%; Average loss: 0.1353\n",
            "Iteration: 28520; Percent complete: 95.1%; Average loss: 0.1550\n",
            "Iteration: 28521; Percent complete: 95.1%; Average loss: 0.1308\n",
            "Iteration: 28522; Percent complete: 95.1%; Average loss: 0.0980\n",
            "Iteration: 28523; Percent complete: 95.1%; Average loss: 0.1236\n",
            "Iteration: 28524; Percent complete: 95.1%; Average loss: 0.1329\n",
            "Iteration: 28525; Percent complete: 95.1%; Average loss: 0.1445\n",
            "Iteration: 28526; Percent complete: 95.1%; Average loss: 0.1426\n",
            "Iteration: 28527; Percent complete: 95.1%; Average loss: 0.0934\n",
            "Iteration: 28528; Percent complete: 95.1%; Average loss: 0.0844\n",
            "Iteration: 28529; Percent complete: 95.1%; Average loss: 0.0930\n",
            "Iteration: 28530; Percent complete: 95.1%; Average loss: 0.0951\n",
            "Iteration: 28531; Percent complete: 95.1%; Average loss: 0.1045\n",
            "Iteration: 28532; Percent complete: 95.1%; Average loss: 0.1395\n",
            "Iteration: 28533; Percent complete: 95.1%; Average loss: 0.1128\n",
            "Iteration: 28534; Percent complete: 95.1%; Average loss: 0.1666\n",
            "Iteration: 28535; Percent complete: 95.1%; Average loss: 0.1207\n",
            "Iteration: 28536; Percent complete: 95.1%; Average loss: 0.1183\n",
            "Iteration: 28537; Percent complete: 95.1%; Average loss: 0.2102\n",
            "Iteration: 28538; Percent complete: 95.1%; Average loss: 0.1145\n",
            "Iteration: 28539; Percent complete: 95.1%; Average loss: 0.1565\n",
            "Iteration: 28540; Percent complete: 95.1%; Average loss: 0.0997\n",
            "Iteration: 28541; Percent complete: 95.1%; Average loss: 0.1267\n",
            "Iteration: 28542; Percent complete: 95.1%; Average loss: 0.1350\n",
            "Iteration: 28543; Percent complete: 95.1%; Average loss: 0.1482\n",
            "Iteration: 28544; Percent complete: 95.1%; Average loss: 0.1140\n",
            "Iteration: 28545; Percent complete: 95.2%; Average loss: 0.0718\n",
            "Iteration: 28546; Percent complete: 95.2%; Average loss: 0.1007\n",
            "Iteration: 28547; Percent complete: 95.2%; Average loss: 0.0815\n",
            "Iteration: 28548; Percent complete: 95.2%; Average loss: 0.1225\n",
            "Iteration: 28549; Percent complete: 95.2%; Average loss: 0.0934\n",
            "Iteration: 28550; Percent complete: 95.2%; Average loss: 0.1577\n",
            "Iteration: 28551; Percent complete: 95.2%; Average loss: 0.1214\n",
            "Iteration: 28552; Percent complete: 95.2%; Average loss: 0.1047\n",
            "Iteration: 28553; Percent complete: 95.2%; Average loss: 0.1578\n",
            "Iteration: 28554; Percent complete: 95.2%; Average loss: 0.1153\n",
            "Iteration: 28555; Percent complete: 95.2%; Average loss: 0.1380\n",
            "Iteration: 28556; Percent complete: 95.2%; Average loss: 0.1802\n",
            "Iteration: 28557; Percent complete: 95.2%; Average loss: 0.1052\n",
            "Iteration: 28558; Percent complete: 95.2%; Average loss: 0.1760\n",
            "Iteration: 28559; Percent complete: 95.2%; Average loss: 0.1629\n",
            "Iteration: 28560; Percent complete: 95.2%; Average loss: 0.1434\n",
            "Iteration: 28561; Percent complete: 95.2%; Average loss: 0.1399\n",
            "Iteration: 28562; Percent complete: 95.2%; Average loss: 0.1225\n",
            "Iteration: 28563; Percent complete: 95.2%; Average loss: 0.1101\n",
            "Iteration: 28564; Percent complete: 95.2%; Average loss: 0.0792\n",
            "Iteration: 28565; Percent complete: 95.2%; Average loss: 0.1350\n",
            "Iteration: 28566; Percent complete: 95.2%; Average loss: 0.0959\n",
            "Iteration: 28567; Percent complete: 95.2%; Average loss: 0.1539\n",
            "Iteration: 28568; Percent complete: 95.2%; Average loss: 0.1069\n",
            "Iteration: 28569; Percent complete: 95.2%; Average loss: 0.1367\n",
            "Iteration: 28570; Percent complete: 95.2%; Average loss: 0.1501\n",
            "Iteration: 28571; Percent complete: 95.2%; Average loss: 0.1373\n",
            "Iteration: 28572; Percent complete: 95.2%; Average loss: 0.1190\n",
            "Iteration: 28573; Percent complete: 95.2%; Average loss: 0.1068\n",
            "Iteration: 28574; Percent complete: 95.2%; Average loss: 0.1202\n",
            "Iteration: 28575; Percent complete: 95.2%; Average loss: 0.1441\n",
            "Iteration: 28576; Percent complete: 95.3%; Average loss: 0.1355\n",
            "Iteration: 28577; Percent complete: 95.3%; Average loss: 0.1117\n",
            "Iteration: 28578; Percent complete: 95.3%; Average loss: 0.1185\n",
            "Iteration: 28579; Percent complete: 95.3%; Average loss: 0.1454\n",
            "Iteration: 28580; Percent complete: 95.3%; Average loss: 0.1274\n",
            "Iteration: 28581; Percent complete: 95.3%; Average loss: 0.1550\n",
            "Iteration: 28582; Percent complete: 95.3%; Average loss: 0.1205\n",
            "Iteration: 28583; Percent complete: 95.3%; Average loss: 0.1824\n",
            "Iteration: 28584; Percent complete: 95.3%; Average loss: 0.1485\n",
            "Iteration: 28585; Percent complete: 95.3%; Average loss: 0.1152\n",
            "Iteration: 28586; Percent complete: 95.3%; Average loss: 0.1242\n",
            "Iteration: 28587; Percent complete: 95.3%; Average loss: 0.1123\n",
            "Iteration: 28588; Percent complete: 95.3%; Average loss: 0.1248\n",
            "Iteration: 28589; Percent complete: 95.3%; Average loss: 0.1195\n",
            "Iteration: 28590; Percent complete: 95.3%; Average loss: 0.1484\n",
            "Iteration: 28591; Percent complete: 95.3%; Average loss: 0.1073\n",
            "Iteration: 28592; Percent complete: 95.3%; Average loss: 0.1383\n",
            "Iteration: 28593; Percent complete: 95.3%; Average loss: 0.1470\n",
            "Iteration: 28594; Percent complete: 95.3%; Average loss: 0.1391\n",
            "Iteration: 28595; Percent complete: 95.3%; Average loss: 0.0973\n",
            "Iteration: 28596; Percent complete: 95.3%; Average loss: 0.1299\n",
            "Iteration: 28597; Percent complete: 95.3%; Average loss: 0.1498\n",
            "Iteration: 28598; Percent complete: 95.3%; Average loss: 0.1017\n",
            "Iteration: 28599; Percent complete: 95.3%; Average loss: 0.1037\n",
            "Iteration: 28600; Percent complete: 95.3%; Average loss: 0.1497\n",
            "Iteration: 28601; Percent complete: 95.3%; Average loss: 0.1099\n",
            "Iteration: 28602; Percent complete: 95.3%; Average loss: 0.1139\n",
            "Iteration: 28603; Percent complete: 95.3%; Average loss: 0.1168\n",
            "Iteration: 28604; Percent complete: 95.3%; Average loss: 0.1147\n",
            "Iteration: 28605; Percent complete: 95.3%; Average loss: 0.1076\n",
            "Iteration: 28606; Percent complete: 95.4%; Average loss: 0.0830\n",
            "Iteration: 28607; Percent complete: 95.4%; Average loss: 0.0935\n",
            "Iteration: 28608; Percent complete: 95.4%; Average loss: 0.1246\n",
            "Iteration: 28609; Percent complete: 95.4%; Average loss: 0.0770\n",
            "Iteration: 28610; Percent complete: 95.4%; Average loss: 0.1018\n",
            "Iteration: 28611; Percent complete: 95.4%; Average loss: 0.1465\n",
            "Iteration: 28612; Percent complete: 95.4%; Average loss: 0.1075\n",
            "Iteration: 28613; Percent complete: 95.4%; Average loss: 0.0999\n",
            "Iteration: 28614; Percent complete: 95.4%; Average loss: 0.1225\n",
            "Iteration: 28615; Percent complete: 95.4%; Average loss: 0.1150\n",
            "Iteration: 28616; Percent complete: 95.4%; Average loss: 0.1778\n",
            "Iteration: 28617; Percent complete: 95.4%; Average loss: 0.1373\n",
            "Iteration: 28618; Percent complete: 95.4%; Average loss: 0.1123\n",
            "Iteration: 28619; Percent complete: 95.4%; Average loss: 0.1269\n",
            "Iteration: 28620; Percent complete: 95.4%; Average loss: 0.1096\n",
            "Iteration: 28621; Percent complete: 95.4%; Average loss: 0.1356\n",
            "Iteration: 28622; Percent complete: 95.4%; Average loss: 0.1174\n",
            "Iteration: 28623; Percent complete: 95.4%; Average loss: 0.1441\n",
            "Iteration: 28624; Percent complete: 95.4%; Average loss: 0.1422\n",
            "Iteration: 28625; Percent complete: 95.4%; Average loss: 0.1388\n",
            "Iteration: 28626; Percent complete: 95.4%; Average loss: 0.1250\n",
            "Iteration: 28627; Percent complete: 95.4%; Average loss: 0.1147\n",
            "Iteration: 28628; Percent complete: 95.4%; Average loss: 0.1262\n",
            "Iteration: 28629; Percent complete: 95.4%; Average loss: 0.1032\n",
            "Iteration: 28630; Percent complete: 95.4%; Average loss: 0.1083\n",
            "Iteration: 28631; Percent complete: 95.4%; Average loss: 0.1380\n",
            "Iteration: 28632; Percent complete: 95.4%; Average loss: 0.1810\n",
            "Iteration: 28633; Percent complete: 95.4%; Average loss: 0.1020\n",
            "Iteration: 28634; Percent complete: 95.4%; Average loss: 0.1169\n",
            "Iteration: 28635; Percent complete: 95.5%; Average loss: 0.1546\n",
            "Iteration: 28636; Percent complete: 95.5%; Average loss: 0.0878\n",
            "Iteration: 28637; Percent complete: 95.5%; Average loss: 0.0994\n",
            "Iteration: 28638; Percent complete: 95.5%; Average loss: 0.0747\n",
            "Iteration: 28639; Percent complete: 95.5%; Average loss: 0.1056\n",
            "Iteration: 28640; Percent complete: 95.5%; Average loss: 0.1101\n",
            "Iteration: 28641; Percent complete: 95.5%; Average loss: 0.1297\n",
            "Iteration: 28642; Percent complete: 95.5%; Average loss: 0.1182\n",
            "Iteration: 28643; Percent complete: 95.5%; Average loss: 0.1188\n",
            "Iteration: 28644; Percent complete: 95.5%; Average loss: 0.1469\n",
            "Iteration: 28645; Percent complete: 95.5%; Average loss: 0.0843\n",
            "Iteration: 28646; Percent complete: 95.5%; Average loss: 0.1150\n",
            "Iteration: 28647; Percent complete: 95.5%; Average loss: 0.0857\n",
            "Iteration: 28648; Percent complete: 95.5%; Average loss: 0.1092\n",
            "Iteration: 28649; Percent complete: 95.5%; Average loss: 0.1518\n",
            "Iteration: 28650; Percent complete: 95.5%; Average loss: 0.1320\n",
            "Iteration: 28651; Percent complete: 95.5%; Average loss: 0.1489\n",
            "Iteration: 28652; Percent complete: 95.5%; Average loss: 0.1706\n",
            "Iteration: 28653; Percent complete: 95.5%; Average loss: 0.1211\n",
            "Iteration: 28654; Percent complete: 95.5%; Average loss: 0.2183\n",
            "Iteration: 28655; Percent complete: 95.5%; Average loss: 0.1346\n",
            "Iteration: 28656; Percent complete: 95.5%; Average loss: 0.1112\n",
            "Iteration: 28657; Percent complete: 95.5%; Average loss: 0.1229\n",
            "Iteration: 28658; Percent complete: 95.5%; Average loss: 0.1514\n",
            "Iteration: 28659; Percent complete: 95.5%; Average loss: 0.0994\n",
            "Iteration: 28660; Percent complete: 95.5%; Average loss: 0.1623\n",
            "Iteration: 28661; Percent complete: 95.5%; Average loss: 0.1273\n",
            "Iteration: 28662; Percent complete: 95.5%; Average loss: 0.1219\n",
            "Iteration: 28663; Percent complete: 95.5%; Average loss: 0.1385\n",
            "Iteration: 28664; Percent complete: 95.5%; Average loss: 0.1223\n",
            "Iteration: 28665; Percent complete: 95.5%; Average loss: 0.1042\n",
            "Iteration: 28666; Percent complete: 95.6%; Average loss: 0.1421\n",
            "Iteration: 28667; Percent complete: 95.6%; Average loss: 0.1425\n",
            "Iteration: 28668; Percent complete: 95.6%; Average loss: 0.1594\n",
            "Iteration: 28669; Percent complete: 95.6%; Average loss: 0.1182\n",
            "Iteration: 28670; Percent complete: 95.6%; Average loss: 0.0764\n",
            "Iteration: 28671; Percent complete: 95.6%; Average loss: 0.1370\n",
            "Iteration: 28672; Percent complete: 95.6%; Average loss: 0.0901\n",
            "Iteration: 28673; Percent complete: 95.6%; Average loss: 0.1177\n",
            "Iteration: 28674; Percent complete: 95.6%; Average loss: 0.0700\n",
            "Iteration: 28675; Percent complete: 95.6%; Average loss: 0.0999\n",
            "Iteration: 28676; Percent complete: 95.6%; Average loss: 0.1112\n",
            "Iteration: 28677; Percent complete: 95.6%; Average loss: 0.1084\n",
            "Iteration: 28678; Percent complete: 95.6%; Average loss: 0.0991\n",
            "Iteration: 28679; Percent complete: 95.6%; Average loss: 0.0825\n",
            "Iteration: 28680; Percent complete: 95.6%; Average loss: 0.1033\n",
            "Iteration: 28681; Percent complete: 95.6%; Average loss: 0.0906\n",
            "Iteration: 28682; Percent complete: 95.6%; Average loss: 0.1032\n",
            "Iteration: 28683; Percent complete: 95.6%; Average loss: 0.1223\n",
            "Iteration: 28684; Percent complete: 95.6%; Average loss: 0.1363\n",
            "Iteration: 28685; Percent complete: 95.6%; Average loss: 0.1263\n",
            "Iteration: 28686; Percent complete: 95.6%; Average loss: 0.1081\n",
            "Iteration: 28687; Percent complete: 95.6%; Average loss: 0.0973\n",
            "Iteration: 28688; Percent complete: 95.6%; Average loss: 0.1488\n",
            "Iteration: 28689; Percent complete: 95.6%; Average loss: 0.1526\n",
            "Iteration: 28690; Percent complete: 95.6%; Average loss: 0.1655\n",
            "Iteration: 28691; Percent complete: 95.6%; Average loss: 0.1014\n",
            "Iteration: 28692; Percent complete: 95.6%; Average loss: 0.0978\n",
            "Iteration: 28693; Percent complete: 95.6%; Average loss: 0.1168\n",
            "Iteration: 28694; Percent complete: 95.6%; Average loss: 0.1359\n",
            "Iteration: 28695; Percent complete: 95.7%; Average loss: 0.1308\n",
            "Iteration: 28696; Percent complete: 95.7%; Average loss: 0.1033\n",
            "Iteration: 28697; Percent complete: 95.7%; Average loss: 0.1242\n",
            "Iteration: 28698; Percent complete: 95.7%; Average loss: 0.1697\n",
            "Iteration: 28699; Percent complete: 95.7%; Average loss: 0.1112\n",
            "Iteration: 28700; Percent complete: 95.7%; Average loss: 0.1038\n",
            "Iteration: 28701; Percent complete: 95.7%; Average loss: 0.0759\n",
            "Iteration: 28702; Percent complete: 95.7%; Average loss: 0.0758\n",
            "Iteration: 28703; Percent complete: 95.7%; Average loss: 0.1146\n",
            "Iteration: 28704; Percent complete: 95.7%; Average loss: 0.1445\n",
            "Iteration: 28705; Percent complete: 95.7%; Average loss: 0.1469\n",
            "Iteration: 28706; Percent complete: 95.7%; Average loss: 0.0940\n",
            "Iteration: 28707; Percent complete: 95.7%; Average loss: 0.1357\n",
            "Iteration: 28708; Percent complete: 95.7%; Average loss: 0.0779\n",
            "Iteration: 28709; Percent complete: 95.7%; Average loss: 0.1802\n",
            "Iteration: 28710; Percent complete: 95.7%; Average loss: 0.1035\n",
            "Iteration: 28711; Percent complete: 95.7%; Average loss: 0.1375\n",
            "Iteration: 28712; Percent complete: 95.7%; Average loss: 0.1448\n",
            "Iteration: 28713; Percent complete: 95.7%; Average loss: 0.1048\n",
            "Iteration: 28714; Percent complete: 95.7%; Average loss: 0.1399\n",
            "Iteration: 28715; Percent complete: 95.7%; Average loss: 0.1585\n",
            "Iteration: 28716; Percent complete: 95.7%; Average loss: 0.1239\n",
            "Iteration: 28717; Percent complete: 95.7%; Average loss: 0.1059\n",
            "Iteration: 28718; Percent complete: 95.7%; Average loss: 0.0935\n",
            "Iteration: 28719; Percent complete: 95.7%; Average loss: 0.0920\n",
            "Iteration: 28720; Percent complete: 95.7%; Average loss: 0.1554\n",
            "Iteration: 28721; Percent complete: 95.7%; Average loss: 0.0935\n",
            "Iteration: 28722; Percent complete: 95.7%; Average loss: 0.0914\n",
            "Iteration: 28723; Percent complete: 95.7%; Average loss: 0.1421\n",
            "Iteration: 28724; Percent complete: 95.7%; Average loss: 0.1294\n",
            "Iteration: 28725; Percent complete: 95.8%; Average loss: 0.1022\n",
            "Iteration: 28726; Percent complete: 95.8%; Average loss: 0.1115\n",
            "Iteration: 28727; Percent complete: 95.8%; Average loss: 0.1383\n",
            "Iteration: 28728; Percent complete: 95.8%; Average loss: 0.1283\n",
            "Iteration: 28729; Percent complete: 95.8%; Average loss: 0.0906\n",
            "Iteration: 28730; Percent complete: 95.8%; Average loss: 0.1488\n",
            "Iteration: 28731; Percent complete: 95.8%; Average loss: 0.1287\n",
            "Iteration: 28732; Percent complete: 95.8%; Average loss: 0.1214\n",
            "Iteration: 28733; Percent complete: 95.8%; Average loss: 0.0793\n",
            "Iteration: 28734; Percent complete: 95.8%; Average loss: 0.0953\n",
            "Iteration: 28735; Percent complete: 95.8%; Average loss: 0.1687\n",
            "Iteration: 28736; Percent complete: 95.8%; Average loss: 0.1195\n",
            "Iteration: 28737; Percent complete: 95.8%; Average loss: 0.1476\n",
            "Iteration: 28738; Percent complete: 95.8%; Average loss: 0.1602\n",
            "Iteration: 28739; Percent complete: 95.8%; Average loss: 0.1053\n",
            "Iteration: 28740; Percent complete: 95.8%; Average loss: 0.1490\n",
            "Iteration: 28741; Percent complete: 95.8%; Average loss: 0.0954\n",
            "Iteration: 28742; Percent complete: 95.8%; Average loss: 0.1626\n",
            "Iteration: 28743; Percent complete: 95.8%; Average loss: 0.1145\n",
            "Iteration: 28744; Percent complete: 95.8%; Average loss: 0.1146\n",
            "Iteration: 28745; Percent complete: 95.8%; Average loss: 0.0812\n",
            "Iteration: 28746; Percent complete: 95.8%; Average loss: 0.0957\n",
            "Iteration: 28747; Percent complete: 95.8%; Average loss: 0.1290\n",
            "Iteration: 28748; Percent complete: 95.8%; Average loss: 0.1282\n",
            "Iteration: 28749; Percent complete: 95.8%; Average loss: 0.1079\n",
            "Iteration: 28750; Percent complete: 95.8%; Average loss: 0.1000\n",
            "Iteration: 28751; Percent complete: 95.8%; Average loss: 0.1410\n",
            "Iteration: 28752; Percent complete: 95.8%; Average loss: 0.1137\n",
            "Iteration: 28753; Percent complete: 95.8%; Average loss: 0.1725\n",
            "Iteration: 28754; Percent complete: 95.8%; Average loss: 0.1191\n",
            "Iteration: 28755; Percent complete: 95.9%; Average loss: 0.0839\n",
            "Iteration: 28756; Percent complete: 95.9%; Average loss: 0.1449\n",
            "Iteration: 28757; Percent complete: 95.9%; Average loss: 0.0946\n",
            "Iteration: 28758; Percent complete: 95.9%; Average loss: 0.1137\n",
            "Iteration: 28759; Percent complete: 95.9%; Average loss: 0.0929\n",
            "Iteration: 28760; Percent complete: 95.9%; Average loss: 0.1567\n",
            "Iteration: 28761; Percent complete: 95.9%; Average loss: 0.1405\n",
            "Iteration: 28762; Percent complete: 95.9%; Average loss: 0.1279\n",
            "Iteration: 28763; Percent complete: 95.9%; Average loss: 0.1869\n",
            "Iteration: 28764; Percent complete: 95.9%; Average loss: 0.1012\n",
            "Iteration: 28765; Percent complete: 95.9%; Average loss: 0.1070\n",
            "Iteration: 28766; Percent complete: 95.9%; Average loss: 0.0922\n",
            "Iteration: 28767; Percent complete: 95.9%; Average loss: 0.1019\n",
            "Iteration: 28768; Percent complete: 95.9%; Average loss: 0.0924\n",
            "Iteration: 28769; Percent complete: 95.9%; Average loss: 0.1063\n",
            "Iteration: 28770; Percent complete: 95.9%; Average loss: 0.1053\n",
            "Iteration: 28771; Percent complete: 95.9%; Average loss: 0.1461\n",
            "Iteration: 28772; Percent complete: 95.9%; Average loss: 0.1514\n",
            "Iteration: 28773; Percent complete: 95.9%; Average loss: 0.0897\n",
            "Iteration: 28774; Percent complete: 95.9%; Average loss: 0.1089\n",
            "Iteration: 28775; Percent complete: 95.9%; Average loss: 0.1194\n",
            "Iteration: 28776; Percent complete: 95.9%; Average loss: 0.1233\n",
            "Iteration: 28777; Percent complete: 95.9%; Average loss: 0.0973\n",
            "Iteration: 28778; Percent complete: 95.9%; Average loss: 0.0964\n",
            "Iteration: 28779; Percent complete: 95.9%; Average loss: 0.1361\n",
            "Iteration: 28780; Percent complete: 95.9%; Average loss: 0.0995\n",
            "Iteration: 28781; Percent complete: 95.9%; Average loss: 0.1365\n",
            "Iteration: 28782; Percent complete: 95.9%; Average loss: 0.0789\n",
            "Iteration: 28783; Percent complete: 95.9%; Average loss: 0.1136\n",
            "Iteration: 28784; Percent complete: 95.9%; Average loss: 0.1536\n",
            "Iteration: 28785; Percent complete: 96.0%; Average loss: 0.1355\n",
            "Iteration: 28786; Percent complete: 96.0%; Average loss: 0.1349\n",
            "Iteration: 28787; Percent complete: 96.0%; Average loss: 0.1300\n",
            "Iteration: 28788; Percent complete: 96.0%; Average loss: 0.1256\n",
            "Iteration: 28789; Percent complete: 96.0%; Average loss: 0.1284\n",
            "Iteration: 28790; Percent complete: 96.0%; Average loss: 0.1123\n",
            "Iteration: 28791; Percent complete: 96.0%; Average loss: 0.1123\n",
            "Iteration: 28792; Percent complete: 96.0%; Average loss: 0.1093\n",
            "Iteration: 28793; Percent complete: 96.0%; Average loss: 0.1360\n",
            "Iteration: 28794; Percent complete: 96.0%; Average loss: 0.1157\n",
            "Iteration: 28795; Percent complete: 96.0%; Average loss: 0.1255\n",
            "Iteration: 28796; Percent complete: 96.0%; Average loss: 0.1208\n",
            "Iteration: 28797; Percent complete: 96.0%; Average loss: 0.1331\n",
            "Iteration: 28798; Percent complete: 96.0%; Average loss: 0.1206\n",
            "Iteration: 28799; Percent complete: 96.0%; Average loss: 0.1385\n",
            "Iteration: 28800; Percent complete: 96.0%; Average loss: 0.0907\n",
            "Iteration: 28801; Percent complete: 96.0%; Average loss: 0.1311\n",
            "Iteration: 28802; Percent complete: 96.0%; Average loss: 0.1018\n",
            "Iteration: 28803; Percent complete: 96.0%; Average loss: 0.1352\n",
            "Iteration: 28804; Percent complete: 96.0%; Average loss: 0.0961\n",
            "Iteration: 28805; Percent complete: 96.0%; Average loss: 0.0857\n",
            "Iteration: 28806; Percent complete: 96.0%; Average loss: 0.1283\n",
            "Iteration: 28807; Percent complete: 96.0%; Average loss: 0.1223\n",
            "Iteration: 28808; Percent complete: 96.0%; Average loss: 0.0767\n",
            "Iteration: 28809; Percent complete: 96.0%; Average loss: 0.1133\n",
            "Iteration: 28810; Percent complete: 96.0%; Average loss: 0.1551\n",
            "Iteration: 28811; Percent complete: 96.0%; Average loss: 0.1193\n",
            "Iteration: 28812; Percent complete: 96.0%; Average loss: 0.1369\n",
            "Iteration: 28813; Percent complete: 96.0%; Average loss: 0.1253\n",
            "Iteration: 28814; Percent complete: 96.0%; Average loss: 0.1378\n",
            "Iteration: 28815; Percent complete: 96.0%; Average loss: 0.1799\n",
            "Iteration: 28816; Percent complete: 96.1%; Average loss: 0.1076\n",
            "Iteration: 28817; Percent complete: 96.1%; Average loss: 0.1480\n",
            "Iteration: 28818; Percent complete: 96.1%; Average loss: 0.1172\n",
            "Iteration: 28819; Percent complete: 96.1%; Average loss: 0.1119\n",
            "Iteration: 28820; Percent complete: 96.1%; Average loss: 0.1430\n",
            "Iteration: 28821; Percent complete: 96.1%; Average loss: 0.1021\n",
            "Iteration: 28822; Percent complete: 96.1%; Average loss: 0.1383\n",
            "Iteration: 28823; Percent complete: 96.1%; Average loss: 0.1294\n",
            "Iteration: 28824; Percent complete: 96.1%; Average loss: 0.0886\n",
            "Iteration: 28825; Percent complete: 96.1%; Average loss: 0.1403\n",
            "Iteration: 28826; Percent complete: 96.1%; Average loss: 0.0851\n",
            "Iteration: 28827; Percent complete: 96.1%; Average loss: 0.1282\n",
            "Iteration: 28828; Percent complete: 96.1%; Average loss: 0.1750\n",
            "Iteration: 28829; Percent complete: 96.1%; Average loss: 0.1264\n",
            "Iteration: 28830; Percent complete: 96.1%; Average loss: 0.0925\n",
            "Iteration: 28831; Percent complete: 96.1%; Average loss: 0.1256\n",
            "Iteration: 28832; Percent complete: 96.1%; Average loss: 0.1095\n",
            "Iteration: 28833; Percent complete: 96.1%; Average loss: 0.1706\n",
            "Iteration: 28834; Percent complete: 96.1%; Average loss: 0.1200\n",
            "Iteration: 28835; Percent complete: 96.1%; Average loss: 0.1272\n",
            "Iteration: 28836; Percent complete: 96.1%; Average loss: 0.1161\n",
            "Iteration: 28837; Percent complete: 96.1%; Average loss: 0.1243\n",
            "Iteration: 28838; Percent complete: 96.1%; Average loss: 0.1215\n",
            "Iteration: 28839; Percent complete: 96.1%; Average loss: 0.0940\n",
            "Iteration: 28840; Percent complete: 96.1%; Average loss: 0.1949\n",
            "Iteration: 28841; Percent complete: 96.1%; Average loss: 0.1231\n",
            "Iteration: 28842; Percent complete: 96.1%; Average loss: 0.0864\n",
            "Iteration: 28843; Percent complete: 96.1%; Average loss: 0.1244\n",
            "Iteration: 28844; Percent complete: 96.1%; Average loss: 0.1301\n",
            "Iteration: 28845; Percent complete: 96.2%; Average loss: 0.0986\n",
            "Iteration: 28846; Percent complete: 96.2%; Average loss: 0.1052\n",
            "Iteration: 28847; Percent complete: 96.2%; Average loss: 0.1394\n",
            "Iteration: 28848; Percent complete: 96.2%; Average loss: 0.1376\n",
            "Iteration: 28849; Percent complete: 96.2%; Average loss: 0.0969\n",
            "Iteration: 28850; Percent complete: 96.2%; Average loss: 0.0973\n",
            "Iteration: 28851; Percent complete: 96.2%; Average loss: 0.1028\n",
            "Iteration: 28852; Percent complete: 96.2%; Average loss: 0.1238\n",
            "Iteration: 28853; Percent complete: 96.2%; Average loss: 0.1148\n",
            "Iteration: 28854; Percent complete: 96.2%; Average loss: 0.1153\n",
            "Iteration: 28855; Percent complete: 96.2%; Average loss: 0.0988\n",
            "Iteration: 28856; Percent complete: 96.2%; Average loss: 0.1811\n",
            "Iteration: 28857; Percent complete: 96.2%; Average loss: 0.1805\n",
            "Iteration: 28858; Percent complete: 96.2%; Average loss: 0.1247\n",
            "Iteration: 28859; Percent complete: 96.2%; Average loss: 0.2238\n",
            "Iteration: 28860; Percent complete: 96.2%; Average loss: 0.0993\n",
            "Iteration: 28861; Percent complete: 96.2%; Average loss: 0.1279\n",
            "Iteration: 28862; Percent complete: 96.2%; Average loss: 0.1005\n",
            "Iteration: 28863; Percent complete: 96.2%; Average loss: 0.1122\n",
            "Iteration: 28864; Percent complete: 96.2%; Average loss: 0.1235\n",
            "Iteration: 28865; Percent complete: 96.2%; Average loss: 0.1344\n",
            "Iteration: 28866; Percent complete: 96.2%; Average loss: 0.1349\n",
            "Iteration: 28867; Percent complete: 96.2%; Average loss: 0.1029\n",
            "Iteration: 28868; Percent complete: 96.2%; Average loss: 0.0946\n",
            "Iteration: 28869; Percent complete: 96.2%; Average loss: 0.1100\n",
            "Iteration: 28870; Percent complete: 96.2%; Average loss: 0.0992\n",
            "Iteration: 28871; Percent complete: 96.2%; Average loss: 0.1100\n",
            "Iteration: 28872; Percent complete: 96.2%; Average loss: 0.1273\n",
            "Iteration: 28873; Percent complete: 96.2%; Average loss: 0.1176\n",
            "Iteration: 28874; Percent complete: 96.2%; Average loss: 0.1207\n",
            "Iteration: 28875; Percent complete: 96.2%; Average loss: 0.0734\n",
            "Iteration: 28876; Percent complete: 96.3%; Average loss: 0.1475\n",
            "Iteration: 28877; Percent complete: 96.3%; Average loss: 0.1392\n",
            "Iteration: 28878; Percent complete: 96.3%; Average loss: 0.1018\n",
            "Iteration: 28879; Percent complete: 96.3%; Average loss: 0.1279\n",
            "Iteration: 28880; Percent complete: 96.3%; Average loss: 0.0850\n",
            "Iteration: 28881; Percent complete: 96.3%; Average loss: 0.1704\n",
            "Iteration: 28882; Percent complete: 96.3%; Average loss: 0.0820\n",
            "Iteration: 28883; Percent complete: 96.3%; Average loss: 0.1864\n",
            "Iteration: 28884; Percent complete: 96.3%; Average loss: 0.1056\n",
            "Iteration: 28885; Percent complete: 96.3%; Average loss: 0.1072\n",
            "Iteration: 28886; Percent complete: 96.3%; Average loss: 0.1309\n",
            "Iteration: 28887; Percent complete: 96.3%; Average loss: 0.1170\n",
            "Iteration: 28888; Percent complete: 96.3%; Average loss: 0.1480\n",
            "Iteration: 28889; Percent complete: 96.3%; Average loss: 0.1057\n",
            "Iteration: 28890; Percent complete: 96.3%; Average loss: 0.0957\n",
            "Iteration: 28891; Percent complete: 96.3%; Average loss: 0.1046\n",
            "Iteration: 28892; Percent complete: 96.3%; Average loss: 0.1478\n",
            "Iteration: 28893; Percent complete: 96.3%; Average loss: 0.1064\n",
            "Iteration: 28894; Percent complete: 96.3%; Average loss: 0.1427\n",
            "Iteration: 28895; Percent complete: 96.3%; Average loss: 0.1068\n",
            "Iteration: 28896; Percent complete: 96.3%; Average loss: 0.0900\n",
            "Iteration: 28897; Percent complete: 96.3%; Average loss: 0.1466\n",
            "Iteration: 28898; Percent complete: 96.3%; Average loss: 0.0872\n",
            "Iteration: 28899; Percent complete: 96.3%; Average loss: 0.1349\n",
            "Iteration: 28900; Percent complete: 96.3%; Average loss: 0.1609\n",
            "Iteration: 28901; Percent complete: 96.3%; Average loss: 0.0858\n",
            "Iteration: 28902; Percent complete: 96.3%; Average loss: 0.0947\n",
            "Iteration: 28903; Percent complete: 96.3%; Average loss: 0.0942\n",
            "Iteration: 28904; Percent complete: 96.3%; Average loss: 0.1625\n",
            "Iteration: 28905; Percent complete: 96.4%; Average loss: 0.1280\n",
            "Iteration: 28906; Percent complete: 96.4%; Average loss: 0.1054\n",
            "Iteration: 28907; Percent complete: 96.4%; Average loss: 0.1226\n",
            "Iteration: 28908; Percent complete: 96.4%; Average loss: 0.1078\n",
            "Iteration: 28909; Percent complete: 96.4%; Average loss: 0.1545\n",
            "Iteration: 28910; Percent complete: 96.4%; Average loss: 0.1027\n",
            "Iteration: 28911; Percent complete: 96.4%; Average loss: 0.0997\n",
            "Iteration: 28912; Percent complete: 96.4%; Average loss: 0.1289\n",
            "Iteration: 28913; Percent complete: 96.4%; Average loss: 0.1512\n",
            "Iteration: 28914; Percent complete: 96.4%; Average loss: 0.1147\n",
            "Iteration: 28915; Percent complete: 96.4%; Average loss: 0.0881\n",
            "Iteration: 28916; Percent complete: 96.4%; Average loss: 0.0999\n",
            "Iteration: 28917; Percent complete: 96.4%; Average loss: 0.1341\n",
            "Iteration: 28918; Percent complete: 96.4%; Average loss: 0.1545\n",
            "Iteration: 28919; Percent complete: 96.4%; Average loss: 0.1510\n",
            "Iteration: 28920; Percent complete: 96.4%; Average loss: 0.2023\n",
            "Iteration: 28921; Percent complete: 96.4%; Average loss: 0.1490\n",
            "Iteration: 28922; Percent complete: 96.4%; Average loss: 0.1449\n",
            "Iteration: 28923; Percent complete: 96.4%; Average loss: 0.0897\n",
            "Iteration: 28924; Percent complete: 96.4%; Average loss: 0.1278\n",
            "Iteration: 28925; Percent complete: 96.4%; Average loss: 0.1682\n",
            "Iteration: 28926; Percent complete: 96.4%; Average loss: 0.1524\n",
            "Iteration: 28927; Percent complete: 96.4%; Average loss: 0.1179\n",
            "Iteration: 28928; Percent complete: 96.4%; Average loss: 0.1507\n",
            "Iteration: 28929; Percent complete: 96.4%; Average loss: 0.1455\n",
            "Iteration: 28930; Percent complete: 96.4%; Average loss: 0.1152\n",
            "Iteration: 28931; Percent complete: 96.4%; Average loss: 0.0813\n",
            "Iteration: 28932; Percent complete: 96.4%; Average loss: 0.1433\n",
            "Iteration: 28933; Percent complete: 96.4%; Average loss: 0.1233\n",
            "Iteration: 28934; Percent complete: 96.4%; Average loss: 0.1363\n",
            "Iteration: 28935; Percent complete: 96.5%; Average loss: 0.1363\n",
            "Iteration: 28936; Percent complete: 96.5%; Average loss: 0.1335\n",
            "Iteration: 28937; Percent complete: 96.5%; Average loss: 0.1109\n",
            "Iteration: 28938; Percent complete: 96.5%; Average loss: 0.0635\n",
            "Iteration: 28939; Percent complete: 96.5%; Average loss: 0.1307\n",
            "Iteration: 28940; Percent complete: 96.5%; Average loss: 0.0972\n",
            "Iteration: 28941; Percent complete: 96.5%; Average loss: 0.1529\n",
            "Iteration: 28942; Percent complete: 96.5%; Average loss: 0.1021\n",
            "Iteration: 28943; Percent complete: 96.5%; Average loss: 0.1457\n",
            "Iteration: 28944; Percent complete: 96.5%; Average loss: 0.1903\n",
            "Iteration: 28945; Percent complete: 96.5%; Average loss: 0.1364\n",
            "Iteration: 28946; Percent complete: 96.5%; Average loss: 0.1815\n",
            "Iteration: 28947; Percent complete: 96.5%; Average loss: 0.1022\n",
            "Iteration: 28948; Percent complete: 96.5%; Average loss: 0.1094\n",
            "Iteration: 28949; Percent complete: 96.5%; Average loss: 0.0951\n",
            "Iteration: 28950; Percent complete: 96.5%; Average loss: 0.1008\n",
            "Iteration: 28951; Percent complete: 96.5%; Average loss: 0.1365\n",
            "Iteration: 28952; Percent complete: 96.5%; Average loss: 0.1364\n",
            "Iteration: 28953; Percent complete: 96.5%; Average loss: 0.1712\n",
            "Iteration: 28954; Percent complete: 96.5%; Average loss: 0.1176\n",
            "Iteration: 28955; Percent complete: 96.5%; Average loss: 0.1303\n",
            "Iteration: 28956; Percent complete: 96.5%; Average loss: 0.1271\n",
            "Iteration: 28957; Percent complete: 96.5%; Average loss: 0.1403\n",
            "Iteration: 28958; Percent complete: 96.5%; Average loss: 0.1342\n",
            "Iteration: 28959; Percent complete: 96.5%; Average loss: 0.0907\n",
            "Iteration: 28960; Percent complete: 96.5%; Average loss: 0.1149\n",
            "Iteration: 28961; Percent complete: 96.5%; Average loss: 0.1317\n",
            "Iteration: 28962; Percent complete: 96.5%; Average loss: 0.1056\n",
            "Iteration: 28963; Percent complete: 96.5%; Average loss: 0.1057\n",
            "Iteration: 28964; Percent complete: 96.5%; Average loss: 0.1631\n",
            "Iteration: 28965; Percent complete: 96.5%; Average loss: 0.1518\n",
            "Iteration: 28966; Percent complete: 96.6%; Average loss: 0.1332\n",
            "Iteration: 28967; Percent complete: 96.6%; Average loss: 0.1183\n",
            "Iteration: 28968; Percent complete: 96.6%; Average loss: 0.1112\n",
            "Iteration: 28969; Percent complete: 96.6%; Average loss: 0.1203\n",
            "Iteration: 28970; Percent complete: 96.6%; Average loss: 0.0925\n",
            "Iteration: 28971; Percent complete: 96.6%; Average loss: 0.1066\n",
            "Iteration: 28972; Percent complete: 96.6%; Average loss: 0.1634\n",
            "Iteration: 28973; Percent complete: 96.6%; Average loss: 0.1149\n",
            "Iteration: 28974; Percent complete: 96.6%; Average loss: 0.1345\n",
            "Iteration: 28975; Percent complete: 96.6%; Average loss: 0.1468\n",
            "Iteration: 28976; Percent complete: 96.6%; Average loss: 0.0747\n",
            "Iteration: 28977; Percent complete: 96.6%; Average loss: 0.1565\n",
            "Iteration: 28978; Percent complete: 96.6%; Average loss: 0.1607\n",
            "Iteration: 28979; Percent complete: 96.6%; Average loss: 0.1353\n",
            "Iteration: 28980; Percent complete: 96.6%; Average loss: 0.1199\n",
            "Iteration: 28981; Percent complete: 96.6%; Average loss: 0.1096\n",
            "Iteration: 28982; Percent complete: 96.6%; Average loss: 0.1139\n",
            "Iteration: 28983; Percent complete: 96.6%; Average loss: 0.1384\n",
            "Iteration: 28984; Percent complete: 96.6%; Average loss: 0.0907\n",
            "Iteration: 28985; Percent complete: 96.6%; Average loss: 0.1365\n",
            "Iteration: 28986; Percent complete: 96.6%; Average loss: 0.1664\n",
            "Iteration: 28987; Percent complete: 96.6%; Average loss: 0.1345\n",
            "Iteration: 28988; Percent complete: 96.6%; Average loss: 0.1448\n",
            "Iteration: 28989; Percent complete: 96.6%; Average loss: 0.1117\n",
            "Iteration: 28990; Percent complete: 96.6%; Average loss: 0.1154\n",
            "Iteration: 28991; Percent complete: 96.6%; Average loss: 0.1313\n",
            "Iteration: 28992; Percent complete: 96.6%; Average loss: 0.1380\n",
            "Iteration: 28993; Percent complete: 96.6%; Average loss: 0.1021\n",
            "Iteration: 28994; Percent complete: 96.6%; Average loss: 0.1496\n",
            "Iteration: 28995; Percent complete: 96.7%; Average loss: 0.1123\n",
            "Iteration: 28996; Percent complete: 96.7%; Average loss: 0.1072\n",
            "Iteration: 28997; Percent complete: 96.7%; Average loss: 0.1105\n",
            "Iteration: 28998; Percent complete: 96.7%; Average loss: 0.1176\n",
            "Iteration: 28999; Percent complete: 96.7%; Average loss: 0.1634\n",
            "Iteration: 29000; Percent complete: 96.7%; Average loss: 0.0815\n",
            "Iteration: 29001; Percent complete: 96.7%; Average loss: 0.1272\n",
            "Iteration: 29002; Percent complete: 96.7%; Average loss: 0.1217\n",
            "Iteration: 29003; Percent complete: 96.7%; Average loss: 0.1199\n",
            "Iteration: 29004; Percent complete: 96.7%; Average loss: 0.1192\n",
            "Iteration: 29005; Percent complete: 96.7%; Average loss: 0.0943\n",
            "Iteration: 29006; Percent complete: 96.7%; Average loss: 0.1232\n",
            "Iteration: 29007; Percent complete: 96.7%; Average loss: 0.1339\n",
            "Iteration: 29008; Percent complete: 96.7%; Average loss: 0.1003\n",
            "Iteration: 29009; Percent complete: 96.7%; Average loss: 0.1612\n",
            "Iteration: 29010; Percent complete: 96.7%; Average loss: 0.1307\n",
            "Iteration: 29011; Percent complete: 96.7%; Average loss: 0.1431\n",
            "Iteration: 29012; Percent complete: 96.7%; Average loss: 0.1136\n",
            "Iteration: 29013; Percent complete: 96.7%; Average loss: 0.1141\n",
            "Iteration: 29014; Percent complete: 96.7%; Average loss: 0.0921\n",
            "Iteration: 29015; Percent complete: 96.7%; Average loss: 0.0916\n",
            "Iteration: 29016; Percent complete: 96.7%; Average loss: 0.1535\n",
            "Iteration: 29017; Percent complete: 96.7%; Average loss: 0.1065\n",
            "Iteration: 29018; Percent complete: 96.7%; Average loss: 0.1341\n",
            "Iteration: 29019; Percent complete: 96.7%; Average loss: 0.1122\n",
            "Iteration: 29020; Percent complete: 96.7%; Average loss: 0.1059\n",
            "Iteration: 29021; Percent complete: 96.7%; Average loss: 0.0850\n",
            "Iteration: 29022; Percent complete: 96.7%; Average loss: 0.0915\n",
            "Iteration: 29023; Percent complete: 96.7%; Average loss: 0.0777\n",
            "Iteration: 29024; Percent complete: 96.7%; Average loss: 0.1320\n",
            "Iteration: 29025; Percent complete: 96.8%; Average loss: 0.1239\n",
            "Iteration: 29026; Percent complete: 96.8%; Average loss: 0.1580\n",
            "Iteration: 29027; Percent complete: 96.8%; Average loss: 0.1153\n",
            "Iteration: 29028; Percent complete: 96.8%; Average loss: 0.1296\n",
            "Iteration: 29029; Percent complete: 96.8%; Average loss: 0.1426\n",
            "Iteration: 29030; Percent complete: 96.8%; Average loss: 0.0939\n",
            "Iteration: 29031; Percent complete: 96.8%; Average loss: 0.1200\n",
            "Iteration: 29032; Percent complete: 96.8%; Average loss: 0.1648\n",
            "Iteration: 29033; Percent complete: 96.8%; Average loss: 0.0870\n",
            "Iteration: 29034; Percent complete: 96.8%; Average loss: 0.1427\n",
            "Iteration: 29035; Percent complete: 96.8%; Average loss: 0.1282\n",
            "Iteration: 29036; Percent complete: 96.8%; Average loss: 0.1193\n",
            "Iteration: 29037; Percent complete: 96.8%; Average loss: 0.1115\n",
            "Iteration: 29038; Percent complete: 96.8%; Average loss: 0.1120\n",
            "Iteration: 29039; Percent complete: 96.8%; Average loss: 0.1327\n",
            "Iteration: 29040; Percent complete: 96.8%; Average loss: 0.1269\n",
            "Iteration: 29041; Percent complete: 96.8%; Average loss: 0.1012\n",
            "Iteration: 29042; Percent complete: 96.8%; Average loss: 0.1232\n",
            "Iteration: 29043; Percent complete: 96.8%; Average loss: 0.1039\n",
            "Iteration: 29044; Percent complete: 96.8%; Average loss: 0.1028\n",
            "Iteration: 29045; Percent complete: 96.8%; Average loss: 0.1248\n",
            "Iteration: 29046; Percent complete: 96.8%; Average loss: 0.0884\n",
            "Iteration: 29047; Percent complete: 96.8%; Average loss: 0.1219\n",
            "Iteration: 29048; Percent complete: 96.8%; Average loss: 0.1037\n",
            "Iteration: 29049; Percent complete: 96.8%; Average loss: 0.1210\n",
            "Iteration: 29050; Percent complete: 96.8%; Average loss: 0.0947\n",
            "Iteration: 29051; Percent complete: 96.8%; Average loss: 0.0960\n",
            "Iteration: 29052; Percent complete: 96.8%; Average loss: 0.0705\n",
            "Iteration: 29053; Percent complete: 96.8%; Average loss: 0.0844\n",
            "Iteration: 29054; Percent complete: 96.8%; Average loss: 0.1039\n",
            "Iteration: 29055; Percent complete: 96.9%; Average loss: 0.1828\n",
            "Iteration: 29056; Percent complete: 96.9%; Average loss: 0.1743\n",
            "Iteration: 29057; Percent complete: 96.9%; Average loss: 0.0707\n",
            "Iteration: 29058; Percent complete: 96.9%; Average loss: 0.1327\n",
            "Iteration: 29059; Percent complete: 96.9%; Average loss: 0.1467\n",
            "Iteration: 29060; Percent complete: 96.9%; Average loss: 0.1249\n",
            "Iteration: 29061; Percent complete: 96.9%; Average loss: 0.1009\n",
            "Iteration: 29062; Percent complete: 96.9%; Average loss: 0.0897\n",
            "Iteration: 29063; Percent complete: 96.9%; Average loss: 0.1178\n",
            "Iteration: 29064; Percent complete: 96.9%; Average loss: 0.1139\n",
            "Iteration: 29065; Percent complete: 96.9%; Average loss: 0.1278\n",
            "Iteration: 29066; Percent complete: 96.9%; Average loss: 0.1547\n",
            "Iteration: 29067; Percent complete: 96.9%; Average loss: 0.1390\n",
            "Iteration: 29068; Percent complete: 96.9%; Average loss: 0.1141\n",
            "Iteration: 29069; Percent complete: 96.9%; Average loss: 0.1431\n",
            "Iteration: 29070; Percent complete: 96.9%; Average loss: 0.1103\n",
            "Iteration: 29071; Percent complete: 96.9%; Average loss: 0.1316\n",
            "Iteration: 29072; Percent complete: 96.9%; Average loss: 0.1634\n",
            "Iteration: 29073; Percent complete: 96.9%; Average loss: 0.1235\n",
            "Iteration: 29074; Percent complete: 96.9%; Average loss: 0.1627\n",
            "Iteration: 29075; Percent complete: 96.9%; Average loss: 0.1488\n",
            "Iteration: 29076; Percent complete: 96.9%; Average loss: 0.1004\n",
            "Iteration: 29077; Percent complete: 96.9%; Average loss: 0.1145\n",
            "Iteration: 29078; Percent complete: 96.9%; Average loss: 0.1126\n",
            "Iteration: 29079; Percent complete: 96.9%; Average loss: 0.1078\n",
            "Iteration: 29080; Percent complete: 96.9%; Average loss: 0.1190\n",
            "Iteration: 29081; Percent complete: 96.9%; Average loss: 0.1129\n",
            "Iteration: 29082; Percent complete: 96.9%; Average loss: 0.1329\n",
            "Iteration: 29083; Percent complete: 96.9%; Average loss: 0.1160\n",
            "Iteration: 29084; Percent complete: 96.9%; Average loss: 0.1027\n",
            "Iteration: 29085; Percent complete: 97.0%; Average loss: 0.1169\n",
            "Iteration: 29086; Percent complete: 97.0%; Average loss: 0.0607\n",
            "Iteration: 29087; Percent complete: 97.0%; Average loss: 0.1111\n",
            "Iteration: 29088; Percent complete: 97.0%; Average loss: 0.1711\n",
            "Iteration: 29089; Percent complete: 97.0%; Average loss: 0.1047\n",
            "Iteration: 29090; Percent complete: 97.0%; Average loss: 0.1020\n",
            "Iteration: 29091; Percent complete: 97.0%; Average loss: 0.0856\n",
            "Iteration: 29092; Percent complete: 97.0%; Average loss: 0.1329\n",
            "Iteration: 29093; Percent complete: 97.0%; Average loss: 0.2248\n",
            "Iteration: 29094; Percent complete: 97.0%; Average loss: 0.1010\n",
            "Iteration: 29095; Percent complete: 97.0%; Average loss: 0.1195\n",
            "Iteration: 29096; Percent complete: 97.0%; Average loss: 0.1254\n",
            "Iteration: 29097; Percent complete: 97.0%; Average loss: 0.1136\n",
            "Iteration: 29098; Percent complete: 97.0%; Average loss: 0.0857\n",
            "Iteration: 29099; Percent complete: 97.0%; Average loss: 0.1218\n",
            "Iteration: 29100; Percent complete: 97.0%; Average loss: 0.1116\n",
            "Iteration: 29101; Percent complete: 97.0%; Average loss: 0.1348\n",
            "Iteration: 29102; Percent complete: 97.0%; Average loss: 0.1498\n",
            "Iteration: 29103; Percent complete: 97.0%; Average loss: 0.1837\n",
            "Iteration: 29104; Percent complete: 97.0%; Average loss: 0.0869\n",
            "Iteration: 29105; Percent complete: 97.0%; Average loss: 0.1406\n",
            "Iteration: 29106; Percent complete: 97.0%; Average loss: 0.1517\n",
            "Iteration: 29107; Percent complete: 97.0%; Average loss: 0.1238\n",
            "Iteration: 29108; Percent complete: 97.0%; Average loss: 0.1551\n",
            "Iteration: 29109; Percent complete: 97.0%; Average loss: 0.1092\n",
            "Iteration: 29110; Percent complete: 97.0%; Average loss: 0.1013\n",
            "Iteration: 29111; Percent complete: 97.0%; Average loss: 0.1292\n",
            "Iteration: 29112; Percent complete: 97.0%; Average loss: 0.1332\n",
            "Iteration: 29113; Percent complete: 97.0%; Average loss: 0.1399\n",
            "Iteration: 29114; Percent complete: 97.0%; Average loss: 0.1079\n",
            "Iteration: 29115; Percent complete: 97.0%; Average loss: 0.0901\n",
            "Iteration: 29116; Percent complete: 97.1%; Average loss: 0.1225\n",
            "Iteration: 29117; Percent complete: 97.1%; Average loss: 0.1060\n",
            "Iteration: 29118; Percent complete: 97.1%; Average loss: 0.1081\n",
            "Iteration: 29119; Percent complete: 97.1%; Average loss: 0.1389\n",
            "Iteration: 29120; Percent complete: 97.1%; Average loss: 0.1262\n",
            "Iteration: 29121; Percent complete: 97.1%; Average loss: 0.1487\n",
            "Iteration: 29122; Percent complete: 97.1%; Average loss: 0.0946\n",
            "Iteration: 29123; Percent complete: 97.1%; Average loss: 0.1201\n",
            "Iteration: 29124; Percent complete: 97.1%; Average loss: 0.0885\n",
            "Iteration: 29125; Percent complete: 97.1%; Average loss: 0.1118\n",
            "Iteration: 29126; Percent complete: 97.1%; Average loss: 0.1175\n",
            "Iteration: 29127; Percent complete: 97.1%; Average loss: 0.1557\n",
            "Iteration: 29128; Percent complete: 97.1%; Average loss: 0.1215\n",
            "Iteration: 29129; Percent complete: 97.1%; Average loss: 0.1202\n",
            "Iteration: 29130; Percent complete: 97.1%; Average loss: 0.1504\n",
            "Iteration: 29131; Percent complete: 97.1%; Average loss: 0.1095\n",
            "Iteration: 29132; Percent complete: 97.1%; Average loss: 0.0922\n",
            "Iteration: 29133; Percent complete: 97.1%; Average loss: 0.1529\n",
            "Iteration: 29134; Percent complete: 97.1%; Average loss: 0.1303\n",
            "Iteration: 29135; Percent complete: 97.1%; Average loss: 0.1218\n",
            "Iteration: 29136; Percent complete: 97.1%; Average loss: 0.1424\n",
            "Iteration: 29137; Percent complete: 97.1%; Average loss: 0.1429\n",
            "Iteration: 29138; Percent complete: 97.1%; Average loss: 0.1357\n",
            "Iteration: 29139; Percent complete: 97.1%; Average loss: 0.1712\n",
            "Iteration: 29140; Percent complete: 97.1%; Average loss: 0.1355\n",
            "Iteration: 29141; Percent complete: 97.1%; Average loss: 0.1512\n",
            "Iteration: 29142; Percent complete: 97.1%; Average loss: 0.1227\n",
            "Iteration: 29143; Percent complete: 97.1%; Average loss: 0.1055\n",
            "Iteration: 29144; Percent complete: 97.1%; Average loss: 0.1275\n",
            "Iteration: 29145; Percent complete: 97.2%; Average loss: 0.0975\n",
            "Iteration: 29146; Percent complete: 97.2%; Average loss: 0.1328\n",
            "Iteration: 29147; Percent complete: 97.2%; Average loss: 0.1504\n",
            "Iteration: 29148; Percent complete: 97.2%; Average loss: 0.0884\n",
            "Iteration: 29149; Percent complete: 97.2%; Average loss: 0.1204\n",
            "Iteration: 29150; Percent complete: 97.2%; Average loss: 0.0981\n",
            "Iteration: 29151; Percent complete: 97.2%; Average loss: 0.0998\n",
            "Iteration: 29152; Percent complete: 97.2%; Average loss: 0.1424\n",
            "Iteration: 29153; Percent complete: 97.2%; Average loss: 0.1553\n",
            "Iteration: 29154; Percent complete: 97.2%; Average loss: 0.1014\n",
            "Iteration: 29155; Percent complete: 97.2%; Average loss: 0.0966\n",
            "Iteration: 29156; Percent complete: 97.2%; Average loss: 0.1399\n",
            "Iteration: 29157; Percent complete: 97.2%; Average loss: 0.1152\n",
            "Iteration: 29158; Percent complete: 97.2%; Average loss: 0.1925\n",
            "Iteration: 29159; Percent complete: 97.2%; Average loss: 0.1098\n",
            "Iteration: 29160; Percent complete: 97.2%; Average loss: 0.0809\n",
            "Iteration: 29161; Percent complete: 97.2%; Average loss: 0.1349\n",
            "Iteration: 29162; Percent complete: 97.2%; Average loss: 0.0791\n",
            "Iteration: 29163; Percent complete: 97.2%; Average loss: 0.1122\n",
            "Iteration: 29164; Percent complete: 97.2%; Average loss: 0.1374\n",
            "Iteration: 29165; Percent complete: 97.2%; Average loss: 0.1112\n",
            "Iteration: 29166; Percent complete: 97.2%; Average loss: 0.0955\n",
            "Iteration: 29167; Percent complete: 97.2%; Average loss: 0.1360\n",
            "Iteration: 29168; Percent complete: 97.2%; Average loss: 0.1281\n",
            "Iteration: 29169; Percent complete: 97.2%; Average loss: 0.0992\n",
            "Iteration: 29170; Percent complete: 97.2%; Average loss: 0.1285\n",
            "Iteration: 29171; Percent complete: 97.2%; Average loss: 0.0815\n",
            "Iteration: 29172; Percent complete: 97.2%; Average loss: 0.1647\n",
            "Iteration: 29173; Percent complete: 97.2%; Average loss: 0.1823\n",
            "Iteration: 29174; Percent complete: 97.2%; Average loss: 0.1139\n",
            "Iteration: 29175; Percent complete: 97.2%; Average loss: 0.1090\n",
            "Iteration: 29176; Percent complete: 97.3%; Average loss: 0.0790\n",
            "Iteration: 29177; Percent complete: 97.3%; Average loss: 0.1053\n",
            "Iteration: 29178; Percent complete: 97.3%; Average loss: 0.1267\n",
            "Iteration: 29179; Percent complete: 97.3%; Average loss: 0.1351\n",
            "Iteration: 29180; Percent complete: 97.3%; Average loss: 0.1197\n",
            "Iteration: 29181; Percent complete: 97.3%; Average loss: 0.1261\n",
            "Iteration: 29182; Percent complete: 97.3%; Average loss: 0.1312\n",
            "Iteration: 29183; Percent complete: 97.3%; Average loss: 0.0885\n",
            "Iteration: 29184; Percent complete: 97.3%; Average loss: 0.0910\n",
            "Iteration: 29185; Percent complete: 97.3%; Average loss: 0.1265\n",
            "Iteration: 29186; Percent complete: 97.3%; Average loss: 0.1351\n",
            "Iteration: 29187; Percent complete: 97.3%; Average loss: 0.1176\n",
            "Iteration: 29188; Percent complete: 97.3%; Average loss: 0.1414\n",
            "Iteration: 29189; Percent complete: 97.3%; Average loss: 0.1625\n",
            "Iteration: 29190; Percent complete: 97.3%; Average loss: 0.0777\n",
            "Iteration: 29191; Percent complete: 97.3%; Average loss: 0.1120\n",
            "Iteration: 29192; Percent complete: 97.3%; Average loss: 0.1525\n",
            "Iteration: 29193; Percent complete: 97.3%; Average loss: 0.0898\n",
            "Iteration: 29194; Percent complete: 97.3%; Average loss: 0.1368\n",
            "Iteration: 29195; Percent complete: 97.3%; Average loss: 0.1159\n",
            "Iteration: 29196; Percent complete: 97.3%; Average loss: 0.1339\n",
            "Iteration: 29197; Percent complete: 97.3%; Average loss: 0.1051\n",
            "Iteration: 29198; Percent complete: 97.3%; Average loss: 0.1225\n",
            "Iteration: 29199; Percent complete: 97.3%; Average loss: 0.1179\n",
            "Iteration: 29200; Percent complete: 97.3%; Average loss: 0.1104\n",
            "Iteration: 29201; Percent complete: 97.3%; Average loss: 0.1569\n",
            "Iteration: 29202; Percent complete: 97.3%; Average loss: 0.1436\n",
            "Iteration: 29203; Percent complete: 97.3%; Average loss: 0.0899\n",
            "Iteration: 29204; Percent complete: 97.3%; Average loss: 0.1017\n",
            "Iteration: 29205; Percent complete: 97.4%; Average loss: 0.1134\n",
            "Iteration: 29206; Percent complete: 97.4%; Average loss: 0.1539\n",
            "Iteration: 29207; Percent complete: 97.4%; Average loss: 0.1150\n",
            "Iteration: 29208; Percent complete: 97.4%; Average loss: 0.1259\n",
            "Iteration: 29209; Percent complete: 97.4%; Average loss: 0.1038\n",
            "Iteration: 29210; Percent complete: 97.4%; Average loss: 0.1085\n",
            "Iteration: 29211; Percent complete: 97.4%; Average loss: 0.1017\n",
            "Iteration: 29212; Percent complete: 97.4%; Average loss: 0.1134\n",
            "Iteration: 29213; Percent complete: 97.4%; Average loss: 0.1053\n",
            "Iteration: 29214; Percent complete: 97.4%; Average loss: 0.1071\n",
            "Iteration: 29215; Percent complete: 97.4%; Average loss: 0.1091\n",
            "Iteration: 29216; Percent complete: 97.4%; Average loss: 0.0932\n",
            "Iteration: 29217; Percent complete: 97.4%; Average loss: 0.1107\n",
            "Iteration: 29218; Percent complete: 97.4%; Average loss: 0.1087\n",
            "Iteration: 29219; Percent complete: 97.4%; Average loss: 0.1646\n",
            "Iteration: 29220; Percent complete: 97.4%; Average loss: 0.1561\n",
            "Iteration: 29221; Percent complete: 97.4%; Average loss: 0.1046\n",
            "Iteration: 29222; Percent complete: 97.4%; Average loss: 0.0664\n",
            "Iteration: 29223; Percent complete: 97.4%; Average loss: 0.0808\n",
            "Iteration: 29224; Percent complete: 97.4%; Average loss: 0.0995\n",
            "Iteration: 29225; Percent complete: 97.4%; Average loss: 0.1075\n",
            "Iteration: 29226; Percent complete: 97.4%; Average loss: 0.1404\n",
            "Iteration: 29227; Percent complete: 97.4%; Average loss: 0.1241\n",
            "Iteration: 29228; Percent complete: 97.4%; Average loss: 0.1141\n",
            "Iteration: 29229; Percent complete: 97.4%; Average loss: 0.0806\n",
            "Iteration: 29230; Percent complete: 97.4%; Average loss: 0.1302\n",
            "Iteration: 29231; Percent complete: 97.4%; Average loss: 0.1585\n",
            "Iteration: 29232; Percent complete: 97.4%; Average loss: 0.0711\n",
            "Iteration: 29233; Percent complete: 97.4%; Average loss: 0.1542\n",
            "Iteration: 29234; Percent complete: 97.4%; Average loss: 0.1421\n",
            "Iteration: 29235; Percent complete: 97.5%; Average loss: 0.1186\n",
            "Iteration: 29236; Percent complete: 97.5%; Average loss: 0.1355\n",
            "Iteration: 29237; Percent complete: 97.5%; Average loss: 0.1064\n",
            "Iteration: 29238; Percent complete: 97.5%; Average loss: 0.0835\n",
            "Iteration: 29239; Percent complete: 97.5%; Average loss: 0.1794\n",
            "Iteration: 29240; Percent complete: 97.5%; Average loss: 0.1066\n",
            "Iteration: 29241; Percent complete: 97.5%; Average loss: 0.1326\n",
            "Iteration: 29242; Percent complete: 97.5%; Average loss: 0.1478\n",
            "Iteration: 29243; Percent complete: 97.5%; Average loss: 0.1649\n",
            "Iteration: 29244; Percent complete: 97.5%; Average loss: 0.1280\n",
            "Iteration: 29245; Percent complete: 97.5%; Average loss: 0.1270\n",
            "Iteration: 29246; Percent complete: 97.5%; Average loss: 0.0902\n",
            "Iteration: 29247; Percent complete: 97.5%; Average loss: 0.1199\n",
            "Iteration: 29248; Percent complete: 97.5%; Average loss: 0.0833\n",
            "Iteration: 29249; Percent complete: 97.5%; Average loss: 0.0724\n",
            "Iteration: 29250; Percent complete: 97.5%; Average loss: 0.0904\n",
            "Iteration: 29251; Percent complete: 97.5%; Average loss: 0.1051\n",
            "Iteration: 29252; Percent complete: 97.5%; Average loss: 0.1854\n",
            "Iteration: 29253; Percent complete: 97.5%; Average loss: 0.1182\n",
            "Iteration: 29254; Percent complete: 97.5%; Average loss: 0.1281\n",
            "Iteration: 29255; Percent complete: 97.5%; Average loss: 0.1140\n",
            "Iteration: 29256; Percent complete: 97.5%; Average loss: 0.1358\n",
            "Iteration: 29257; Percent complete: 97.5%; Average loss: 0.0754\n",
            "Iteration: 29258; Percent complete: 97.5%; Average loss: 0.1352\n",
            "Iteration: 29259; Percent complete: 97.5%; Average loss: 0.1570\n",
            "Iteration: 29260; Percent complete: 97.5%; Average loss: 0.0875\n",
            "Iteration: 29261; Percent complete: 97.5%; Average loss: 0.1096\n",
            "Iteration: 29262; Percent complete: 97.5%; Average loss: 0.1163\n",
            "Iteration: 29263; Percent complete: 97.5%; Average loss: 0.1344\n",
            "Iteration: 29264; Percent complete: 97.5%; Average loss: 0.1473\n",
            "Iteration: 29265; Percent complete: 97.5%; Average loss: 0.1063\n",
            "Iteration: 29266; Percent complete: 97.6%; Average loss: 0.1139\n",
            "Iteration: 29267; Percent complete: 97.6%; Average loss: 0.1225\n",
            "Iteration: 29268; Percent complete: 97.6%; Average loss: 0.1287\n",
            "Iteration: 29269; Percent complete: 97.6%; Average loss: 0.1089\n",
            "Iteration: 29270; Percent complete: 97.6%; Average loss: 0.0927\n",
            "Iteration: 29271; Percent complete: 97.6%; Average loss: 0.1154\n",
            "Iteration: 29272; Percent complete: 97.6%; Average loss: 0.0987\n",
            "Iteration: 29273; Percent complete: 97.6%; Average loss: 0.1319\n",
            "Iteration: 29274; Percent complete: 97.6%; Average loss: 0.1279\n",
            "Iteration: 29275; Percent complete: 97.6%; Average loss: 0.1038\n",
            "Iteration: 29276; Percent complete: 97.6%; Average loss: 0.0976\n",
            "Iteration: 29277; Percent complete: 97.6%; Average loss: 0.1615\n",
            "Iteration: 29278; Percent complete: 97.6%; Average loss: 0.1005\n",
            "Iteration: 29279; Percent complete: 97.6%; Average loss: 0.1182\n",
            "Iteration: 29280; Percent complete: 97.6%; Average loss: 0.1365\n",
            "Iteration: 29281; Percent complete: 97.6%; Average loss: 0.1717\n",
            "Iteration: 29282; Percent complete: 97.6%; Average loss: 0.1166\n",
            "Iteration: 29283; Percent complete: 97.6%; Average loss: 0.1094\n",
            "Iteration: 29284; Percent complete: 97.6%; Average loss: 0.0953\n",
            "Iteration: 29285; Percent complete: 97.6%; Average loss: 0.0901\n",
            "Iteration: 29286; Percent complete: 97.6%; Average loss: 0.1661\n",
            "Iteration: 29287; Percent complete: 97.6%; Average loss: 0.0860\n",
            "Iteration: 29288; Percent complete: 97.6%; Average loss: 0.1246\n",
            "Iteration: 29289; Percent complete: 97.6%; Average loss: 0.1208\n",
            "Iteration: 29290; Percent complete: 97.6%; Average loss: 0.1612\n",
            "Iteration: 29291; Percent complete: 97.6%; Average loss: 0.1375\n",
            "Iteration: 29292; Percent complete: 97.6%; Average loss: 0.1113\n",
            "Iteration: 29293; Percent complete: 97.6%; Average loss: 0.1568\n",
            "Iteration: 29294; Percent complete: 97.6%; Average loss: 0.1410\n",
            "Iteration: 29295; Percent complete: 97.7%; Average loss: 0.1135\n",
            "Iteration: 29296; Percent complete: 97.7%; Average loss: 0.0916\n",
            "Iteration: 29297; Percent complete: 97.7%; Average loss: 0.1138\n",
            "Iteration: 29298; Percent complete: 97.7%; Average loss: 0.1492\n",
            "Iteration: 29299; Percent complete: 97.7%; Average loss: 0.1022\n",
            "Iteration: 29300; Percent complete: 97.7%; Average loss: 0.0958\n",
            "Iteration: 29301; Percent complete: 97.7%; Average loss: 0.1405\n",
            "Iteration: 29302; Percent complete: 97.7%; Average loss: 0.1310\n",
            "Iteration: 29303; Percent complete: 97.7%; Average loss: 0.1588\n",
            "Iteration: 29304; Percent complete: 97.7%; Average loss: 0.1191\n",
            "Iteration: 29305; Percent complete: 97.7%; Average loss: 0.1410\n",
            "Iteration: 29306; Percent complete: 97.7%; Average loss: 0.1138\n",
            "Iteration: 29307; Percent complete: 97.7%; Average loss: 0.1349\n",
            "Iteration: 29308; Percent complete: 97.7%; Average loss: 0.1333\n",
            "Iteration: 29309; Percent complete: 97.7%; Average loss: 0.1105\n",
            "Iteration: 29310; Percent complete: 97.7%; Average loss: 0.1009\n",
            "Iteration: 29311; Percent complete: 97.7%; Average loss: 0.1297\n",
            "Iteration: 29312; Percent complete: 97.7%; Average loss: 0.1142\n",
            "Iteration: 29313; Percent complete: 97.7%; Average loss: 0.1287\n",
            "Iteration: 29314; Percent complete: 97.7%; Average loss: 0.1478\n",
            "Iteration: 29315; Percent complete: 97.7%; Average loss: 0.0596\n",
            "Iteration: 29316; Percent complete: 97.7%; Average loss: 0.1195\n",
            "Iteration: 29317; Percent complete: 97.7%; Average loss: 0.1351\n",
            "Iteration: 29318; Percent complete: 97.7%; Average loss: 0.1174\n",
            "Iteration: 29319; Percent complete: 97.7%; Average loss: 0.1130\n",
            "Iteration: 29320; Percent complete: 97.7%; Average loss: 0.1249\n",
            "Iteration: 29321; Percent complete: 97.7%; Average loss: 0.1526\n",
            "Iteration: 29322; Percent complete: 97.7%; Average loss: 0.1720\n",
            "Iteration: 29323; Percent complete: 97.7%; Average loss: 0.1545\n",
            "Iteration: 29324; Percent complete: 97.7%; Average loss: 0.1289\n",
            "Iteration: 29325; Percent complete: 97.8%; Average loss: 0.1271\n",
            "Iteration: 29326; Percent complete: 97.8%; Average loss: 0.1097\n",
            "Iteration: 29327; Percent complete: 97.8%; Average loss: 0.1324\n",
            "Iteration: 29328; Percent complete: 97.8%; Average loss: 0.1003\n",
            "Iteration: 29329; Percent complete: 97.8%; Average loss: 0.1397\n",
            "Iteration: 29330; Percent complete: 97.8%; Average loss: 0.1138\n",
            "Iteration: 29331; Percent complete: 97.8%; Average loss: 0.1471\n",
            "Iteration: 29332; Percent complete: 97.8%; Average loss: 0.0878\n",
            "Iteration: 29333; Percent complete: 97.8%; Average loss: 0.1298\n",
            "Iteration: 29334; Percent complete: 97.8%; Average loss: 0.1023\n",
            "Iteration: 29335; Percent complete: 97.8%; Average loss: 0.0855\n",
            "Iteration: 29336; Percent complete: 97.8%; Average loss: 0.1331\n",
            "Iteration: 29337; Percent complete: 97.8%; Average loss: 0.1278\n",
            "Iteration: 29338; Percent complete: 97.8%; Average loss: 0.1318\n",
            "Iteration: 29339; Percent complete: 97.8%; Average loss: 0.1128\n",
            "Iteration: 29340; Percent complete: 97.8%; Average loss: 0.1586\n",
            "Iteration: 29341; Percent complete: 97.8%; Average loss: 0.1198\n",
            "Iteration: 29342; Percent complete: 97.8%; Average loss: 0.1118\n",
            "Iteration: 29343; Percent complete: 97.8%; Average loss: 0.1514\n",
            "Iteration: 29344; Percent complete: 97.8%; Average loss: 0.1293\n",
            "Iteration: 29345; Percent complete: 97.8%; Average loss: 0.1427\n",
            "Iteration: 29346; Percent complete: 97.8%; Average loss: 0.1423\n",
            "Iteration: 29347; Percent complete: 97.8%; Average loss: 0.1177\n",
            "Iteration: 29348; Percent complete: 97.8%; Average loss: 0.0819\n",
            "Iteration: 29349; Percent complete: 97.8%; Average loss: 0.1080\n",
            "Iteration: 29350; Percent complete: 97.8%; Average loss: 0.1114\n",
            "Iteration: 29351; Percent complete: 97.8%; Average loss: 0.1530\n",
            "Iteration: 29352; Percent complete: 97.8%; Average loss: 0.1260\n",
            "Iteration: 29353; Percent complete: 97.8%; Average loss: 0.1021\n",
            "Iteration: 29354; Percent complete: 97.8%; Average loss: 0.1590\n",
            "Iteration: 29355; Percent complete: 97.9%; Average loss: 0.1059\n",
            "Iteration: 29356; Percent complete: 97.9%; Average loss: 0.0922\n",
            "Iteration: 29357; Percent complete: 97.9%; Average loss: 0.0855\n",
            "Iteration: 29358; Percent complete: 97.9%; Average loss: 0.0915\n",
            "Iteration: 29359; Percent complete: 97.9%; Average loss: 0.1445\n",
            "Iteration: 29360; Percent complete: 97.9%; Average loss: 0.1639\n",
            "Iteration: 29361; Percent complete: 97.9%; Average loss: 0.1432\n",
            "Iteration: 29362; Percent complete: 97.9%; Average loss: 0.0852\n",
            "Iteration: 29363; Percent complete: 97.9%; Average loss: 0.0744\n",
            "Iteration: 29364; Percent complete: 97.9%; Average loss: 0.1133\n",
            "Iteration: 29365; Percent complete: 97.9%; Average loss: 0.1405\n",
            "Iteration: 29366; Percent complete: 97.9%; Average loss: 0.0570\n",
            "Iteration: 29367; Percent complete: 97.9%; Average loss: 0.1610\n",
            "Iteration: 29368; Percent complete: 97.9%; Average loss: 0.0795\n",
            "Iteration: 29369; Percent complete: 97.9%; Average loss: 0.0884\n",
            "Iteration: 29370; Percent complete: 97.9%; Average loss: 0.1003\n",
            "Iteration: 29371; Percent complete: 97.9%; Average loss: 0.0722\n",
            "Iteration: 29372; Percent complete: 97.9%; Average loss: 0.1498\n",
            "Iteration: 29373; Percent complete: 97.9%; Average loss: 0.1313\n",
            "Iteration: 29374; Percent complete: 97.9%; Average loss: 0.1106\n",
            "Iteration: 29375; Percent complete: 97.9%; Average loss: 0.1337\n",
            "Iteration: 29376; Percent complete: 97.9%; Average loss: 0.0929\n",
            "Iteration: 29377; Percent complete: 97.9%; Average loss: 0.1209\n",
            "Iteration: 29378; Percent complete: 97.9%; Average loss: 0.1132\n",
            "Iteration: 29379; Percent complete: 97.9%; Average loss: 0.1096\n",
            "Iteration: 29380; Percent complete: 97.9%; Average loss: 0.1601\n",
            "Iteration: 29381; Percent complete: 97.9%; Average loss: 0.1432\n",
            "Iteration: 29382; Percent complete: 97.9%; Average loss: 0.0871\n",
            "Iteration: 29383; Percent complete: 97.9%; Average loss: 0.1363\n",
            "Iteration: 29384; Percent complete: 97.9%; Average loss: 0.1205\n",
            "Iteration: 29385; Percent complete: 98.0%; Average loss: 0.1368\n",
            "Iteration: 29386; Percent complete: 98.0%; Average loss: 0.0752\n",
            "Iteration: 29387; Percent complete: 98.0%; Average loss: 0.0738\n",
            "Iteration: 29388; Percent complete: 98.0%; Average loss: 0.0878\n",
            "Iteration: 29389; Percent complete: 98.0%; Average loss: 0.1366\n",
            "Iteration: 29390; Percent complete: 98.0%; Average loss: 0.1219\n",
            "Iteration: 29391; Percent complete: 98.0%; Average loss: 0.1303\n",
            "Iteration: 29392; Percent complete: 98.0%; Average loss: 0.0732\n",
            "Iteration: 29393; Percent complete: 98.0%; Average loss: 0.1141\n",
            "Iteration: 29394; Percent complete: 98.0%; Average loss: 0.1187\n",
            "Iteration: 29395; Percent complete: 98.0%; Average loss: 0.1487\n",
            "Iteration: 29396; Percent complete: 98.0%; Average loss: 0.1374\n",
            "Iteration: 29397; Percent complete: 98.0%; Average loss: 0.1486\n",
            "Iteration: 29398; Percent complete: 98.0%; Average loss: 0.0850\n",
            "Iteration: 29399; Percent complete: 98.0%; Average loss: 0.1418\n",
            "Iteration: 29400; Percent complete: 98.0%; Average loss: 0.0993\n",
            "Iteration: 29401; Percent complete: 98.0%; Average loss: 0.1159\n",
            "Iteration: 29402; Percent complete: 98.0%; Average loss: 0.0743\n",
            "Iteration: 29403; Percent complete: 98.0%; Average loss: 0.1656\n",
            "Iteration: 29404; Percent complete: 98.0%; Average loss: 0.0834\n",
            "Iteration: 29405; Percent complete: 98.0%; Average loss: 0.0991\n",
            "Iteration: 29406; Percent complete: 98.0%; Average loss: 0.0893\n",
            "Iteration: 29407; Percent complete: 98.0%; Average loss: 0.1147\n",
            "Iteration: 29408; Percent complete: 98.0%; Average loss: 0.1332\n",
            "Iteration: 29409; Percent complete: 98.0%; Average loss: 0.1330\n",
            "Iteration: 29410; Percent complete: 98.0%; Average loss: 0.0916\n",
            "Iteration: 29411; Percent complete: 98.0%; Average loss: 0.1282\n",
            "Iteration: 29412; Percent complete: 98.0%; Average loss: 0.1129\n",
            "Iteration: 29413; Percent complete: 98.0%; Average loss: 0.0958\n",
            "Iteration: 29414; Percent complete: 98.0%; Average loss: 0.1680\n",
            "Iteration: 29415; Percent complete: 98.0%; Average loss: 0.0731\n",
            "Iteration: 29416; Percent complete: 98.1%; Average loss: 0.1255\n",
            "Iteration: 29417; Percent complete: 98.1%; Average loss: 0.0835\n",
            "Iteration: 29418; Percent complete: 98.1%; Average loss: 0.0768\n",
            "Iteration: 29419; Percent complete: 98.1%; Average loss: 0.1265\n",
            "Iteration: 29420; Percent complete: 98.1%; Average loss: 0.1189\n",
            "Iteration: 29421; Percent complete: 98.1%; Average loss: 0.1881\n",
            "Iteration: 29422; Percent complete: 98.1%; Average loss: 0.1157\n",
            "Iteration: 29423; Percent complete: 98.1%; Average loss: 0.1049\n",
            "Iteration: 29424; Percent complete: 98.1%; Average loss: 0.1088\n",
            "Iteration: 29425; Percent complete: 98.1%; Average loss: 0.1046\n",
            "Iteration: 29426; Percent complete: 98.1%; Average loss: 0.1263\n",
            "Iteration: 29427; Percent complete: 98.1%; Average loss: 0.0900\n",
            "Iteration: 29428; Percent complete: 98.1%; Average loss: 0.1622\n",
            "Iteration: 29429; Percent complete: 98.1%; Average loss: 0.1446\n",
            "Iteration: 29430; Percent complete: 98.1%; Average loss: 0.1110\n",
            "Iteration: 29431; Percent complete: 98.1%; Average loss: 0.1517\n",
            "Iteration: 29432; Percent complete: 98.1%; Average loss: 0.0920\n",
            "Iteration: 29433; Percent complete: 98.1%; Average loss: 0.1077\n",
            "Iteration: 29434; Percent complete: 98.1%; Average loss: 0.1109\n",
            "Iteration: 29435; Percent complete: 98.1%; Average loss: 0.1283\n",
            "Iteration: 29436; Percent complete: 98.1%; Average loss: 0.1375\n",
            "Iteration: 29437; Percent complete: 98.1%; Average loss: 0.0701\n",
            "Iteration: 29438; Percent complete: 98.1%; Average loss: 0.1027\n",
            "Iteration: 29439; Percent complete: 98.1%; Average loss: 0.1338\n",
            "Iteration: 29440; Percent complete: 98.1%; Average loss: 0.1131\n",
            "Iteration: 29441; Percent complete: 98.1%; Average loss: 0.1220\n",
            "Iteration: 29442; Percent complete: 98.1%; Average loss: 0.1140\n",
            "Iteration: 29443; Percent complete: 98.1%; Average loss: 0.1352\n",
            "Iteration: 29444; Percent complete: 98.1%; Average loss: 0.0743\n",
            "Iteration: 29445; Percent complete: 98.2%; Average loss: 0.0741\n",
            "Iteration: 29446; Percent complete: 98.2%; Average loss: 0.1415\n",
            "Iteration: 29447; Percent complete: 98.2%; Average loss: 0.0985\n",
            "Iteration: 29448; Percent complete: 98.2%; Average loss: 0.1122\n",
            "Iteration: 29449; Percent complete: 98.2%; Average loss: 0.0785\n",
            "Iteration: 29450; Percent complete: 98.2%; Average loss: 0.1279\n",
            "Iteration: 29451; Percent complete: 98.2%; Average loss: 0.1549\n",
            "Iteration: 29452; Percent complete: 98.2%; Average loss: 0.1280\n",
            "Iteration: 29453; Percent complete: 98.2%; Average loss: 0.1120\n",
            "Iteration: 29454; Percent complete: 98.2%; Average loss: 0.1209\n",
            "Iteration: 29455; Percent complete: 98.2%; Average loss: 0.1161\n",
            "Iteration: 29456; Percent complete: 98.2%; Average loss: 0.1421\n",
            "Iteration: 29457; Percent complete: 98.2%; Average loss: 0.1528\n",
            "Iteration: 29458; Percent complete: 98.2%; Average loss: 0.1491\n",
            "Iteration: 29459; Percent complete: 98.2%; Average loss: 0.0975\n",
            "Iteration: 29460; Percent complete: 98.2%; Average loss: 0.1288\n",
            "Iteration: 29461; Percent complete: 98.2%; Average loss: 0.1360\n",
            "Iteration: 29462; Percent complete: 98.2%; Average loss: 0.1134\n",
            "Iteration: 29463; Percent complete: 98.2%; Average loss: 0.0950\n",
            "Iteration: 29464; Percent complete: 98.2%; Average loss: 0.0702\n",
            "Iteration: 29465; Percent complete: 98.2%; Average loss: 0.1672\n",
            "Iteration: 29466; Percent complete: 98.2%; Average loss: 0.1800\n",
            "Iteration: 29467; Percent complete: 98.2%; Average loss: 0.1138\n",
            "Iteration: 29468; Percent complete: 98.2%; Average loss: 0.0849\n",
            "Iteration: 29469; Percent complete: 98.2%; Average loss: 0.1861\n",
            "Iteration: 29470; Percent complete: 98.2%; Average loss: 0.0736\n",
            "Iteration: 29471; Percent complete: 98.2%; Average loss: 0.1347\n",
            "Iteration: 29472; Percent complete: 98.2%; Average loss: 0.1249\n",
            "Iteration: 29473; Percent complete: 98.2%; Average loss: 0.1320\n",
            "Iteration: 29474; Percent complete: 98.2%; Average loss: 0.1318\n",
            "Iteration: 29475; Percent complete: 98.2%; Average loss: 0.1525\n",
            "Iteration: 29476; Percent complete: 98.3%; Average loss: 0.1048\n",
            "Iteration: 29477; Percent complete: 98.3%; Average loss: 0.1207\n",
            "Iteration: 29478; Percent complete: 98.3%; Average loss: 0.1187\n",
            "Iteration: 29479; Percent complete: 98.3%; Average loss: 0.1302\n",
            "Iteration: 29480; Percent complete: 98.3%; Average loss: 0.1166\n",
            "Iteration: 29481; Percent complete: 98.3%; Average loss: 0.1150\n",
            "Iteration: 29482; Percent complete: 98.3%; Average loss: 0.1241\n",
            "Iteration: 29483; Percent complete: 98.3%; Average loss: 0.1541\n",
            "Iteration: 29484; Percent complete: 98.3%; Average loss: 0.1259\n",
            "Iteration: 29485; Percent complete: 98.3%; Average loss: 0.1536\n",
            "Iteration: 29486; Percent complete: 98.3%; Average loss: 0.1037\n",
            "Iteration: 29487; Percent complete: 98.3%; Average loss: 0.1527\n",
            "Iteration: 29488; Percent complete: 98.3%; Average loss: 0.1284\n",
            "Iteration: 29489; Percent complete: 98.3%; Average loss: 0.1173\n",
            "Iteration: 29490; Percent complete: 98.3%; Average loss: 0.1272\n",
            "Iteration: 29491; Percent complete: 98.3%; Average loss: 0.1099\n",
            "Iteration: 29492; Percent complete: 98.3%; Average loss: 0.1101\n",
            "Iteration: 29493; Percent complete: 98.3%; Average loss: 0.1091\n",
            "Iteration: 29494; Percent complete: 98.3%; Average loss: 0.1748\n",
            "Iteration: 29495; Percent complete: 98.3%; Average loss: 0.1747\n",
            "Iteration: 29496; Percent complete: 98.3%; Average loss: 0.0821\n",
            "Iteration: 29497; Percent complete: 98.3%; Average loss: 0.0735\n",
            "Iteration: 29498; Percent complete: 98.3%; Average loss: 0.0837\n",
            "Iteration: 29499; Percent complete: 98.3%; Average loss: 0.1005\n",
            "Iteration: 29500; Percent complete: 98.3%; Average loss: 0.1506\n",
            "Iteration: 29501; Percent complete: 98.3%; Average loss: 0.1450\n",
            "Iteration: 29502; Percent complete: 98.3%; Average loss: 0.1273\n",
            "Iteration: 29503; Percent complete: 98.3%; Average loss: 0.1520\n",
            "Iteration: 29504; Percent complete: 98.3%; Average loss: 0.1628\n",
            "Iteration: 29505; Percent complete: 98.4%; Average loss: 0.1622\n",
            "Iteration: 29506; Percent complete: 98.4%; Average loss: 0.1213\n",
            "Iteration: 29507; Percent complete: 98.4%; Average loss: 0.1429\n",
            "Iteration: 29508; Percent complete: 98.4%; Average loss: 0.0993\n",
            "Iteration: 29509; Percent complete: 98.4%; Average loss: 0.1671\n",
            "Iteration: 29510; Percent complete: 98.4%; Average loss: 0.1442\n",
            "Iteration: 29511; Percent complete: 98.4%; Average loss: 0.1149\n",
            "Iteration: 29512; Percent complete: 98.4%; Average loss: 0.1417\n",
            "Iteration: 29513; Percent complete: 98.4%; Average loss: 0.1144\n",
            "Iteration: 29514; Percent complete: 98.4%; Average loss: 0.0995\n",
            "Iteration: 29515; Percent complete: 98.4%; Average loss: 0.1422\n",
            "Iteration: 29516; Percent complete: 98.4%; Average loss: 0.1516\n",
            "Iteration: 29517; Percent complete: 98.4%; Average loss: 0.0933\n",
            "Iteration: 29518; Percent complete: 98.4%; Average loss: 0.0684\n",
            "Iteration: 29519; Percent complete: 98.4%; Average loss: 0.1181\n",
            "Iteration: 29520; Percent complete: 98.4%; Average loss: 0.1079\n",
            "Iteration: 29521; Percent complete: 98.4%; Average loss: 0.1656\n",
            "Iteration: 29522; Percent complete: 98.4%; Average loss: 0.1312\n",
            "Iteration: 29523; Percent complete: 98.4%; Average loss: 0.1320\n",
            "Iteration: 29524; Percent complete: 98.4%; Average loss: 0.1029\n",
            "Iteration: 29525; Percent complete: 98.4%; Average loss: 0.0946\n",
            "Iteration: 29526; Percent complete: 98.4%; Average loss: 0.1937\n",
            "Iteration: 29527; Percent complete: 98.4%; Average loss: 0.1202\n",
            "Iteration: 29528; Percent complete: 98.4%; Average loss: 0.1054\n",
            "Iteration: 29529; Percent complete: 98.4%; Average loss: 0.1283\n",
            "Iteration: 29530; Percent complete: 98.4%; Average loss: 0.1147\n",
            "Iteration: 29531; Percent complete: 98.4%; Average loss: 0.1372\n",
            "Iteration: 29532; Percent complete: 98.4%; Average loss: 0.0960\n",
            "Iteration: 29533; Percent complete: 98.4%; Average loss: 0.0905\n",
            "Iteration: 29534; Percent complete: 98.4%; Average loss: 0.1413\n",
            "Iteration: 29535; Percent complete: 98.5%; Average loss: 0.1731\n",
            "Iteration: 29536; Percent complete: 98.5%; Average loss: 0.1579\n",
            "Iteration: 29537; Percent complete: 98.5%; Average loss: 0.1026\n",
            "Iteration: 29538; Percent complete: 98.5%; Average loss: 0.1208\n",
            "Iteration: 29539; Percent complete: 98.5%; Average loss: 0.1136\n",
            "Iteration: 29540; Percent complete: 98.5%; Average loss: 0.1110\n",
            "Iteration: 29541; Percent complete: 98.5%; Average loss: 0.1860\n",
            "Iteration: 29542; Percent complete: 98.5%; Average loss: 0.0618\n",
            "Iteration: 29543; Percent complete: 98.5%; Average loss: 0.1168\n",
            "Iteration: 29544; Percent complete: 98.5%; Average loss: 0.1798\n",
            "Iteration: 29545; Percent complete: 98.5%; Average loss: 0.1101\n",
            "Iteration: 29546; Percent complete: 98.5%; Average loss: 0.1056\n",
            "Iteration: 29547; Percent complete: 98.5%; Average loss: 0.1084\n",
            "Iteration: 29548; Percent complete: 98.5%; Average loss: 0.1509\n",
            "Iteration: 29549; Percent complete: 98.5%; Average loss: 0.1876\n",
            "Iteration: 29550; Percent complete: 98.5%; Average loss: 0.1148\n",
            "Iteration: 29551; Percent complete: 98.5%; Average loss: 0.1357\n",
            "Iteration: 29552; Percent complete: 98.5%; Average loss: 0.0830\n",
            "Iteration: 29553; Percent complete: 98.5%; Average loss: 0.1425\n",
            "Iteration: 29554; Percent complete: 98.5%; Average loss: 0.1226\n",
            "Iteration: 29555; Percent complete: 98.5%; Average loss: 0.1279\n",
            "Iteration: 29556; Percent complete: 98.5%; Average loss: 0.0854\n",
            "Iteration: 29557; Percent complete: 98.5%; Average loss: 0.0835\n",
            "Iteration: 29558; Percent complete: 98.5%; Average loss: 0.1511\n",
            "Iteration: 29559; Percent complete: 98.5%; Average loss: 0.1570\n",
            "Iteration: 29560; Percent complete: 98.5%; Average loss: 0.1032\n",
            "Iteration: 29561; Percent complete: 98.5%; Average loss: 0.1523\n",
            "Iteration: 29562; Percent complete: 98.5%; Average loss: 0.1515\n",
            "Iteration: 29563; Percent complete: 98.5%; Average loss: 0.0781\n",
            "Iteration: 29564; Percent complete: 98.5%; Average loss: 0.1210\n",
            "Iteration: 29565; Percent complete: 98.6%; Average loss: 0.1104\n",
            "Iteration: 29566; Percent complete: 98.6%; Average loss: 0.1118\n",
            "Iteration: 29567; Percent complete: 98.6%; Average loss: 0.0876\n",
            "Iteration: 29568; Percent complete: 98.6%; Average loss: 0.1296\n",
            "Iteration: 29569; Percent complete: 98.6%; Average loss: 0.1450\n",
            "Iteration: 29570; Percent complete: 98.6%; Average loss: 0.1382\n",
            "Iteration: 29571; Percent complete: 98.6%; Average loss: 0.1186\n",
            "Iteration: 29572; Percent complete: 98.6%; Average loss: 0.0940\n",
            "Iteration: 29573; Percent complete: 98.6%; Average loss: 0.1461\n",
            "Iteration: 29574; Percent complete: 98.6%; Average loss: 0.1043\n",
            "Iteration: 29575; Percent complete: 98.6%; Average loss: 0.1044\n",
            "Iteration: 29576; Percent complete: 98.6%; Average loss: 0.1325\n",
            "Iteration: 29577; Percent complete: 98.6%; Average loss: 0.1041\n",
            "Iteration: 29578; Percent complete: 98.6%; Average loss: 0.0958\n",
            "Iteration: 29579; Percent complete: 98.6%; Average loss: 0.1272\n",
            "Iteration: 29580; Percent complete: 98.6%; Average loss: 0.1386\n",
            "Iteration: 29581; Percent complete: 98.6%; Average loss: 0.0954\n",
            "Iteration: 29582; Percent complete: 98.6%; Average loss: 0.1201\n",
            "Iteration: 29583; Percent complete: 98.6%; Average loss: 0.0981\n",
            "Iteration: 29584; Percent complete: 98.6%; Average loss: 0.1345\n",
            "Iteration: 29585; Percent complete: 98.6%; Average loss: 0.1444\n",
            "Iteration: 29586; Percent complete: 98.6%; Average loss: 0.1165\n",
            "Iteration: 29587; Percent complete: 98.6%; Average loss: 0.1434\n",
            "Iteration: 29588; Percent complete: 98.6%; Average loss: 0.1192\n",
            "Iteration: 29589; Percent complete: 98.6%; Average loss: 0.0880\n",
            "Iteration: 29590; Percent complete: 98.6%; Average loss: 0.1418\n",
            "Iteration: 29591; Percent complete: 98.6%; Average loss: 0.1128\n",
            "Iteration: 29592; Percent complete: 98.6%; Average loss: 0.0757\n",
            "Iteration: 29593; Percent complete: 98.6%; Average loss: 0.1437\n",
            "Iteration: 29594; Percent complete: 98.6%; Average loss: 0.0914\n",
            "Iteration: 29595; Percent complete: 98.7%; Average loss: 0.1419\n",
            "Iteration: 29596; Percent complete: 98.7%; Average loss: 0.1429\n",
            "Iteration: 29597; Percent complete: 98.7%; Average loss: 0.1252\n",
            "Iteration: 29598; Percent complete: 98.7%; Average loss: 0.1436\n",
            "Iteration: 29599; Percent complete: 98.7%; Average loss: 0.1575\n",
            "Iteration: 29600; Percent complete: 98.7%; Average loss: 0.1310\n",
            "Iteration: 29601; Percent complete: 98.7%; Average loss: 0.1064\n",
            "Iteration: 29602; Percent complete: 98.7%; Average loss: 0.1496\n",
            "Iteration: 29603; Percent complete: 98.7%; Average loss: 0.0708\n",
            "Iteration: 29604; Percent complete: 98.7%; Average loss: 0.1494\n",
            "Iteration: 29605; Percent complete: 98.7%; Average loss: 0.0907\n",
            "Iteration: 29606; Percent complete: 98.7%; Average loss: 0.1152\n",
            "Iteration: 29607; Percent complete: 98.7%; Average loss: 0.1130\n",
            "Iteration: 29608; Percent complete: 98.7%; Average loss: 0.1216\n",
            "Iteration: 29609; Percent complete: 98.7%; Average loss: 0.1295\n",
            "Iteration: 29610; Percent complete: 98.7%; Average loss: 0.1117\n",
            "Iteration: 29611; Percent complete: 98.7%; Average loss: 0.0737\n",
            "Iteration: 29612; Percent complete: 98.7%; Average loss: 0.0708\n",
            "Iteration: 29613; Percent complete: 98.7%; Average loss: 0.1213\n",
            "Iteration: 29614; Percent complete: 98.7%; Average loss: 0.1162\n",
            "Iteration: 29615; Percent complete: 98.7%; Average loss: 0.1016\n",
            "Iteration: 29616; Percent complete: 98.7%; Average loss: 0.1202\n",
            "Iteration: 29617; Percent complete: 98.7%; Average loss: 0.1518\n",
            "Iteration: 29618; Percent complete: 98.7%; Average loss: 0.0982\n",
            "Iteration: 29619; Percent complete: 98.7%; Average loss: 0.1078\n",
            "Iteration: 29620; Percent complete: 98.7%; Average loss: 0.1354\n",
            "Iteration: 29621; Percent complete: 98.7%; Average loss: 0.0887\n",
            "Iteration: 29622; Percent complete: 98.7%; Average loss: 0.1131\n",
            "Iteration: 29623; Percent complete: 98.7%; Average loss: 0.1237\n",
            "Iteration: 29624; Percent complete: 98.7%; Average loss: 0.1257\n",
            "Iteration: 29625; Percent complete: 98.8%; Average loss: 0.1696\n",
            "Iteration: 29626; Percent complete: 98.8%; Average loss: 0.1260\n",
            "Iteration: 29627; Percent complete: 98.8%; Average loss: 0.1249\n",
            "Iteration: 29628; Percent complete: 98.8%; Average loss: 0.1432\n",
            "Iteration: 29629; Percent complete: 98.8%; Average loss: 0.0952\n",
            "Iteration: 29630; Percent complete: 98.8%; Average loss: 0.1016\n",
            "Iteration: 29631; Percent complete: 98.8%; Average loss: 0.1572\n",
            "Iteration: 29632; Percent complete: 98.8%; Average loss: 0.0885\n",
            "Iteration: 29633; Percent complete: 98.8%; Average loss: 0.1081\n",
            "Iteration: 29634; Percent complete: 98.8%; Average loss: 0.1510\n",
            "Iteration: 29635; Percent complete: 98.8%; Average loss: 0.1119\n",
            "Iteration: 29636; Percent complete: 98.8%; Average loss: 0.0911\n",
            "Iteration: 29637; Percent complete: 98.8%; Average loss: 0.1339\n",
            "Iteration: 29638; Percent complete: 98.8%; Average loss: 0.0873\n",
            "Iteration: 29639; Percent complete: 98.8%; Average loss: 0.1260\n",
            "Iteration: 29640; Percent complete: 98.8%; Average loss: 0.1616\n",
            "Iteration: 29641; Percent complete: 98.8%; Average loss: 0.0985\n",
            "Iteration: 29642; Percent complete: 98.8%; Average loss: 0.1282\n",
            "Iteration: 29643; Percent complete: 98.8%; Average loss: 0.1342\n",
            "Iteration: 29644; Percent complete: 98.8%; Average loss: 0.1484\n",
            "Iteration: 29645; Percent complete: 98.8%; Average loss: 0.0677\n",
            "Iteration: 29646; Percent complete: 98.8%; Average loss: 0.1567\n",
            "Iteration: 29647; Percent complete: 98.8%; Average loss: 0.0945\n",
            "Iteration: 29648; Percent complete: 98.8%; Average loss: 0.1083\n",
            "Iteration: 29649; Percent complete: 98.8%; Average loss: 0.1049\n",
            "Iteration: 29650; Percent complete: 98.8%; Average loss: 0.1105\n",
            "Iteration: 29651; Percent complete: 98.8%; Average loss: 0.0804\n",
            "Iteration: 29652; Percent complete: 98.8%; Average loss: 0.1040\n",
            "Iteration: 29653; Percent complete: 98.8%; Average loss: 0.1159\n",
            "Iteration: 29654; Percent complete: 98.8%; Average loss: 0.0984\n",
            "Iteration: 29655; Percent complete: 98.9%; Average loss: 0.1090\n",
            "Iteration: 29656; Percent complete: 98.9%; Average loss: 0.0836\n",
            "Iteration: 29657; Percent complete: 98.9%; Average loss: 0.1215\n",
            "Iteration: 29658; Percent complete: 98.9%; Average loss: 0.0963\n",
            "Iteration: 29659; Percent complete: 98.9%; Average loss: 0.1440\n",
            "Iteration: 29660; Percent complete: 98.9%; Average loss: 0.1387\n",
            "Iteration: 29661; Percent complete: 98.9%; Average loss: 0.0831\n",
            "Iteration: 29662; Percent complete: 98.9%; Average loss: 0.1076\n",
            "Iteration: 29663; Percent complete: 98.9%; Average loss: 0.1104\n",
            "Iteration: 29664; Percent complete: 98.9%; Average loss: 0.1135\n",
            "Iteration: 29665; Percent complete: 98.9%; Average loss: 0.1182\n",
            "Iteration: 29666; Percent complete: 98.9%; Average loss: 0.1504\n",
            "Iteration: 29667; Percent complete: 98.9%; Average loss: 0.1134\n",
            "Iteration: 29668; Percent complete: 98.9%; Average loss: 0.0724\n",
            "Iteration: 29669; Percent complete: 98.9%; Average loss: 0.1338\n",
            "Iteration: 29670; Percent complete: 98.9%; Average loss: 0.1490\n",
            "Iteration: 29671; Percent complete: 98.9%; Average loss: 0.1181\n",
            "Iteration: 29672; Percent complete: 98.9%; Average loss: 0.1151\n",
            "Iteration: 29673; Percent complete: 98.9%; Average loss: 0.1709\n",
            "Iteration: 29674; Percent complete: 98.9%; Average loss: 0.1255\n",
            "Iteration: 29675; Percent complete: 98.9%; Average loss: 0.1010\n",
            "Iteration: 29676; Percent complete: 98.9%; Average loss: 0.0921\n",
            "Iteration: 29677; Percent complete: 98.9%; Average loss: 0.1146\n",
            "Iteration: 29678; Percent complete: 98.9%; Average loss: 0.1048\n",
            "Iteration: 29679; Percent complete: 98.9%; Average loss: 0.0993\n",
            "Iteration: 29680; Percent complete: 98.9%; Average loss: 0.1432\n",
            "Iteration: 29681; Percent complete: 98.9%; Average loss: 0.1079\n",
            "Iteration: 29682; Percent complete: 98.9%; Average loss: 0.1077\n",
            "Iteration: 29683; Percent complete: 98.9%; Average loss: 0.1277\n",
            "Iteration: 29684; Percent complete: 98.9%; Average loss: 0.1470\n",
            "Iteration: 29685; Percent complete: 99.0%; Average loss: 0.1129\n",
            "Iteration: 29686; Percent complete: 99.0%; Average loss: 0.1161\n",
            "Iteration: 29687; Percent complete: 99.0%; Average loss: 0.1178\n",
            "Iteration: 29688; Percent complete: 99.0%; Average loss: 0.1003\n",
            "Iteration: 29689; Percent complete: 99.0%; Average loss: 0.1021\n",
            "Iteration: 29690; Percent complete: 99.0%; Average loss: 0.1504\n",
            "Iteration: 29691; Percent complete: 99.0%; Average loss: 0.1243\n",
            "Iteration: 29692; Percent complete: 99.0%; Average loss: 0.1174\n",
            "Iteration: 29693; Percent complete: 99.0%; Average loss: 0.0772\n",
            "Iteration: 29694; Percent complete: 99.0%; Average loss: 0.0783\n",
            "Iteration: 29695; Percent complete: 99.0%; Average loss: 0.1394\n",
            "Iteration: 29696; Percent complete: 99.0%; Average loss: 0.1079\n",
            "Iteration: 29697; Percent complete: 99.0%; Average loss: 0.1000\n",
            "Iteration: 29698; Percent complete: 99.0%; Average loss: 0.1349\n",
            "Iteration: 29699; Percent complete: 99.0%; Average loss: 0.1175\n",
            "Iteration: 29700; Percent complete: 99.0%; Average loss: 0.1525\n",
            "Iteration: 29701; Percent complete: 99.0%; Average loss: 0.1408\n",
            "Iteration: 29702; Percent complete: 99.0%; Average loss: 0.1396\n",
            "Iteration: 29703; Percent complete: 99.0%; Average loss: 0.0870\n",
            "Iteration: 29704; Percent complete: 99.0%; Average loss: 0.0877\n",
            "Iteration: 29705; Percent complete: 99.0%; Average loss: 0.1367\n",
            "Iteration: 29706; Percent complete: 99.0%; Average loss: 0.1489\n",
            "Iteration: 29707; Percent complete: 99.0%; Average loss: 0.1273\n",
            "Iteration: 29708; Percent complete: 99.0%; Average loss: 0.1056\n",
            "Iteration: 29709; Percent complete: 99.0%; Average loss: 0.1391\n",
            "Iteration: 29710; Percent complete: 99.0%; Average loss: 0.1590\n",
            "Iteration: 29711; Percent complete: 99.0%; Average loss: 0.1247\n",
            "Iteration: 29712; Percent complete: 99.0%; Average loss: 0.1377\n",
            "Iteration: 29713; Percent complete: 99.0%; Average loss: 0.1389\n",
            "Iteration: 29714; Percent complete: 99.0%; Average loss: 0.1406\n",
            "Iteration: 29715; Percent complete: 99.1%; Average loss: 0.1198\n",
            "Iteration: 29716; Percent complete: 99.1%; Average loss: 0.1056\n",
            "Iteration: 29717; Percent complete: 99.1%; Average loss: 0.0907\n",
            "Iteration: 29718; Percent complete: 99.1%; Average loss: 0.1389\n",
            "Iteration: 29719; Percent complete: 99.1%; Average loss: 0.1046\n",
            "Iteration: 29720; Percent complete: 99.1%; Average loss: 0.1197\n",
            "Iteration: 29721; Percent complete: 99.1%; Average loss: 0.0949\n",
            "Iteration: 29722; Percent complete: 99.1%; Average loss: 0.1027\n",
            "Iteration: 29723; Percent complete: 99.1%; Average loss: 0.1036\n",
            "Iteration: 29724; Percent complete: 99.1%; Average loss: 0.1385\n",
            "Iteration: 29725; Percent complete: 99.1%; Average loss: 0.1353\n",
            "Iteration: 29726; Percent complete: 99.1%; Average loss: 0.0897\n",
            "Iteration: 29727; Percent complete: 99.1%; Average loss: 0.0976\n",
            "Iteration: 29728; Percent complete: 99.1%; Average loss: 0.1449\n",
            "Iteration: 29729; Percent complete: 99.1%; Average loss: 0.1061\n",
            "Iteration: 29730; Percent complete: 99.1%; Average loss: 0.1284\n",
            "Iteration: 29731; Percent complete: 99.1%; Average loss: 0.0844\n",
            "Iteration: 29732; Percent complete: 99.1%; Average loss: 0.1753\n",
            "Iteration: 29733; Percent complete: 99.1%; Average loss: 0.1075\n",
            "Iteration: 29734; Percent complete: 99.1%; Average loss: 0.0979\n",
            "Iteration: 29735; Percent complete: 99.1%; Average loss: 0.1351\n",
            "Iteration: 29736; Percent complete: 99.1%; Average loss: 0.0845\n",
            "Iteration: 29737; Percent complete: 99.1%; Average loss: 0.1311\n",
            "Iteration: 29738; Percent complete: 99.1%; Average loss: 0.1555\n",
            "Iteration: 29739; Percent complete: 99.1%; Average loss: 0.1331\n",
            "Iteration: 29740; Percent complete: 99.1%; Average loss: 0.1404\n",
            "Iteration: 29741; Percent complete: 99.1%; Average loss: 0.1113\n",
            "Iteration: 29742; Percent complete: 99.1%; Average loss: 0.0987\n",
            "Iteration: 29743; Percent complete: 99.1%; Average loss: 0.0664\n",
            "Iteration: 29744; Percent complete: 99.1%; Average loss: 0.1104\n",
            "Iteration: 29745; Percent complete: 99.2%; Average loss: 0.1473\n",
            "Iteration: 29746; Percent complete: 99.2%; Average loss: 0.1208\n",
            "Iteration: 29747; Percent complete: 99.2%; Average loss: 0.1861\n",
            "Iteration: 29748; Percent complete: 99.2%; Average loss: 0.1168\n",
            "Iteration: 29749; Percent complete: 99.2%; Average loss: 0.1570\n",
            "Iteration: 29750; Percent complete: 99.2%; Average loss: 0.0947\n",
            "Iteration: 29751; Percent complete: 99.2%; Average loss: 0.1065\n",
            "Iteration: 29752; Percent complete: 99.2%; Average loss: 0.1408\n",
            "Iteration: 29753; Percent complete: 99.2%; Average loss: 0.0699\n",
            "Iteration: 29754; Percent complete: 99.2%; Average loss: 0.0778\n",
            "Iteration: 29755; Percent complete: 99.2%; Average loss: 0.1204\n",
            "Iteration: 29756; Percent complete: 99.2%; Average loss: 0.1224\n",
            "Iteration: 29757; Percent complete: 99.2%; Average loss: 0.1167\n",
            "Iteration: 29758; Percent complete: 99.2%; Average loss: 0.1145\n",
            "Iteration: 29759; Percent complete: 99.2%; Average loss: 0.1457\n",
            "Iteration: 29760; Percent complete: 99.2%; Average loss: 0.1213\n",
            "Iteration: 29761; Percent complete: 99.2%; Average loss: 0.1762\n",
            "Iteration: 29762; Percent complete: 99.2%; Average loss: 0.1335\n",
            "Iteration: 29763; Percent complete: 99.2%; Average loss: 0.1236\n",
            "Iteration: 29764; Percent complete: 99.2%; Average loss: 0.1245\n",
            "Iteration: 29765; Percent complete: 99.2%; Average loss: 0.1043\n",
            "Iteration: 29766; Percent complete: 99.2%; Average loss: 0.1228\n",
            "Iteration: 29767; Percent complete: 99.2%; Average loss: 0.1281\n",
            "Iteration: 29768; Percent complete: 99.2%; Average loss: 0.1167\n",
            "Iteration: 29769; Percent complete: 99.2%; Average loss: 0.1125\n",
            "Iteration: 29770; Percent complete: 99.2%; Average loss: 0.1163\n",
            "Iteration: 29771; Percent complete: 99.2%; Average loss: 0.1902\n",
            "Iteration: 29772; Percent complete: 99.2%; Average loss: 0.1615\n",
            "Iteration: 29773; Percent complete: 99.2%; Average loss: 0.1540\n",
            "Iteration: 29774; Percent complete: 99.2%; Average loss: 0.1073\n",
            "Iteration: 29775; Percent complete: 99.2%; Average loss: 0.1251\n",
            "Iteration: 29776; Percent complete: 99.3%; Average loss: 0.1228\n",
            "Iteration: 29777; Percent complete: 99.3%; Average loss: 0.1221\n",
            "Iteration: 29778; Percent complete: 99.3%; Average loss: 0.1023\n",
            "Iteration: 29779; Percent complete: 99.3%; Average loss: 0.0782\n",
            "Iteration: 29780; Percent complete: 99.3%; Average loss: 0.1241\n",
            "Iteration: 29781; Percent complete: 99.3%; Average loss: 0.1068\n",
            "Iteration: 29782; Percent complete: 99.3%; Average loss: 0.1158\n",
            "Iteration: 29783; Percent complete: 99.3%; Average loss: 0.1215\n",
            "Iteration: 29784; Percent complete: 99.3%; Average loss: 0.1156\n",
            "Iteration: 29785; Percent complete: 99.3%; Average loss: 0.0787\n",
            "Iteration: 29786; Percent complete: 99.3%; Average loss: 0.1687\n",
            "Iteration: 29787; Percent complete: 99.3%; Average loss: 0.1206\n",
            "Iteration: 29788; Percent complete: 99.3%; Average loss: 0.1023\n",
            "Iteration: 29789; Percent complete: 99.3%; Average loss: 0.1367\n",
            "Iteration: 29790; Percent complete: 99.3%; Average loss: 0.1466\n",
            "Iteration: 29791; Percent complete: 99.3%; Average loss: 0.1109\n",
            "Iteration: 29792; Percent complete: 99.3%; Average loss: 0.1247\n",
            "Iteration: 29793; Percent complete: 99.3%; Average loss: 0.1135\n",
            "Iteration: 29794; Percent complete: 99.3%; Average loss: 0.0941\n",
            "Iteration: 29795; Percent complete: 99.3%; Average loss: 0.1559\n",
            "Iteration: 29796; Percent complete: 99.3%; Average loss: 0.0908\n",
            "Iteration: 29797; Percent complete: 99.3%; Average loss: 0.1011\n",
            "Iteration: 29798; Percent complete: 99.3%; Average loss: 0.1146\n",
            "Iteration: 29799; Percent complete: 99.3%; Average loss: 0.0992\n",
            "Iteration: 29800; Percent complete: 99.3%; Average loss: 0.1564\n",
            "Iteration: 29801; Percent complete: 99.3%; Average loss: 0.1112\n",
            "Iteration: 29802; Percent complete: 99.3%; Average loss: 0.1032\n",
            "Iteration: 29803; Percent complete: 99.3%; Average loss: 0.1757\n",
            "Iteration: 29804; Percent complete: 99.3%; Average loss: 0.1404\n",
            "Iteration: 29805; Percent complete: 99.4%; Average loss: 0.1418\n",
            "Iteration: 29806; Percent complete: 99.4%; Average loss: 0.1244\n",
            "Iteration: 29807; Percent complete: 99.4%; Average loss: 0.1325\n",
            "Iteration: 29808; Percent complete: 99.4%; Average loss: 0.1419\n",
            "Iteration: 29809; Percent complete: 99.4%; Average loss: 0.1247\n",
            "Iteration: 29810; Percent complete: 99.4%; Average loss: 0.1021\n",
            "Iteration: 29811; Percent complete: 99.4%; Average loss: 0.0911\n",
            "Iteration: 29812; Percent complete: 99.4%; Average loss: 0.1372\n",
            "Iteration: 29813; Percent complete: 99.4%; Average loss: 0.1258\n",
            "Iteration: 29814; Percent complete: 99.4%; Average loss: 0.0843\n",
            "Iteration: 29815; Percent complete: 99.4%; Average loss: 0.1690\n",
            "Iteration: 29816; Percent complete: 99.4%; Average loss: 0.1234\n",
            "Iteration: 29817; Percent complete: 99.4%; Average loss: 0.1231\n",
            "Iteration: 29818; Percent complete: 99.4%; Average loss: 0.0868\n",
            "Iteration: 29819; Percent complete: 99.4%; Average loss: 0.1310\n",
            "Iteration: 29820; Percent complete: 99.4%; Average loss: 0.1003\n",
            "Iteration: 29821; Percent complete: 99.4%; Average loss: 0.1243\n",
            "Iteration: 29822; Percent complete: 99.4%; Average loss: 0.1309\n",
            "Iteration: 29823; Percent complete: 99.4%; Average loss: 0.1772\n",
            "Iteration: 29824; Percent complete: 99.4%; Average loss: 0.1221\n",
            "Iteration: 29825; Percent complete: 99.4%; Average loss: 0.1108\n",
            "Iteration: 29826; Percent complete: 99.4%; Average loss: 0.0806\n",
            "Iteration: 29827; Percent complete: 99.4%; Average loss: 0.1491\n",
            "Iteration: 29828; Percent complete: 99.4%; Average loss: 0.1137\n",
            "Iteration: 29829; Percent complete: 99.4%; Average loss: 0.0617\n",
            "Iteration: 29830; Percent complete: 99.4%; Average loss: 0.1479\n",
            "Iteration: 29831; Percent complete: 99.4%; Average loss: 0.1059\n",
            "Iteration: 29832; Percent complete: 99.4%; Average loss: 0.1096\n",
            "Iteration: 29833; Percent complete: 99.4%; Average loss: 0.1630\n",
            "Iteration: 29834; Percent complete: 99.4%; Average loss: 0.1538\n",
            "Iteration: 29835; Percent complete: 99.5%; Average loss: 0.0973\n",
            "Iteration: 29836; Percent complete: 99.5%; Average loss: 0.1046\n",
            "Iteration: 29837; Percent complete: 99.5%; Average loss: 0.1230\n",
            "Iteration: 29838; Percent complete: 99.5%; Average loss: 0.1156\n",
            "Iteration: 29839; Percent complete: 99.5%; Average loss: 0.1144\n",
            "Iteration: 29840; Percent complete: 99.5%; Average loss: 0.1462\n",
            "Iteration: 29841; Percent complete: 99.5%; Average loss: 0.1072\n",
            "Iteration: 29842; Percent complete: 99.5%; Average loss: 0.1481\n",
            "Iteration: 29843; Percent complete: 99.5%; Average loss: 0.1268\n",
            "Iteration: 29844; Percent complete: 99.5%; Average loss: 0.0952\n",
            "Iteration: 29845; Percent complete: 99.5%; Average loss: 0.0595\n",
            "Iteration: 29846; Percent complete: 99.5%; Average loss: 0.1103\n",
            "Iteration: 29847; Percent complete: 99.5%; Average loss: 0.1407\n",
            "Iteration: 29848; Percent complete: 99.5%; Average loss: 0.0961\n",
            "Iteration: 29849; Percent complete: 99.5%; Average loss: 0.1078\n",
            "Iteration: 29850; Percent complete: 99.5%; Average loss: 0.1413\n",
            "Iteration: 29851; Percent complete: 99.5%; Average loss: 0.1745\n",
            "Iteration: 29852; Percent complete: 99.5%; Average loss: 0.1395\n",
            "Iteration: 29853; Percent complete: 99.5%; Average loss: 0.1092\n",
            "Iteration: 29854; Percent complete: 99.5%; Average loss: 0.1172\n",
            "Iteration: 29855; Percent complete: 99.5%; Average loss: 0.1165\n",
            "Iteration: 29856; Percent complete: 99.5%; Average loss: 0.1115\n",
            "Iteration: 29857; Percent complete: 99.5%; Average loss: 0.1354\n",
            "Iteration: 29858; Percent complete: 99.5%; Average loss: 0.1523\n",
            "Iteration: 29859; Percent complete: 99.5%; Average loss: 0.0975\n",
            "Iteration: 29860; Percent complete: 99.5%; Average loss: 0.1058\n",
            "Iteration: 29861; Percent complete: 99.5%; Average loss: 0.1411\n",
            "Iteration: 29862; Percent complete: 99.5%; Average loss: 0.1502\n",
            "Iteration: 29863; Percent complete: 99.5%; Average loss: 0.1366\n",
            "Iteration: 29864; Percent complete: 99.5%; Average loss: 0.1115\n",
            "Iteration: 29865; Percent complete: 99.6%; Average loss: 0.1082\n",
            "Iteration: 29866; Percent complete: 99.6%; Average loss: 0.1508\n",
            "Iteration: 29867; Percent complete: 99.6%; Average loss: 0.1012\n",
            "Iteration: 29868; Percent complete: 99.6%; Average loss: 0.1077\n",
            "Iteration: 29869; Percent complete: 99.6%; Average loss: 0.1192\n",
            "Iteration: 29870; Percent complete: 99.6%; Average loss: 0.0854\n",
            "Iteration: 29871; Percent complete: 99.6%; Average loss: 0.1417\n",
            "Iteration: 29872; Percent complete: 99.6%; Average loss: 0.1156\n",
            "Iteration: 29873; Percent complete: 99.6%; Average loss: 0.0948\n",
            "Iteration: 29874; Percent complete: 99.6%; Average loss: 0.1320\n",
            "Iteration: 29875; Percent complete: 99.6%; Average loss: 0.1002\n",
            "Iteration: 29876; Percent complete: 99.6%; Average loss: 0.1235\n",
            "Iteration: 29877; Percent complete: 99.6%; Average loss: 0.1591\n",
            "Iteration: 29878; Percent complete: 99.6%; Average loss: 0.1673\n",
            "Iteration: 29879; Percent complete: 99.6%; Average loss: 0.0655\n",
            "Iteration: 29880; Percent complete: 99.6%; Average loss: 0.1656\n",
            "Iteration: 29881; Percent complete: 99.6%; Average loss: 0.0956\n",
            "Iteration: 29882; Percent complete: 99.6%; Average loss: 0.1450\n",
            "Iteration: 29883; Percent complete: 99.6%; Average loss: 0.0689\n",
            "Iteration: 29884; Percent complete: 99.6%; Average loss: 0.1166\n",
            "Iteration: 29885; Percent complete: 99.6%; Average loss: 0.1049\n",
            "Iteration: 29886; Percent complete: 99.6%; Average loss: 0.1026\n",
            "Iteration: 29887; Percent complete: 99.6%; Average loss: 0.0923\n",
            "Iteration: 29888; Percent complete: 99.6%; Average loss: 0.0931\n",
            "Iteration: 29889; Percent complete: 99.6%; Average loss: 0.1216\n",
            "Iteration: 29890; Percent complete: 99.6%; Average loss: 0.1368\n",
            "Iteration: 29891; Percent complete: 99.6%; Average loss: 0.1160\n",
            "Iteration: 29892; Percent complete: 99.6%; Average loss: 0.1510\n",
            "Iteration: 29893; Percent complete: 99.6%; Average loss: 0.1423\n",
            "Iteration: 29894; Percent complete: 99.6%; Average loss: 0.1321\n",
            "Iteration: 29895; Percent complete: 99.7%; Average loss: 0.1294\n",
            "Iteration: 29896; Percent complete: 99.7%; Average loss: 0.1319\n",
            "Iteration: 29897; Percent complete: 99.7%; Average loss: 0.1065\n",
            "Iteration: 29898; Percent complete: 99.7%; Average loss: 0.1361\n",
            "Iteration: 29899; Percent complete: 99.7%; Average loss: 0.1700\n",
            "Iteration: 29900; Percent complete: 99.7%; Average loss: 0.1138\n",
            "Iteration: 29901; Percent complete: 99.7%; Average loss: 0.1535\n",
            "Iteration: 29902; Percent complete: 99.7%; Average loss: 0.0849\n",
            "Iteration: 29903; Percent complete: 99.7%; Average loss: 0.0929\n",
            "Iteration: 29904; Percent complete: 99.7%; Average loss: 0.1330\n",
            "Iteration: 29905; Percent complete: 99.7%; Average loss: 0.1112\n",
            "Iteration: 29906; Percent complete: 99.7%; Average loss: 0.1490\n",
            "Iteration: 29907; Percent complete: 99.7%; Average loss: 0.1309\n",
            "Iteration: 29908; Percent complete: 99.7%; Average loss: 0.1323\n",
            "Iteration: 29909; Percent complete: 99.7%; Average loss: 0.0970\n",
            "Iteration: 29910; Percent complete: 99.7%; Average loss: 0.1121\n",
            "Iteration: 29911; Percent complete: 99.7%; Average loss: 0.1692\n",
            "Iteration: 29912; Percent complete: 99.7%; Average loss: 0.1102\n",
            "Iteration: 29913; Percent complete: 99.7%; Average loss: 0.1207\n",
            "Iteration: 29914; Percent complete: 99.7%; Average loss: 0.1259\n",
            "Iteration: 29915; Percent complete: 99.7%; Average loss: 0.1133\n",
            "Iteration: 29916; Percent complete: 99.7%; Average loss: 0.1285\n",
            "Iteration: 29917; Percent complete: 99.7%; Average loss: 0.1138\n",
            "Iteration: 29918; Percent complete: 99.7%; Average loss: 0.1377\n",
            "Iteration: 29919; Percent complete: 99.7%; Average loss: 0.0919\n",
            "Iteration: 29920; Percent complete: 99.7%; Average loss: 0.0977\n",
            "Iteration: 29921; Percent complete: 99.7%; Average loss: 0.1128\n",
            "Iteration: 29922; Percent complete: 99.7%; Average loss: 0.1087\n",
            "Iteration: 29923; Percent complete: 99.7%; Average loss: 0.0918\n",
            "Iteration: 29924; Percent complete: 99.7%; Average loss: 0.1301\n",
            "Iteration: 29925; Percent complete: 99.8%; Average loss: 0.0984\n",
            "Iteration: 29926; Percent complete: 99.8%; Average loss: 0.1251\n",
            "Iteration: 29927; Percent complete: 99.8%; Average loss: 0.1230\n",
            "Iteration: 29928; Percent complete: 99.8%; Average loss: 0.1215\n",
            "Iteration: 29929; Percent complete: 99.8%; Average loss: 0.1059\n",
            "Iteration: 29930; Percent complete: 99.8%; Average loss: 0.1107\n",
            "Iteration: 29931; Percent complete: 99.8%; Average loss: 0.1473\n",
            "Iteration: 29932; Percent complete: 99.8%; Average loss: 0.0990\n",
            "Iteration: 29933; Percent complete: 99.8%; Average loss: 0.1419\n",
            "Iteration: 29934; Percent complete: 99.8%; Average loss: 0.1240\n",
            "Iteration: 29935; Percent complete: 99.8%; Average loss: 0.0971\n",
            "Iteration: 29936; Percent complete: 99.8%; Average loss: 0.1615\n",
            "Iteration: 29937; Percent complete: 99.8%; Average loss: 0.1199\n",
            "Iteration: 29938; Percent complete: 99.8%; Average loss: 0.1216\n",
            "Iteration: 29939; Percent complete: 99.8%; Average loss: 0.1073\n",
            "Iteration: 29940; Percent complete: 99.8%; Average loss: 0.0955\n",
            "Iteration: 29941; Percent complete: 99.8%; Average loss: 0.0917\n",
            "Iteration: 29942; Percent complete: 99.8%; Average loss: 0.1046\n",
            "Iteration: 29943; Percent complete: 99.8%; Average loss: 0.1066\n",
            "Iteration: 29944; Percent complete: 99.8%; Average loss: 0.1124\n",
            "Iteration: 29945; Percent complete: 99.8%; Average loss: 0.1421\n",
            "Iteration: 29946; Percent complete: 99.8%; Average loss: 0.1156\n",
            "Iteration: 29947; Percent complete: 99.8%; Average loss: 0.1384\n",
            "Iteration: 29948; Percent complete: 99.8%; Average loss: 0.1707\n",
            "Iteration: 29949; Percent complete: 99.8%; Average loss: 0.1537\n",
            "Iteration: 29950; Percent complete: 99.8%; Average loss: 0.1271\n",
            "Iteration: 29951; Percent complete: 99.8%; Average loss: 0.1032\n",
            "Iteration: 29952; Percent complete: 99.8%; Average loss: 0.1133\n",
            "Iteration: 29953; Percent complete: 99.8%; Average loss: 0.0886\n",
            "Iteration: 29954; Percent complete: 99.8%; Average loss: 0.1105\n",
            "Iteration: 29955; Percent complete: 99.9%; Average loss: 0.1378\n",
            "Iteration: 29956; Percent complete: 99.9%; Average loss: 0.1344\n",
            "Iteration: 29957; Percent complete: 99.9%; Average loss: 0.1326\n",
            "Iteration: 29958; Percent complete: 99.9%; Average loss: 0.1460\n",
            "Iteration: 29959; Percent complete: 99.9%; Average loss: 0.1125\n",
            "Iteration: 29960; Percent complete: 99.9%; Average loss: 0.1445\n",
            "Iteration: 29961; Percent complete: 99.9%; Average loss: 0.1845\n",
            "Iteration: 29962; Percent complete: 99.9%; Average loss: 0.1338\n",
            "Iteration: 29963; Percent complete: 99.9%; Average loss: 0.1405\n",
            "Iteration: 29964; Percent complete: 99.9%; Average loss: 0.1122\n",
            "Iteration: 29965; Percent complete: 99.9%; Average loss: 0.0694\n",
            "Iteration: 29966; Percent complete: 99.9%; Average loss: 0.1079\n",
            "Iteration: 29967; Percent complete: 99.9%; Average loss: 0.0581\n",
            "Iteration: 29968; Percent complete: 99.9%; Average loss: 0.1394\n",
            "Iteration: 29969; Percent complete: 99.9%; Average loss: 0.1253\n",
            "Iteration: 29970; Percent complete: 99.9%; Average loss: 0.0910\n",
            "Iteration: 29971; Percent complete: 99.9%; Average loss: 0.1482\n",
            "Iteration: 29972; Percent complete: 99.9%; Average loss: 0.1298\n",
            "Iteration: 29973; Percent complete: 99.9%; Average loss: 0.0880\n",
            "Iteration: 29974; Percent complete: 99.9%; Average loss: 0.1222\n",
            "Iteration: 29975; Percent complete: 99.9%; Average loss: 0.1354\n",
            "Iteration: 29976; Percent complete: 99.9%; Average loss: 0.1260\n",
            "Iteration: 29977; Percent complete: 99.9%; Average loss: 0.1418\n",
            "Iteration: 29978; Percent complete: 99.9%; Average loss: 0.1321\n",
            "Iteration: 29979; Percent complete: 99.9%; Average loss: 0.1152\n",
            "Iteration: 29980; Percent complete: 99.9%; Average loss: 0.1180\n",
            "Iteration: 29981; Percent complete: 99.9%; Average loss: 0.1058\n",
            "Iteration: 29982; Percent complete: 99.9%; Average loss: 0.1036\n",
            "Iteration: 29983; Percent complete: 99.9%; Average loss: 0.1892\n",
            "Iteration: 29984; Percent complete: 99.9%; Average loss: 0.1424\n",
            "Iteration: 29985; Percent complete: 100.0%; Average loss: 0.0985\n",
            "Iteration: 29986; Percent complete: 100.0%; Average loss: 0.1326\n",
            "Iteration: 29987; Percent complete: 100.0%; Average loss: 0.1367\n",
            "Iteration: 29988; Percent complete: 100.0%; Average loss: 0.1688\n",
            "Iteration: 29989; Percent complete: 100.0%; Average loss: 0.1068\n",
            "Iteration: 29990; Percent complete: 100.0%; Average loss: 0.1545\n",
            "Iteration: 29991; Percent complete: 100.0%; Average loss: 0.0768\n",
            "Iteration: 29992; Percent complete: 100.0%; Average loss: 0.1026\n",
            "Iteration: 29993; Percent complete: 100.0%; Average loss: 0.1678\n",
            "Iteration: 29994; Percent complete: 100.0%; Average loss: 0.1235\n",
            "Iteration: 29995; Percent complete: 100.0%; Average loss: 0.1149\n",
            "Iteration: 29996; Percent complete: 100.0%; Average loss: 0.1710\n",
            "Iteration: 29997; Percent complete: 100.0%; Average loss: 0.1269\n",
            "Iteration: 29998; Percent complete: 100.0%; Average loss: 0.1059\n",
            "Iteration: 29999; Percent complete: 100.0%; Average loss: 0.0927\n",
            "Iteration: 30000; Percent complete: 100.0%; Average loss: 0.1137\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "matched-david"
      },
      "source": [
        "## RUN AN PLAY"
      ],
      "id": "matched-david"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Clcxmkslg-_4"
      },
      "source": [
        "## Thử nghiệm câu đơn với SEQ2SEQ model"
      ],
      "id": "Clcxmkslg-_4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "preliminary-nothing",
        "outputId": "776614ee-5e62-40c6-c2ec-efaa576f705e"
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "# Begin chatting\n",
        "evaluateInput(encoder, decoder, searcher, voc)"
      ],
      "id": "preliminary-nothing",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> \"I don't want to go home tonight\",\n",
            "Quesion\n",
            "\"I don't want to go home tonight\",\n",
            "Bot: why ? that s not so cute .\n",
            "> \"Shall we get started?\",\n",
            "Quesion\n",
            "\"Shall we get started?\",\n",
            "Bot: we don t have to make an appointment .\n",
            "> \"What's your favorite food?\",\n",
            "Quesion\n",
            "\"What's your favorite food?\",\n",
            "Bot: i like fish and vegetables . food\n",
            "> \"What kinds of films do you like?\",\n",
            "Quesion\n",
            "\"What kinds of films do you like?\",\n",
            "Bot: i like all kinds . about something like that .\n",
            "> i m planning to be there at twelve .\n",
            "Quesion\n",
            "i m planning to be there at twelve .\n",
            "Bot: we are going to do this week .\n",
            "> \"How long have you learning English?\",\n",
            "Quesion\n",
            "\"How long have you learning English?\",\n",
            "Bot: in english . most english in english . english\n",
            "> \"What's your favorite subject?\",\n",
            "Quesion\n",
            "\"What's your favorite subject?\",\n",
            "Bot: i d like to have her birthday and give you more sweet .\n",
            "> \"Do you need anything else?\",\n",
            "Quesion\n",
            "\"Do you need anything else?\",\n",
            "Bot: no . that s all . thank you . . .\n",
            "> \"Music is my hobby\",\n",
            "Quesion\n",
            "\"Music is my hobby\",\n",
            "Bot: what kind of music is she ?\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-229-9b46599acbc0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-224-060ed9158945>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Get input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Check if it is quit case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generous-justice"
      },
      "source": [
        "## Reinforcement learning"
      ],
      "id": "generous-justice"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assured-listening"
      },
      "source": [
        "### Forward and backward models "
      ],
      "id": "assured-listening"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egyptian-adobe"
      },
      "source": [
        "#forward_encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "#forward_decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "#forward_encoder = forward_encoder.to(device)\n",
        "#forward_decoder = forward_decoder.to(device)"
      ],
      "id": "egyptian-adobe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brutal-jones"
      },
      "source": [
        "forward_encoder = encoder\n",
        "forward_decoder = decoder\n",
        "forward_encoder = forward_encoder.to(device)\n",
        "forward_decoder = forward_decoder.to(device)"
      ],
      "id": "brutal-jones",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "united-correspondence"
      },
      "source": [
        "backward_encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "backward_decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "backward_encoder = backward_encoder.to(device)\n",
        "backward_decoder = backward_decoder.to(device)"
      ],
      "id": "united-correspondence",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "understanding-opening"
      },
      "source": [
        "### Utility functions"
      ],
      "id": "understanding-opening"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "entertaining-catering"
      },
      "source": [
        "def convertResponse(response):\n",
        "    size1 = len(response)\n",
        "    size2 = batch_size\n",
        "    npRes = np.zeros((size1, size2), dtype=np.int64)\n",
        "    npLengths = np.zeros(size2, dtype=np.int64)\n",
        "    for i in range(size1):\n",
        "        prov = response[i].cpu().numpy()\n",
        "        for j in range(prov.size):\n",
        "            npLengths[j] = npLengths[j] + 1\n",
        "            if prov.size > 1:\n",
        "                npRes[i][j] = prov[j]\n",
        "            else:\n",
        "                npRes[i][j] = prov \n",
        "    res = torch.from_numpy(npRes)\n",
        "    lengths = torch.from_numpy(npLengths)\n",
        "    return res, lengths"
      ],
      "id": "entertaining-catering",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crude-inside"
      },
      "source": [
        "def convertTarget(target):\n",
        "    size1 = len(target)\n",
        "    size2 = batch_size\n",
        "    npRes = np.zeros((size1, size2), dtype=np.int64)\n",
        "    mask = np.zeros((size1, size2), dtype=np.bool_)\n",
        "    npLengths = np.zeros(size2, dtype=np.int64)\n",
        "    for i in range(size1):\n",
        "        prov = target[i].cpu().numpy()\n",
        "        for j in range(prov.size):\n",
        "            npLengths[j] = npLengths[j] + 1\n",
        "            if prov.size > 1:\n",
        "                npRes[i][j] = prov[j]\n",
        "            else:\n",
        "                npRes[i][j] = prov \n",
        "                \n",
        "            if npRes[i][j] > 0:\n",
        "                mask[i][j] = True\n",
        "            else:\n",
        "                mask[i][j] = False\n",
        "            \n",
        "    res = torch.from_numpy(npRes)\n",
        "    lengths = torch.from_numpy(npLengths)\n",
        "    mask= torch.from_numpy(mask)\n",
        "    max_target_len = torch.max(lengths) #.detach().numpy()\n",
        "    return res, mask, max_target_len"
      ],
      "id": "crude-inside",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hispanic-diameter"
      },
      "source": [
        "def transformTensorToSameShapeAs(tensor, shape):\n",
        "    size1, size2 = shape\n",
        "    npNewT = np.zeros((size1, size2), dtype=np.int64)\n",
        "    npNewMask = np.zeros((size1, size2), dtype=np.bool_)\n",
        "    tensorSize1, tensorSize2 = tensor.size()\n",
        "    for i in range(tensorSize1):\n",
        "        for j in range(tensorSize2):\n",
        "            npNewT[i][j] = tensor[i][j]\n",
        "            npNewMask[i][j]= True\n",
        "    return torch.from_numpy(npNewT), torch.from_numpy(npNewMask)"
      ],
      "id": "hispanic-diameter",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grateful-messenger"
      },
      "source": [
        "### Training step for a single iteration "
      ],
      "id": "grateful-messenger"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liable-lebanon"
      },
      "source": [
        "def RL(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio):\n",
        "    # Set device options\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    # Lengths for rnn packing should always be on the cpu\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "    \n",
        "    #Initialize variables\n",
        "    loss=0\n",
        "    #print_losses = []\n",
        "    response=[]\n",
        "    \n",
        "    # Forward pass through encoder\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "    \n",
        "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
        "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    \n",
        "    \n",
        "    # Set initial decoder hidden state to the encoder's final hidden state\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "     # Determine if we are using teacher forcing this iteration\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "    \n",
        "     # Forward batch of sequences through decoder one time step at a time\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            #print_losses.append(mask_loss.item() * nTotal)\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs\n",
        "            )\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            #print_losses.append(mask_loss.item() * nTotal)\n",
        "            \n",
        "            #ni or decoder_output\n",
        "            response.append(topi)\n",
        "            \n",
        "    return loss, max_target_len, response"
      ],
      "id": "liable-lebanon",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "driven-wayne"
      },
      "source": [
        "Let's define the rewards method for each action.\n",
        "\n",
        "It is composed of 3 different types of rewards :\n",
        "- `ease of answering` which is basically saying that a turn generated by a machineshould be easy to respond to. \n",
        "- `information flow` : We  want  the  agent  to  contribute new information at each turn to keep the dialogue moving and avoid repetitive sequences. \n",
        "- `semantic coherence` measures the adequacy of responses to avoid situations in which the generated replies are highly rewarded but are ungrammatical or not coherent. "
      ],
      "id": "driven-wayne"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dressed-symbol"
      },
      "source": [
        "def easeOfAnswering(input_variable, lengths, dull_responses, mask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio):\n",
        "    NS=len(dull_responses)\n",
        "    r1=0\n",
        "    for d in dull_responses:\n",
        "        d, mask, max_target_len = outputVar(d, voc)\n",
        "        newD, newMask = transformTensorToSameShapeAs(d, input_variable.size())\n",
        "        #tar, mask, max_target_len = convertTarget(d)\n",
        "        forward_loss, forward_len, _ = RL(input_variable, lengths, newD, newMask, max_target_len, encoder, decoder, batch_size, teacher_forcing_ratio)\n",
        "        # log (1/P(a|s)) = CE  --> log(P(a | s)) = - CE\n",
        "        if forward_len > 0:\n",
        "            r1 -= forward_loss / forward_len\n",
        "    if len(dull_responses) > 0:\n",
        "        r1 = r1 / NS\n",
        "    return r1"
      ],
      "id": "dressed-symbol",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abandoned-hierarchy"
      },
      "source": [
        "def informationFlow(responses):\n",
        "    r2=0\n",
        "    if(len(responses) > 2):\n",
        "        #2 representations obtained from the encoder for two consecutive turns pi and pi+1\n",
        "        h_pi = responses[-3]\n",
        "        h_pi1 = responses[-1]\n",
        "        # length of the two vector might not match\n",
        "        min_length = min(len(h_pi), len(h_pi+1))\n",
        "        h_pi = h_pi[:min_length]\n",
        "        h_pi1 = h_pi1[:min_length]\n",
        "        #cosine similarity \n",
        "        #cos_sim = 1 - distance.cosine(h_pi, h_pi1)\n",
        "        cos_sim = 1 - distance.cdist(h_pi.cpu().numpy(), h_pi1.cpu().numpy(), 'cosine')\n",
        "        #Handle negative cos_sim\n",
        "        if np.any(cos_sim <= 0):\n",
        "            r2 = - cos_sim\n",
        "        else:\n",
        "            r2 = - np.log(cos_sim)\n",
        "        r2 = np.mean(r2)\n",
        "    return r2"
      ],
      "id": "abandoned-hierarchy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mighty-webster"
      },
      "source": [
        "def semanticCoherence(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio):\n",
        "    #print(\"IN R3:\")\n",
        "    #print(\"Input_variable :\", input_variable.shape)\n",
        "    #print(\"Lengths :\", lengths.shape)\n",
        "    #print(\"Target_variable :\", target_variable.shape)\n",
        "    #print(\"Mask :\", mask.shape)\n",
        "    #print(\"Max_Target_Len :\", max_target_len)\n",
        "    r3 = 0\n",
        "    forward_loss, forward_len, _ = RL(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "    ep_input, lengths_trans = convertResponse(target_variable)\n",
        "    #print(\"ep_input :\", ep_input.shape)\n",
        "    #print(\"Lengths transformed :\", lengths_trans.shape)\n",
        "    ep_target, mask_trans, max_target_len_trans = convertTarget(input_variable)\n",
        "    #print(\"ep_target :\", ep_target.shape)\n",
        "    #print(\"mask transformed :\", mask_trans.shape)\n",
        "    #print(\"max_target_len_trans :\", max_target_len_trans)\n",
        "    backward_loss, backward_len, _ = RL(ep_input, lengths_trans, ep_target, mask_trans, max_target_len_trans, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
        "    if forward_len > 0:\n",
        "        r3 += forward_loss / forward_len\n",
        "    if backward_len > 0:\n",
        "        r3+= backward_loss / backward_len\n",
        "    return r3"
      ],
      "id": "mighty-webster",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dried-breathing"
      },
      "source": [
        "l1=0.6\n",
        "l2=0.1\n",
        "l3=0.3\n"
      ],
      "id": "dried-breathing",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "loaded-python"
      },
      "source": [
        "def calculate_rewards(input_var, lengths, target_var, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio):\n",
        "    #rewards per episode\n",
        "    ep_rewards = []\n",
        "    #indice of current episode\n",
        "    ep_num = 1\n",
        "    #list of responses\n",
        "    responses = []\n",
        "    #input of current episode\n",
        "    ep_input = input_var\n",
        "    #target of current episode\n",
        "    ep_target = target_var\n",
        "    \n",
        "    #ep_num bounded -> to redefine (MEDIUM POST)\n",
        "    while (ep_num <= 10):\n",
        "        \n",
        "        print(ep_num)\n",
        "        #generate current response with the forward model\n",
        "        _, _, curr_response = RL(ep_input, lengths, ep_target, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        #Break if :\n",
        "        # 1 -> dull response\n",
        "        # 2 -> response is less than MIN_LENGTH\n",
        "        # 3 -> repetition ie curr_response in responses\n",
        "        if(len(curr_response) < MIN_COUNT):# or (curr_response in dull_responses) or (curr_response in responses)):\n",
        "            break\n",
        "            \n",
        "            \n",
        "        #We can add the response to responses list\n",
        "        #curr_response = torch.LongTensor(curr_response).view(-1, 1)\n",
        "        #transform curr_response size\n",
        "        #target = torch.zeros(960, 1)\n",
        "        #target[:15, :] = curr_response\n",
        "        #curr_response = target\n",
        "        #print(curr_response.size())\n",
        "        #curr_response = torch.reshape(curr_response, (15, 64))\n",
        "        #print(curr_response.size())\n",
        "        #curr_response = curr_response.to(device)\n",
        "        #responses.append(curr_response) \n",
        "        \n",
        "        \n",
        "        #Ease of answering\n",
        "        r1 = easeOfAnswering(ep_input, lengths, dull_responses, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        #Information flow\n",
        "        r2 = informationFlow(responses)\n",
        "        \n",
        "        #Semantic coherence\n",
        "        r3 = semanticCoherence(ep_input, lengths, target_var, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        #Final reward as a weighted sum of rewards\n",
        "        r = l1*r1 + l2*r2 + l3*r3\n",
        "        print(f\"reward:{r}\")\n",
        "      \n",
        "        #Add the current reward to the list\n",
        "        ep_rewards.append(r.detach().cpu().numpy())\n",
        "        \n",
        "        #We can add the response to responses list\n",
        "        curr_response, lengths = convertResponse(curr_response)\n",
        "        curr_response = curr_response.to(device)\n",
        "        responses.append(curr_response)\n",
        "        \n",
        "        #Next input is the current response\n",
        "        ep_input = curr_response\n",
        "        #Next target -> dummy\n",
        "        ep_target = torch.zeros(MAX_LENGTH,batch_size,dtype=torch.int64)\n",
        "        #ep_target = torch.LongTensor(torch.LongTensor([0] * MAX_LENGTH)).view(-1, 1)\n",
        "        ep_target = ep_target.to(device)\n",
        "        \n",
        "        #Turn off the teacher forcing  after first iteration -> dummy target\n",
        "        teacher_forcing_ratio = 0\n",
        "        ep_num +=1\n",
        "    #Take the mean of the episodic rewards\n",
        "    return np.mean(ep_rewards) if len(ep_rewards) > 0 else 0 "
      ],
      "id": "loaded-python",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "facial-giving"
      },
      "source": [
        "Dull responses "
      ],
      "id": "facial-giving"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "focal-blink"
      },
      "source": [
        "dull_responses = [\"i do not know what you are talking about.\", \"i do not know.\", \n",
        " \"you do not know.\", \"you know what i mean.\", \"i know what you mean.\", \n",
        " \"you know what i am saying.\", \"you do not know anything.\"]\n",
        "#dull_responses = []"
      ],
      "id": "focal-blink",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDUGDHBFRwlt"
      },
      "source": [
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "id": "WDUGDHBFRwlt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ix-LdC9RUE-c"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n"
      ],
      "id": "ix-LdC9RUE-c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "violent-mixer"
      },
      "source": [
        "### Training RL loop"
      ],
      "id": "violent-mixer"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "disciplinary-slide"
      },
      "source": [
        "def trainingRLLoop(model_name, voc, pairs, batch_size, forward_encoder, forward_encoder_optimizer, forward_decoder, forward_decoder_optimizer, backward_encoder, backward_encoder_optimizer, backward_decoder, backward_decoder_optimizer,teacher_forcing_ratio, dull_responses, n_iteration, print_every, save_every, save_dir):\n",
        "\n",
        "    # Load batches for each iteration\n",
        "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
        "                      for _ in range(n_iteration)]\n",
        "    plot_every=print_every\n",
        "    # Initializations\n",
        "    print('Initializing ...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "    plot_losses = []\n",
        "    plot_loss=0\n",
        "    #Training loop\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        print(\"Iteration\", iteration)\n",
        "        training_batch = training_batches[iteration - 1]\n",
        "        # Extract fields from batch\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        ##MODIFS HERE\n",
        "        # Zero gradients the optimizer\n",
        "        forward_encoder_optimizer.zero_grad()\n",
        "        forward_decoder_optimizer.zero_grad()\n",
        "        \n",
        "        backward_encoder_optimizer.zero_grad()\n",
        "        backward_decoder_optimizer.zero_grad()\n",
        "        \n",
        "        #Forward\n",
        "        forward_loss, forward_len, _ = RL(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        \n",
        "        #Calculate reward\n",
        "        reward = calculate_rewards(input_variable, lengths, target_variable, mask, max_target_len, forward_encoder, forward_decoder, backward_encoder, backward_decoder, batch_size, teacher_forcing_ratio)\n",
        "        print(reward)\n",
        "        reward_arr.append(reward)\n",
        "        #Update forward seq2seq with loss scaled by reward\n",
        "        loss = forward_loss * reward\n",
        "        \n",
        "        loss.backward()\n",
        "        forward_encoder_optimizer.step()\n",
        "        forward_decoder_optimizer.step()\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        print_loss += loss / forward_len\n",
        "        plot_loss += loss / forward_len\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "        if iteration % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss= 0\n",
        "            \n",
        "        #SAVE CHECKPOINT TO DO\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name)#, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': forward_encoder.state_dict(),\n",
        "                'de': forward_decoder.state_dict(),\n",
        "                'en_opt': forward_encoder_optimizer.state_dict(),\n",
        "                'de_opt': forward_decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': voc.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n",
        "    showPlot(plot_losses)"
      ],
      "id": "disciplinary-slide",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foheCIANs-GQ"
      },
      "source": [
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "id": "foheCIANs-GQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIqrWZ__csdW"
      },
      "source": [
        "reward_arr=[]"
      ],
      "id": "HIqrWZ__csdW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assisted-phenomenon"
      },
      "source": [
        "## RUN "
      ],
      "id": "assisted-phenomenon"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "faced-metadata",
        "scrolled": true,
        "outputId": "002e1b28-9948-43ba-fd96-7cf532fca386"
      },
      "source": [
        "#Configure RL model\n",
        "\n",
        "model_name='RL_model_seq'\n",
        "n_iteration = 10000\n",
        "print_every=500\n",
        "save_every=1000\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "forward_encoder.train()\n",
        "forward_decoder.train()\n",
        "\n",
        "backward_encoder.train()\n",
        "backward_decoder.train()\n",
        "\n",
        "# Initialize optimizers\n",
        "print('Building optimizers ...')\n",
        "forward_encoder_optimizer = optim.Adam(forward_encoder.parameters(), lr=learning_rate)\n",
        "forward_decoder_optimizer = optim.Adam(forward_decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "backward_encoder_optimizer = optim.Adam(backward_encoder.parameters(), lr=learning_rate)\n",
        "backward_decoder_optimizer = optim.Adam(backward_decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in forward_encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in forward_decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "            \n",
        "for state in backward_encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in backward_decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "        \n",
        "# Run training iterations\n",
        "print(\"Starting Training!\")\n",
        "trainingRLLoop(model_name, voc, pairs, batch_size, forward_encoder, forward_encoder_optimizer, forward_decoder, forward_decoder_optimizer, backward_encoder, backward_encoder_optimizer, backward_decoder, backward_decoder_optimizer, teacher_forcing_ratio, dull_responses, n_iteration, print_every, save_every, save_dir)"
      ],
      "id": "faced-metadata",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-d874616ae9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Ensure dropout layers are in train mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mforward_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mforward_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'forward_encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvGtld-BWyOB",
        "outputId": "73ea2c99-5751-4e2d-a58b-4c7f6c1bdbeb"
      },
      "source": [
        "print(len(reward_arr))"
      ],
      "id": "wvGtld-BWyOB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "352\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t4fkaDgXALI",
        "outputId": "459beb4c-0561-4fc6-cfa9-af99cda884ae"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame({'reward':reward_arr})\n",
        "print (df)"
      ],
      "id": "6t4fkaDgXALI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       reward\n",
            "0    0.000000\n",
            "1   -5.658504\n",
            "2   -5.606884\n",
            "3   -5.749689\n",
            "4    0.000000\n",
            "..        ...\n",
            "347       NaN\n",
            "348  0.000000\n",
            "349  0.000000\n",
            "350  0.000000\n",
            "351  0.000000\n",
            "\n",
            "[352 rows x 1 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIBdlM0aXXJo",
        "outputId": "9d02511b-6354-4ae0-ab52-2c859a5c4895"
      },
      "source": [
        "%cd /content/\n",
        "df.to_csv('0.6_0.1_0.3.csv')#ghi file csv và tải về"
      ],
      "id": "mIBdlM0aXXJo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a43EJ5COO6Jw",
        "outputId": "6a18b540-d591-4f66-fd1c-6467b4585f63"
      },
      "source": [
        "%cd /content/drive/MyDrive/CS106 # trở lại chỗ load model"
      ],
      "id": "a43EJ5COO6Jw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/CS106\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE7_9gGwsdaT"
      },
      "source": [
        "# Load model trong drive ra để sử dụng"
      ],
      "id": "IE7_9gGwsdaT"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry7BvgCHsgwg",
        "outputId": "8ae46622-794a-4271-e5e8-8d51dc1cacb5"
      },
      "source": [
        "loadFilename ='data/save/RL_model_seq0.2-0.2-0.6/dailydialog/10000_checkpoint.tar' # thay đổi model, và trọng số ở đây 'data/save/XXX/dailydialog/YYY_checkpoint.tar' với XXX là tên model RL chọn,YYY là số iteration\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    #checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    voc.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "\n",
        "print('Building encoder and decoder ...')\n",
        "# Initialize word embeddings\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "# Initialize encoder & decoder models\n",
        "forward_encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "forward_decoder= LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
        "if loadFilename:\n",
        "    forward_encoder.load_state_dict(encoder_sd)\n",
        "    forward_decoder.load_state_dict(decoder_sd)\n",
        "# Use appropriate device\n",
        "forward_encoder = forward_encoder.to(device)\n",
        "forward_decoder = forward_decoder.to(device)\n",
        "print('Models built and ready to go!')"
      ],
      "id": "Ry7BvgCHsgwg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building encoder and decoder ...\n",
            "Models built and ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mASK4DBnAuSf"
      },
      "source": [
        "def evaluateInput(encoder, decoder, searcher, voc):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            # Get input sentence\n",
        "            input_sentence = input('> ')\n",
        "            # Check if it is quit case\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "            # Normalize sentence\n",
        "            print(\"Quesion\")\n",
        "            print(input_sentence)\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            # Evaluate sentence\n",
        "            \n",
        "            \n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            # Format and print response sentence\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Bot:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Encountered unknown word.\")"
      ],
      "id": "mASK4DBnAuSf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Hm07BRzAOD3"
      },
      "source": [
        "def Simulate(forward_encoder, forward_decoder, searcher, voc,input_sentence):\n",
        "  #print(input_sentence)\n",
        "  for i in range(0,8):\n",
        "    try:\n",
        "              # Get input sentence\n",
        "              # Check if it is quit case\n",
        "              if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "              # Normalize sentence\n",
        "              \n",
        "              input_sentence = normalizeString(input_sentence)\n",
        "              # Evaluate sentence\n",
        "              \n",
        "              \n",
        "              output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "              # Format and print response sentence\n",
        "              output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "              print( ' '.join(output_words))\n",
        "          \n",
        "              input_sentence=' '.join(output_words)\n",
        "\n",
        "    except KeyError:\n",
        "              print(\"Error: Encountered unknown word.\")"
      ],
      "id": "0Hm07BRzAOD3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5e1sMMuhpkR"
      },
      "source": [
        "## Chạy thử câu đơn với model RL"
      ],
      "id": "S5e1sMMuhpkR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "intelligent-jones",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "outputId": "7a1a66dc-7c66-4f55-9615-8ed839f4c19c"
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "forward_encoder.eval()\n",
        "forward_decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(forward_encoder, forward_decoder)\n",
        "\n",
        "# Begin chatting\n",
        "evaluateInput(forward_encoder, forward_decoder, searcher, voc)"
      ],
      "id": "intelligent-jones",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-164-a122baedd810>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Begin chatting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mevaluateInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mforward_encoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_decoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-162-88e130b7effb>\u001b[0m in \u001b[0;36mevaluateInput\u001b[0;34m(encoder, decoder, searcher, voc)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0;31m# Get input sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0minput_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0;31m# Check if it is quit case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'q'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0minput_sentence\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNiuKLeRFoh7"
      },
      "source": [
        "def EvaluateMultiInput(forward_encoder, forward_decoder, searcher, voc,input_arr):\n",
        "  for input_sentence in input_arr:\n",
        "    try:\n",
        "              # print(input_sentence)\n",
        "              # Get input sentence\n",
        "              # Check if it is quit case\n",
        "              if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "              # Normalize sentence\n",
        "              \n",
        "              input_sentence = normalizeString(input_sentence)\n",
        "              # Evaluate sentence\n",
        "              \n",
        "              \n",
        "              output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "              # Format and print response sentence\n",
        "              output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "              print( ' '.join(output_words))\n",
        "          \n",
        "              input_sentence=' '.join(output_words)\n",
        "\n",
        "    except KeyError:\n",
        "              print(\"Error: Encountered unknown word.\")\n"
      ],
      "id": "HNiuKLeRFoh7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeA9fgUbhvFk"
      },
      "source": [
        "## Đánh giá nhiều câu cùng lúc"
      ],
      "id": "XeA9fgUbhvFk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6u0vC0OGh-U",
        "outputId": "9f719da4-31e8-43ce-b72d-90d0867918f0"
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "forward_encoder.eval()\n",
        "forward_decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(forward_encoder, forward_decoder)\n",
        "input_arr=[\n",
        "\"I don't want to go home tonight\",\n",
        "\"Shall we get started?\",\n",
        "\"How come you never say it?\",\n",
        "\"Let's try other places\",\n",
        "\"How about tea?\",\n",
        "\"What a lovely day we are having\",\n",
        "\"Nice to meet you\",\n",
        "\"Would you like to have a chat?\",\n",
        "\"What made you think so?\",\n",
        "\"I need you to stay here\",\n",
        "\"Can you repeat that?\",\n",
        "\"I am a human\",\n",
        "\"The food was delicious\",\n",
        "\"I am honored to be here\",\n",
        "\"I go to a university\",\n",
        "\"Why don't we go out for dinner?\",\n",
        "\"Are you sure about that?\",\n",
        "\"Can you be more specific?\",\n",
        "\"I don't think so\",\n",
        "\"I absolutely agree\",\n",
        "\"Music is my hobby\",\n",
        "\"I brought this since you like it\",\n",
        "\"Do you agree with that?\",\n",
        "\"Congratulations\",\n",
        "\"I am happy for you\",\n",
        "\"I am a women\",\n",
        "\"He is handsome, don't you think?\",\n",
        "\"She is beautiful, wouldn't you agree?\",\n",
        "\"Likewise, I share the same thoughts\",\n",
        "\"This is nothing to be scared of\",\n",
        "\"A phone call is a good idea\",\n",
        "\"Bring me a cup of water, please\",\n",
        "\"That is horrible\",\n",
        "\"I can't imagine what you are going through\",\n",
        "\"I have to say no\",\n",
        "\"Not at all, i don't mind\",\n",
        "\"That has bothered me for a while\",\n",
        "\"But how would that change anything?\",\n",
        "\"Why bother with it?\",\n",
        "\"Let's change topic\",\n",
        "\"Please, it is my treat\",\n",
        "\"You are beautiful\",\n",
        "\"Can I have a bit of your time?\",\n",
        "\"How would you go about doing that?\",\n",
        "\"Can it be done?\",\n",
        "\"You mustn't do it\",\n",
        "\"Can you tell?\",\n",
        "\"How old are you?\",\n",
        "\"What is your full name?\",\n",
        "\"Do you have any feelings for me?\",\n",
        "\"How much time do you have here?\",\n",
        "\"Do you play football?\",\n",
        "\"Where are you from?\",\n",
        "\"How is the weather today?\",\n",
        "\"Did you go to school?\",\n",
        "\"How is your family doing?\",\n",
        "\"Where are you going?\",\n",
        "\"Where do you live?\",\n",
        "\"When is your birthday?\",\n",
        "\"What is your phone number?\",\n",
        "\"What's your email address?\",\n",
        "\"What do you do?\",\n",
        "\"What's your job?\",\n",
        "\"What line of work are you in?\",\n",
        "\"What company do you work for?\",\n",
        "\"Are you married?\",\n",
        "\"Are you in a relationship?\",\n",
        "\"Do you have any siblings?\",\n",
        "\"Who do you live with?\",\n",
        "\"What time do you get up?\",\n",
        "\"What time do you have breakfast?\",\n",
        "\"What time do you go to bed?\",\n",
        "\"What's your hobby?\",\n",
        "\"What's your favorite food?\",\n",
        "\"What's your favorite color?\",\n",
        "\"What's your favorite drink?\",\n",
        "\"What kinds of films do you like?\",\n",
        "\"Did you like the movie?\",\n",
        "\"Where do you study?\",\n",
        "\"What time do you go to school?\",\n",
        "\"How do you get to school?\",\n",
        "\"What is your major?\",\n",
        "\"Why do you study English?\",\n",
        "\"Why are you studying English?\",\n",
        "\"What do you want to do after you graduate?\",\n",
        "\"How did you learn English?\",\n",
        "\"How many languages do you speak?\",\n",
        "\"Can you speak English?\",\n",
        "\"How long have you learning English?\",\n",
        "\"Which grade are you in?\",\n",
        "\"Which year are you in?\",\n",
        "\"Do you have any exams coming up?\",\n",
        "\"What's your favorite subject?\",\n",
        "\"What's the weather like?\",\n",
        "\"How's the weather?\",\n",
        "\"What's the temperature?\",\n",
        "\"What time is it?\",\n",
        "\"What's the date today?\",\n",
        "\"How are you?\",\n",
        "\"Where are you going?\",\n",
        "\"What are you going to do today?\",\n",
        "\"What are you doing?\",\n",
        "\"Where would you like to go?\",\n",
        "\"What's the matter?\",\n",
        "\"Is there anything I can do to help?\",\n",
        "\"What's on your mind?\",\n",
        "\"What did you do last night?\",\n",
        "\"What are you going to do tomorrow?\",\n",
        "\"What sports can you play?\",\n",
        "\"Can you give me a hand?\",\n",
        "\"Could you do me a favor?\",\n",
        "\"Could you please give me that book?\",\n",
        "\"Would you mind opening the window?\",\n",
        "\"How can I help you?\",\n",
        "\"May I help you?\",\n",
        "\"Can I try it on?\",\n",
        "\"Can I try it on somewhere?\",\n",
        "\"What size do you wear?\",\n",
        "\"What size do you take?\",\n",
        "\"Is that a good fit?\",\n",
        "\"Have you got something bigger?\",\n",
        "\"How much is it?\",\n",
        "\"How much does it cost?\",\n",
        "\"How would you like to pay?\",\n",
        "\"Can I pay by credit card?\",\n",
        "\"Do you need anything else?\",\n",
        "\"How are you feeling?\",\n",
        "\"How was the party?\",\n",
        "\"Are you ready to order?\",\n",
        "\"Would you like chicken or pasta?\",\n",
        "\"What would you like to drink?\",\n",
        "\"Did you save room for dessert?\",\n",
        "\"How does it taste?\",\n",
        "\"Can I get you anything else?\",\n",
        "\"What do you think about this?\",\n",
        "\"Are you here with anybody?\",\n",
        "\"Have you got a pet?\",\n",
        "\"How do you do?\",\n",
        "\"What's this?\",\n",
        "\"What does she look like?\",\n",
        "\"How is she?\",\n",
        "\"What is he like?\",\n",
        "\"Are you hungry?\",\n",
        "\"How do you feel about him?\",\n",
        "\"May I open the window?\",\n",
        "\"Do you need help?\",\n",
        "\"What do you do every day?\",\n",
        "\"What are you planning to do today?\",\n",
        "\"What are you planning for after work?\",\n",
        "\"Are you free tomorrow?\",\n",
        "\"Will you join me for coffee?\",\n",
        "\"Could we have lunch together one day?\",\n",
        "\"Would you like to have dinner with me?\",\n",
        "\"Where do you want to go?\",\n",
        "\"Would you like to come to the cinema with me tonight?\",\n",
        "\"Would you like to play a round of golf this weekend?\",\n",
        "\"How about coming to the barbecue at the tennis club?\",\n",
        "\"Have you been waiting long?\",\n",
        "\"How do you get to work?\",\n",
        "\"Is it close to the subway station?\",\n",
        "\"How long does it take?\",\n",
        "\"How often do you ride the bus?\",\n",
        "\"Could you tell me how to get to the police station?\",\n",
        "\"Excuse me! Is there a bank near here?\",\n",
        "\"What's the best way to the museum?\",\n",
        "\"Can I speak to her, please?\",\n",
        "\"When will she be back?\",\n",
        "\"Did you get my message?\",\n",
        "\"Would you like to leave a message?\",\n",
        "\"How long have you been working here?\",\n",
        "\"What time does the meeting start?\",\n",
        "\"What time does the meeting finish?\",\n",
        "\"Are you sick?\",\n",
        "\"What are your symptoms?\",\n",
        "\"How long have you been feeling like this?\",\n",
        "\"Are you on any sort of medication?\",\n",
        "\"Where's the counter, please?\",\n",
        "\"May I have your passport, please?\",\n",
        "\"Do you have anything to declare?\",\n",
        "\"Do you have any bags to check?\",\n",
        "\"Would you prefer an aisle seat or a window seat?\",\n",
        "\"What's the purpose of your trip?\",\n",
        "\"Where are you going to be staying?\",\n",
        "\"How long are you going to stay here?\",\n",
        "\"Could you tell me where Gate E is?\",\n",
        "\"Can I help you find something?\",\n",
        "\"Could you tell me where the meat is?\",\n",
        "\"Where can I find the non-fat biscuits?\",\n",
        "\"How much would you like?\",\n",
        "\"Can you offer me any discount?\",\n",
        "\"Do you need any help packing?\",\n",
        "\"Would you like a receipt?\",\n",
        "\"What times are you open?\",\n",
        "\"What time do you open?\",\n",
        "\"What time do you close?\",\n",
        "\"Do you have a reservation?\",\n",
        "\"What kind of room would you like?\",\n",
        "\"May I see your ID, please?\",\n",
        "\"Could I see the room?\",\n",
        "\"Do you have a credit card, sir?\",\n",
        "\"How long will you be staying?\",\n",
        "\"Do you have any rooms available for the 14th of April?\",\n",
        "\"How many places have you traveled to?\",\n",
        "\"Have you ever been abroad?\",\n",
        "\"What do you usually do during your trip?\",\n",
        "\"Do you prefer traveling by car, train or plane?\",\n",
        "\"Do you prefer traveling alone or joining a guided tour?\",\n",
        "\"Did you enjoy yourself last night?\",\n",
        "\"I'm ready to try again, if you are?\",\n",
        "\"It looks like you're in trouble there. Can I help?\",\n",
        "\"You got a package for me?\",\n",
        "\"Is there something you want to tell me, son?\",\n",
        "\"How, exactly, did you think that having an affair would help our marriage?\",\n",
        "\"You blame me but where were you at the time?\",\n",
        "\"What's in that bag and why are you hiding it here?\",\n",
        "\"She's been missing since Friday and you're not worried?\",\n",
        "\"Could you be happy here with me?\",\n",
        "\"What do you remember about your mother?\",\n",
        "\"Do you think he crashed the car on purpose?\",\n",
        "\"Do you ever think we should just stop doing this?\",\n",
        "\"Have you read the newspaper stories about my wife?\",\n",
        "\"You had time to call the police. Why didn't you?\",\n",
        "\"Are you taking his side against me?\",\n",
        "\"Do you remember we used to do that in school?\",\n",
        "\"How did he get away?\",\n",
        "\"You've taken her back? You can't be serious?\",\n",
        "\"Spare some change, please?\",\n",
        "\"Were you really looking for me?\",\n",
        "\"What did you have in the garden?\",\n",
        "\"Why did you talk like that?\",\n",
        "\"What do you mean, you've lost the lottery ticket?\",\n",
        "\"Am I supposed to be scared now?\",\n",
        "\"I just want a nice, easy life. What's wrong with that?\",\n",
        "\"How have you been all these years?\",\n",
        "\"Oh yeah, you really told him, didn't you?\",\n",
        "\"Why didn't he come and talk to me himself?\",\n",
        "\"Why would you want to put yourself through something like that?\",\n",
        "\"Perhaps you'll take me out one day - or do I have to make an appointment?\",\n",
        "\"Where would a child go in a small place like this?\",\n",
        "\"You don't think that was just lemonade in your glass, do you?\",\n",
        "\"How could I leave my children?\",\n",
        "\"Would you come to my funeral?\",\n",
        "\"Does he know about the baby?\",\n",
        "\"Am I under arrest, or not?\",\n",
        "\"How long have you been standing there?\",\n",
        "\"What are you major in university\",\n",
        "]\n",
        "EvaluateMultiInput(forward_encoder, forward_decoder, searcher, voc,input_arr)"
      ],
      "id": "W6u0vC0OGh-U",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "so why don t you ?\n",
            "we have we put out the top we we . .\n",
            "i saw a three times if i ve looked for a salesperson .\n",
            "oh . i ll tell be . .\n",
            "only about tea tea . i ll have tea at .\n",
            "we d like to order breakfast a may and we .\n",
            "would you like to show me a meeting ?\n",
            "what ll i do ? fine ? fine ?\n",
            "she does always good for me . . plus .\n",
            "that s my pleasure . have\n",
            "yes i have . so . camera .\n",
            "which one would you be ? moment\n",
            "before the food really is the food . food food .\n",
            "i have been going to have your flight .\n",
            "what did you do ? ? ?\n",
            "great idea . i know where you want .\n",
            "i m not certain . i ve always loved such such a few .\n",
            "i want to got my things . my for my my my . .\n",
            "that s a good idea . . concert .\n",
            "do you have your decision with the suspect ? ?\n",
            "what kind of music do you you you ?\n",
            "i love the same day .\n",
            "yes i have . your place is very much of this use .\n",
            "thanks . i appreciate it . . .\n",
            "thank you for the same . .\n",
            "i d like to see your please . . ?\n",
            "yes . let s say . said . . .\n",
            "no . it s very kind of you .\n",
            "that s a good idea . what did you you you you done ?\n",
            "i m serious . wrong wrong .\n",
            "yeah you re your card and where your you you friend ?\n",
            "do you have anything else ? ?\n",
            "i ll be glad to that . . .\n",
            "i hope i re going out of time . .\n",
            "let you d better wait for a visit . . story\n",
            "if you have always in chinese you have to always eat english .\n",
            "do you have any best ? ?\n",
            "i would like to talk with you else .\n",
            "i don t understand . did . . . it\n",
            "would you like to come there ? ?\n",
            "are you sure ? we ll to the the same . .\n",
            "yes i have learned a lot of you and i . .\n",
            "sure . how it you for it ?\n",
            "i just need to bring some money with such a month .\n",
            "sure . ll be finished built built ? .\n",
            "do you have some time ?\n",
            "why not ? so of for for .\n",
            "that s a secret . old . . .\n",
            "my name is james . .\n",
            "yes . the table is over there by the moment .\n",
            "i like this one . . small small . .\n",
            "yes i so . very very very .\n",
            "i m from germany .\n",
            "it s cloudy . . the . days . modern .\n",
            "yeah i didn t mean to . . .\n",
            "great . she is a lot of good as as of as well .\n",
            "i m accompany . the going\n",
            "i m living at room building n n . p . . .\n",
            "august st .\n",
            "it s area . about is . .\n",
            "it s bulldog . . .\n",
            "i work in a in company as i think .\n",
            "i m a keyboard operator . what s this going to ?\n",
            "i m in an english teacher . .\n",
            "i m going to work in a company with a company .\n",
            "yes i am . . important\n",
            "yes . just a nice family . . family\n",
            "Error: Encountered unknown word.\n",
            "my office me that you how to live around .\n",
            "i ll be in in in room . in in\n",
            "i usually have a to come at in . breakfast breakfast\n",
            "i will be there . . .\n",
            "photograph . door out .\n",
            "it s very good . and it s out of the world .\n",
            "in my opinion orange is very nice . . . .\n",
            "i m not feeling well . . . .\n",
            "i like like to . . .\n",
            "yes i did . .\n",
            "i work in the company . in . in in\n",
            "i sooner be there no . will be be in .\n",
            "i ve got a new . . .\n",
            "my major is business administration . major . in .\n",
            "my mother in english is english above fun . .\n",
            "i am studying german . german german german . german english german german german\n",
            "i am not sure . . . do .\n",
            "in a little learn . it . it it .\n",
            "a little doctor . .\n",
            "yes i can . i speak it very well . .\n",
            "english most . others english .\n",
            "i don t have any .\n",
            "i m it .\n",
            "no . i ve got the few different few few few .\n",
            "i actually just not eating the regular . . . .\n",
            "i don t know . i just woke up . .\n",
            "i ve just a better because and a little bit .\n",
            "it s degrees now . it was even colder this morning .\n",
            "it is . a . m . . . .\n",
            "today is may first . . . .\n",
            "i m fabulous . asking . .\n",
            "i m accompany . the going\n",
            "i m just going to stay in the yet . .\n",
            "i m going to my parents house for my father s birthday .\n",
            "the seaside . railway .\n",
            "which subway should i take to get to the side side ?\n",
            "yes . i need some help help . computers .\n",
            "it s hard to go but i guess the new . . .\n",
            "i just stayed at home and watched tv . .\n",
            "i m not sure yet . and you ?\n",
            "i guess i wasn t either of either . . play\n",
            "of course .\n",
            "what is it ? it it ?\n",
            "that s fine . . few this book .\n",
            "of course . . . . .\n",
            "i d like to a a look . cleared for my girlfriend .\n",
            "i m not sure . ballot book some bag .\n",
            "of course . the fitting room is this way please .\n",
            "of course . way\n",
            "medium . .\n",
            "i m sorry . i don t know . .\n",
            "it s true . . up .\n",
            "yes may have a camera . have\n",
            "yuan . your . hundred .\n",
            "yuan . . . . .\n",
            "can i pay by card ?\n",
            "of course you can . . your .\n",
            "no that s all . thanks . . .\n",
            "bad . keeps nose is running running . . . burning .\n",
            "i had a fantastic time . fun . . .\n",
            "yes . i ll have the fresh fruit cocktail .\n",
            "i d love to .\n",
            "a glass of white wine would be a good day .\n",
            "yes i bought them black with dessert .\n",
            "it looks good .\n",
            "no i ll be . that . . .\n",
            "it s a bit thing i ve a a larger a . .\n",
            "yes i used a change on a change .\n",
            "yes i did . running running running running a .\n",
            "how do you do ? you advertised in in advertised\n",
            "that s oolong us guess .\n",
            "well she s quite short . .\n",
            "she s quite a . back .\n",
            "he s not really in .\n",
            "not too bad . but i will be soon .\n",
            "i like him . he is a very tight . . .\n",
            "yes . it is . . .\n",
            "i m looking for a book my for help .\n",
            "i m a clerk in and the . to .\n",
            "i m not sure yet . .\n",
            "i m going to some about . some of the lamb and have a\n",
            "yes what s up ? to to\n",
            "yes i have been . two you . weeks .\n",
            "that s it nice . what s the one ?\n",
            "oh that would be lovely . i would love to .\n",
            "i m going to the hospital . . .\n",
            "i d be glad to . good .\n",
            "that sounds great . where i that ?\n",
            "do you travel a library ? ? ?\n",
            "no . . . i only had a few years quickly .\n",
            "i m going to work every once in a week . .\n",
            "yes it is . . . . .\n",
            "about a week . . .\n",
            "i usually just park . i . . .\n",
            "i ll be in let if you let me .\n",
            "here . some some some some .\n",
            "well . . . at the bridge . .\n",
            "i ll be sorry she s out number .\n",
            "why don t she call back later this afternoon .\n",
            "yes i called . to up get up get .\n",
            "no thanks . i will call you to that .\n",
            "months months . months years years years . .\n",
            "i m planning on the beijing . .\n",
            "at . . . . . .\n",
            "i feel dizzy .\n",
            "they give . . . .\n",
            "since yesterday . for three days days days days days\n",
            "i like it very well . well it should get english .\n",
            "in . . . . . .\n",
            "yes . here you are . . .\n",
            "no these are all my personal effects . .\n",
            "no i have one just this one . .\n",
            "table to be one . if one seat is available .\n",
            "it s a . . . . .\n",
            "i m not sure yet . get in killing me yet .\n",
            "only one night . . . . .\n",
            "don t see see let s go and see . .\n",
            "i would like to buy a new fridge . . .\n",
            "you can go straight to the . then by the . .\n",
            "there is some counter for the meat . .\n",
            "i would like . .\n",
            "sure . can you you lower the price by the smoking ?\n",
            "no . you would need a problem . .\n",
            "no thanks . ll . .\n",
            "i am not out for for but or am . .\n",
            "we open a at at pm pm\n",
            "i am seven in in twenty o clock . . .\n",
            "yes i do . name\n",
            "double rooms with twin beds .\n",
            "here it is . passport here . here\n",
            "sure .come on in at .\n",
            "yes could you please go with me ? ?\n",
            "three nights . days longer perhaps\n",
            "we have extra the same under the national the . .\n",
            "the . is there the tour .\n",
            "yes . they ve been working hard . .\n",
            "it s a lot better than the other . . .\n",
            "i prefer to ride one by train . .\n",
            "we are a lot time .what .what shall be going ?\n",
            "yes i did . i just bought this lost .\n",
            "three you a little bit tight . do you have alcohol ?\n",
            "yes i will try the kilos . . . .\n",
            "let you have a i taken student you . a . .\n",
            "are you serious ? ! such ! such ! so ! ! !\n",
            "i did with that . with won t . in .\n",
            "it s fair fair at . fair fair story\n",
            "in some of the card and have a . .\n",
            "i m only nervous about what about you ? about ?\n",
            "sure . the the for for for me to for my credit card .\n",
            "she s a secret of teacher . . .\n",
            "no but it s no but it it does . it s . .\n",
            "perhaps you d like to . have a while .\n",
            "i ve only had an international business in three times .\n",
            "yes i have to have to take me . next\n",
            "of course ! most apologize such t t\n",
            "i m not sure . to to\n",
            "he was out of front . .\n",
            "no i couldn t joke . . .\n",
            "ok . i ll have a change .\n",
            "i don t know . what s the do of your ?\n",
            "i saw a court can . . . . .\n",
            "i can t understand if i can .\n",
            "i am just broke with my impression .\n",
            "i ll be back . . .\n",
            "you have a great driver . that .\n",
            "fine yes . few very three three . been .\n",
            "of course not . i ll get you . .\n",
            "good idea ! s not not not . . .\n",
            "i would like a chicken please . moment cup\n",
            "we need appointment monday night . need need\n",
            "it s a that for to . do .\n",
            "no but i just feel gym better than than than it .\n",
            "did you tell you up enough ?\n",
            "of course .\n",
            "no he looked at the next . thing . . .\n",
            "you should have a part of movie . .\n",
            "i began to there online . about . about . .\n",
            "i studied my food . i major major major\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLch8uX5hzAN"
      },
      "source": [
        "## Tạo đoạn hội thoại ngẫu nhiên"
      ],
      "id": "DLch8uX5hzAN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RsDKZmtWCwPF",
        "outputId": "1f931408-3779-44c5-9382-4f5039372c32"
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "forward_encoder.eval()\n",
        "forward_decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(forward_encoder, forward_decoder)\n",
        "#input_sentence=input('')\n",
        "input_sentence=input('')\n",
        "Simulate(forward_encoder,forward_decoder,searcher,voc,input_sentence)"
      ],
      "id": "RsDKZmtWCwPF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hi\n",
            "oh sounds good . i ll have to meet you at .\n",
            "thank you for coming . and you re coming along .\n",
            "i m just coming coming . what about you ? ?\n",
            "i m just at work house . . . .\n",
            "what are you going to do then ? do do !\n",
            "can t i sit just put it on the ? put in the\n",
            "i hope you ll do the wrong to wash . . .\n",
            "all right . ?\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fpExO6pNIy0",
        "outputId": "15d9a3fd-d442-4f80-ccdc-042fa9f565f2"
      },
      "source": [
        "# Set dropout layers to eval mode\n",
        "forward_encoder.eval()\n",
        "forward_decoder.eval()\n",
        "\n",
        "# Initialize search module\n",
        "searcher = GreedySearchDecoder(forward_encoder, forward_decoder)\n",
        "questions = [\n",
        "    \"Hi\",\n",
        "    \"Hello\",\n",
        "    \"Nice to see you\",\n",
        "    \"How are you doing?\",\n",
        "    \"Good morning\",\n",
        "    \"Good evening\",\n",
        "    \"What is your name?\",\n",
        "    \"How can I help you?\",\n",
        "    \"What are you up to?\",\n",
        "    \"Do you want to hang out?\",\n",
        "    \"Let's go out for dinner\",\n",
        "    \"I will see you again soon\",\n",
        "    \"What do you want to talk about?\",\n",
        "    \"Can you help me?\",\n",
        "    \"When are you free?\",\n",
        "    \"How are you at school?\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "  print(question)\n",
        "  Simulate(forward_encoder,forward_decoder,searcher,voc,question)\n",
        "  print(\"-------------------------------------------\")"
      ],
      "id": "4fpExO6pNIy0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hi\n",
            "oh that s a good . see you there .\n",
            "thank you . . you . . . .\n",
            "not ! . . . .\n",
            "what s you matter ?\n",
            "i m not a . you in what your .\n",
            "well that sounds good . i will always go in the .\n",
            "really ? i love you !\n",
            "you are so right . i you you to\n",
            "-------------------------------------------\n",
            "Hello\n",
            "hi . ?\n",
            "no . . . what s happening ?\n",
            "that s a night . . . . . . . . .\n",
            "yeah . the performance the the\n",
            "let s show it .\n",
            "do we get something wrong ?\n",
            "it s up . no because t . .\n",
            "what ?\n",
            "-------------------------------------------\n",
            "Nice to see you\n",
            "it s a good time hasn t you ? family family\n",
            "yes i can do very much . family .\n",
            "are you willing to go go to school by\n",
            "which school sounds good ?\n",
            "i think most is the most .\n",
            "the best of the is the the . .\n",
            "it goes like . i ll take it .\n",
            "how much is it ?\n",
            "-------------------------------------------\n",
            "How are you doing?\n",
            "i m fine . thanks for asking .\n",
            "what did you need ?\n",
            "i just need some hair .\n",
            "how much did you get me ?\n",
            "it s the end of the . . .\n",
            "what does the dear say ?\n",
            "it s to the .\n",
            "do you have to go to ? ? ?\n",
            "-------------------------------------------\n",
            "Good morning\n",
            "good morning . ?\n",
            "good morning . ? ? ? ? ?\n",
            "no . not just like . .\n",
            "i see . don t . like to . not . .\n",
            "i can t it pleased to this .\n",
            "so i don t know how long does it be ?\n",
            "i believe you how is everything at the\n",
            "i ll come back to the doctor . .\n",
            "-------------------------------------------\n",
            "Good evening\n",
            "good evening . have you a reservation ?\n",
            "no . i d like to . evening evening\n",
            "ok here will you ? will\n",
            "oh let s come here . .\n",
            "ok . here is for the .\n",
            "thank you . . . . .\n",
            "is there anything else you want to to ?\n",
            "no that s a small to the . that s .\n",
            "-------------------------------------------\n",
            "What is your name?\n",
            "my name is sam jean . . .\n",
            "how may i have your vacation sam ?\n",
            "you can get a little longer . .\n",
            "i can t know your temperature .\n",
            "i have a a a little a a . doctor\n",
            "do you mind it mind me to get hurt ?\n",
            "let me get not much .\n",
            "if you have to treat you . this\n",
            "-------------------------------------------\n",
            "How can I help you?\n",
            "i d like to buy some check and check up .\n",
            "i got check for two those check please .\n",
            "all right . please have a check .\n",
            "thank you . check .\n",
            "i am sorry . i will write it here .\n",
            "will you have your id card with you ?\n",
            "yes i have my ticket and my card here .\n",
            "you should have some id . id id\n",
            "-------------------------------------------\n",
            "What are you up to?\n",
            "i just just come to the my city . my wanna come .\n",
            "when will you come to ?\n",
            "when will it come to my ? ?\n",
            "the should is half an . . . .\n",
            "how much do do you want to use it ?\n",
            "i want to to it . .\n",
            "here s the keep together . .\n",
            "how long have a been let ?\n",
            "-------------------------------------------\n",
            "Do you want to hang out?\n",
            "yes i do .\n",
            "do you you one one ? one\n",
            "of the case . i ll take the ride .\n",
            "do you have your to pay on your phone ?\n",
            "yes do you have it pay to pay ?\n",
            "no i don t pay for .\n",
            "ok . would you have to pay someone for ?\n",
            "well . how much is it please ?\n",
            "-------------------------------------------\n",
            "Let's go out for dinner\n",
            "don t be a where .\n",
            "what kind of ? does she have ?\n",
            "she s a private garden and the room the\n",
            "what floor the the floor ?\n",
            "it should be getting to the book in the .\n",
            "thank you . we should have it should . . .\n",
            "it s can take you . it .\n",
            "ok . i ll take it .\n",
            "-------------------------------------------\n",
            "I will see you again soon\n",
            "thank you professor a taxi . you you you\n",
            "where are you going ?\n",
            "i m going to the station .\n",
            "you will take the .\n",
            "how much do i take ?\n",
            ". . . .\n",
            "where s you ?\n",
            "i m .\n",
            "-------------------------------------------\n",
            "What do you want to talk about?\n",
            "i have a some questions about my lease .\n",
            "what s like ? ? like ?\n",
            "it s very nice and . . .\n",
            "may i get the last day ?\n",
            "yes . you what what\n",
            "we saw a great party . how about\n",
            "i ll have that you could that .\n",
            "i ll get you the what that you\n",
            "-------------------------------------------\n",
            "Can you help me?\n",
            "sure . we do .\n",
            "is it your table ?\n",
            "yes it is . our\n",
            "i short the yellow ones paul .\n",
            "yeah i can teach some some you .\n",
            "you were going to be too kind . you\n",
            "i know where i get the place\n",
            "you do get there in the street .\n",
            "-------------------------------------------\n",
            "When are you free?\n",
            "on the next saturday . .\n",
            "is it on the right now ? ?\n",
            "yes that s right .\n",
            "what do you do ? name in ?\n",
            "my father is name just in the the .\n",
            "well he has a seat . .\n",
            "yeah . he has a bit to for me !\n",
            "how s he ? he\n",
            "-------------------------------------------\n",
            "How are you at school?\n",
            "i m school . i . .\n",
            "good luck !\n",
            "thanks !\n",
            "you re forgotten .\n",
            "what ? ?\n",
            "i said i don t have insurance in the . .\n",
            "i have mine to eat to get to . . . .\n",
            "what ? ! ! !\n",
            "-------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}